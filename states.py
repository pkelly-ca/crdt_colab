# -*- coding: utf-8 -*-
"""[CTP] CRDT Long Calculations (Official)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CExjy3Af3guUTKub0mVIVdCZKQgj3h4L
"""

# Necessary Libraries
import pandas as pd
import requests
import numpy as np
import urllib
import inspect
from urllib.parse import unquote
from io import StringIO, BytesIO
from bs4 import BeautifulSoup
import re
import zipfile
import datetime
from datetime import date, timedelta
import PyPDF2
import tabula
import auth
import gspread
from IPython.display import display
import os, time
os.environ['TZ'] = 'America/New_York'
time.tzset()
import json
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
import inspect
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains

# for S3 backup
import boto3


# Functions

#start webdriver #to use just write wd = init_driver() in the state function
def init_driver():
  options = webdriver.ChromeOptions()
  options.add_argument('--headless')
  options.add_argument('--no-sandbox')
  options.add_argument('--disable-dev-shm-usage')
  #wd = webdriver.Chrome('chromedriver',options=options)
  wd = webdriver.Chrome(ChromeDriverManager().install(),options=options)
  return wd

def writeTable(df,title,startCell,ws):
  i = 0
  maindir = 'crdt_' +  date.today().strftime('%m%d%y')
  func = inspect.stack()[1][3]
  state = func[-2:]
  statedir = maindir + "/" + state
  if not os.path.exists(maindir):
    os.mkdir(maindir)
  if not os.path.exists(statedir):
    os.mkdir(statedir)
  highest_num = 0
  for f in os.listdir(statedir):
    if os.path.isfile(os.path.join(statedir, f)):
        file = os.path.splitext(f)[0]
        file_num = int(file[-1])
        if file_num > highest_num:
            highest_num = file_num
  i = highest_num + 1

  # save this to a filename like: "AK-20210304-1.csv"
  date_str = datetime.datetime.now().strftime("%Y%m%d")
  path = '%s-%s-%d.csv' % (state, date_str, i)
  full_path = os.path.join(statedir, path)
  df.to_csv(full_path)
  if ws == True:
      gd_path = "/Users/User/Google Drive (patkellyatx@gmail.com)/CRDT"
      state_path = gd_path + "/" + state
      if not os.path.exists(state_path):
          os.mkdir(state_path)
      df.to_csv(state_path + "/" + path)
  else:
  # attempt to write to S3
    s3 = boto3.resource('s3')
    bucket_name = 'covid-tracking-project-data'
    s3_path = os.path.join('CRDT', state, path)
    try:
        s3.meta.client.upload_file(full_path, bucket_name, s3_path)
        display('S3 upload successful to %s' % s3_path)
    except Exception as e:
        display('Skipping S3 upload for %s' % state)
    
def retry_wait_click_all(wd, interval, type, path):
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver import ActionChains
    from webdriver_manager.chrome import ChromeDriverManager

    wait = WebDriverWait(wd, interval)
   
    result = False
    attempts = 0
    while attempts < 5:
        try:
          display('Trying...',type,path)
          if type == 'xpath':
              ele = wait.until(EC.element_to_be_clickable((By.XPATH,path)))
              ele.click()
          elif type == 'css':
              ele = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR,path)))
              ele.click()
          else:
              print("error in wait.until..... choice - not XPATH or CSS")
   
          display('number of attempts = ',attempts+1)
          result = True;
          break
        except Exception as e:
          display('...failed (sleeping)')
          display('Exception:',e)
          time.sleep(5)
          attempts+=1
    return result   
  

# -*- coding: utf-8 -*-
"""[CTP] CRDT Long Calculations (Official)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CExjy3Af3guUTKub0mVIVdCZKQgj3h4L
"""

# AK
#basic dependencies
#from bs4 import BeautifulSoup
#import pandas as pd
#import time

def runAK(ws, write):
  link ="https://coronavirus-response-alaska-dhss.hub.arcgis.com/datasets/table-3-demographic-distribution-of-confirmed-cases/data"
  wd = init_driver()
  wd.get(link) #visit site
  time.sleep(5) #ensure enough time for page to load
  soup = BeautifulSoup(wd.page_source, "html.parser")
  csv_tag = soup.find("a", class_="csv")
  csv = csv_tag["href"]
  display(csv)
  data = pd.read_csv(csv)[13:]
  df_dem = data.loc[:,["Demographic","All_Cases", "Hospitalizations", "Deaths"]]
  df_dem.columns = ["Category", "Cases", "Hospitalizations", "Deaths"]
  df_dem = df_dem.set_index('Category')
  df_dem.loc['Total'] = df_dem.iloc[0:4].sum()
  df_dem = df_dem.reset_index()
  display(df_dem)
  wd.quit()

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('J17',dataToWrite)

    # Write Data To Sheet
    writeTable(df_dem,'AK Hospitalization & Death Demographics (Residents Only)','J19',ws)

# AL
def runAL(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains

  def colstr2int(df,col):
    df.loc[:,col] = df.loc[:,col].replace(',','', regex=True)
    df[col] = df[col].astype('int')

  url = 'https://alpublichealth.maps.arcgis.com/apps/dashboards/6d2771faa9da4a2786a509d82c8cf0f7'

  wd = init_driver()
  wd.get(url)
  wd.maximize_window()
  wait = WebDriverWait(wd, 60)
  prob_cases = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR,"body > div > div > div > div.flex-fluid.flex-horizontal.position-relative.overflow-hidden > div > div > div > margin-container > full-container > div:nth-child(35) > margin-container > full-container > div > div.widget-body.flex-fluid.full-width.flex-vertical.justify-content-center.overflow-hidden > div > div > svg > g.responsive-text-label > text"))).text
#  prob_cases = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR,"div:nth-child(34) > margin-container > full-container > div > div.widget-body.flex-fluid.full-width.flex-vertical.justify-content-center.overflow-hidden > div > div > svg > g:nth-child(2) > text"))).text
  prob_deaths = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR,"body > div > div > div > div.flex-fluid.flex-horizontal.position-relative.overflow-hidden > div > div > div > margin-container > full-container > div:nth-child(44) > margin-container > full-container > div > div.widget-body.flex-fluid.full-width.flex-vertical.justify-content-center.overflow-hidden > div > div > svg > g.responsive-text-label > text"))).text
#  prob_deaths = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR,"div:nth-child(45) > margin-container > full-container > div > div.widget-body.flex-fluid.full-width.flex-vertical.justify-content-center.overflow-hidden > div > div > svg > g.responsive-text-label > text"))).text
  wd.quit()

  df_prob = pd.DataFrame([['Probable',prob_cases,prob_deaths]],columns=['Category','Cases','Deaths'])
  colstr2int(df_prob,'Cases')
  colstr2int(df_prob,'Deaths')
  display(df_prob)



  url = 'https://services7.arcgis.com/4RQmZZ0yaZkGR1zy/ArcGIS/rest/services/DIED_FROM_COVID19_STWD_DEMO_PUBLIC/FeatureServer/0/query?where=1%3D1&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&returnGeodetic=false&outFields=EthnicityCat%2C+DiedFromCovid19&returnGeometry=false&returnCentroid=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pjson&token='
  req = requests.get(url)
  df_deaths_eth = pd.json_normalize(req.json()['features'], sep='_')
  df_deaths_eth = df_deaths_eth.rename(columns=lambda x: re.sub('attributes_','',x))
  display(df_deaths_eth)

  url = 'https://services7.arcgis.com/4RQmZZ0yaZkGR1zy/ArcGIS/rest/services/DIED_FROM_COVID19_STWD_DEMO_PUBLIC/FeatureServer/1/query?where=1%3D1&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&returnGeodetic=false&outFields=RaceCat%2C+DiedFromCovid19&returnGeometry=false&returnCentroid=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pjson&token='
  req = requests.get(url)
  df_deaths_race = pd.json_normalize(req.json()['features'], sep='_')
  df_deaths_race = df_deaths_race.rename(columns=lambda x: re.sub('attributes_','',x))
  display(df_deaths_race)

  url = 'https://services7.arcgis.com/4RQmZZ0yaZkGR1zy/ArcGIS/rest/services/Statewide_COVID19_CONFIRMED_DEMOG_PUBLIC/FeatureServer/1/query?where=1%3D1&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&returnGeodetic=false&outFields=Ethnicity%2C+Ethnicity_counts&returnGeometry=false&returnCentroid=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pjson&token='
  req = requests.get(url)
  df_cases_eth = pd.json_normalize(req.json()['features'], sep='_')
  df_cases_eth = df_cases_eth.rename(columns=lambda x: re.sub('attributes_','',x))
  display(df_cases_eth)

  url = 'https://services7.arcgis.com/4RQmZZ0yaZkGR1zy/ArcGIS/rest/services/Statewide_COVID19_CONFIRMED_DEMOG_PUBLIC/FeatureServer/3/query?where=1%3D1&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&returnGeodetic=false&outFields=Racecat%2C+Race_counts&returnGeometry=false&returnCentroid=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pjson&token='
  req = requests.get(url)
  df_cases_race = pd.json_normalize(req.json()['features'], sep='_')
  df_cases_race = df_cases_race.rename(columns=lambda x: re.sub('attributes_','',x))
  display(df_cases_race)



  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('B36',dataToWrite)

    # Write Data To Sheet
    writeTable(df_cases_eth,'Ethnicity Cases','A38',ws)
    writeTable(df_cases_race,'Race Cases','A43',ws)
    writeTable(df_deaths_eth,'Ethnicity Deaths','E38',ws)
    writeTable(df_deaths_race,'Race Deaths','E43',ws)
    writeTable(df_prob,'','',ws)

# AR
def runAR(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains

  print("\nAR Hispanic Ethnicity % Deaths")
  url = 'https://adem.maps.arcgis.com/apps/opsdashboard/index.html#/f09960f2948d43d39913400dad1af77c'

  wd=init_driver()
  wd.get(url)
  wd.maximize_window()
  wait = WebDriverWait(wd, 160)
  hisp_eth = wait.until(EC.presence_of_element_located((By.XPATH,"//*[text()[contains(.,'are Hispanic:')]]")))
  time.sleep(15)
  display(hisp_eth.get_attribute('innerHTML'))
  hisp_eth = hisp_eth.get_attribute('innerHTML').split(': ')[1]
  hisp_eth = re.sub(r'[%\n ]','',hisp_eth)
  wd.quit()

  df_eth_d = pd.DataFrame([['Hispanic',hisp_eth + '%']],columns=['Category','Deaths'])
  display(df_eth_d)

  #totals, case dems, death dems
  print("\nAR Cases")
  urlCases='https://services.arcgis.com/PwY9ZuZRDiI5nXUB/ArcGIS/rest/services/UPDATED_ADH_COVID19_STATE_METRICS/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=positives%2C+black%2C+white%2C+na%2C+asian%2C+pi%2C+other_race%2C+unk_race%2C+multi_race%2C+nonhispanic%2C+hispanic&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urlCases)
  df_cases=pd.json_normalize(req.json()['features'])
  df_cases=df_cases.rename(columns=lambda x: re.sub('properties.','',x))
  display(df_cases)

  print("\nAR Deaths")
  urlDeaths='https://services.arcgis.com/PwY9ZuZRDiI5nXUB/ArcGIS/rest/services/UPDATED_ADH_COVID19_STATE_METRICS/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=deaths%2C+d_black%2C+d_white%2C+d_na%2C+d_asian%2C+d_pi%2C+d_other_race%2Cd_unk_race%2Cd_multi_race&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urlDeaths)
  df_deaths=pd.json_normalize(req.json()['features'])
  df_deaths=df_deaths.rename(columns=lambda x: re.sub('properties.','',x))
  display(df_deaths)
  #split into two tables

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('B33',dataToWrite)

    # Write Data To Sheet
    writeTable(df_cases,'Case Demographics','A34',ws)
    writeTable(df_deaths,'Death Demographics','A37',ws)
    writeTable(df_eth_d,'','',ws)

# CA
#import pandas as pd
# import requests

def runCA(ws, write):
  # CA Case & Deaths Totals, Confirmed & Probables"
  #url = 'https://data.ca.gov/dataset/590188d5-8545-4c93-a9a0-e230f0db7290/resource/926fd08f-cc91-4828-af38-bd45de97f8c3/download/statewide_cases.csv'
  url = 'https://data.chhs.ca.gov/dataset/f333528b-4d38-4814-bebb-12db1f10f535/resource/046cdd2b-31e5-4d34-9ed3-b48cdbc4be7a/download/covid19cases_test.csv'
  df_totals = pd.read_csv(url,parse_dates=['date'])
  maxdateTot = df_totals ['date'].max()
  print('\nCase & Death Totals')
  print(maxdateTot,'\n')
  df_totals = df_totals.groupby(['area'])[['reported_cases','reported_deaths']].sum()
  df_totals = df_totals[df_totals.index.isin(['California'])]
  df_totals = df_totals.astype('int')
  display(df_totals)

  # CA Race & Ethnicity
  #url = 'https://data.ca.gov/dataset/590188d5-8545-4c93-a9a0-e230f0db7290/resource/7e477adb-d7ab-4d4b-a198-dc4c6dc634c9/download/case_demographics_ethnicity.csv'
  url = 'https://data.chhs.ca.gov/dataset/f333528b-4d38-4814-bebb-12db1f10f535/resource/e2c6a86b-d269-4ce1-b484-570353265183/download/covid19casesdemographics.csv'
  #df_raceeth = pd.read_csv(url,parse_dates=['date'])
  df_raceeth = pd.read_csv(url,parse_dates=['report_date'])

  print ('\nCA Race and Ethnicity Totals')
  maxdateRace = df_raceeth ['report_date'].max()
  #maxdateRace = df_raceeth ['date'].max()
  print(maxdateRace,'\n')
  df_dem = df_raceeth[df_raceeth['report_date'] == maxdateRace]
  df_dem = df_dem[df_dem['demographic_category'] == 'Race Ethnicity']
  df_dem = df_dem.drop(df_dem.columns[[0,3,5,6]],axis=1)
  display(df_dem)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('F16',dataToWrite)

    # Write Data To Sheet
    writeTable(df_dem,'CA Race and Ethnicity Totals','G23',ws)
    writeTable(df_totals,'Case & Death Totals','G18',ws)

#CO

def runCO(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC

  # CO Total Cases, Total Deaths, Percent of Cases by Race, Percent of Cases by Ethnicity
  #url='https://data-cdphe.opendata.arcgis.com/datasets/cdphe-covid19-state-level-expanded-case-data/data'
  #dnld_xpath='//*[@class="btn dropdown-toggle btn-default hub-download"]'
  #dnld_xpath_caret='//*[@class="caret"]'
  #url='https://opendata.arcgis.com/datasets/15883575464d46f686044d2c1aa84ef9_0.csv'
  url='https://prod-hub-indexer.s3.amazonaws.com/files/a0f52ab12eb4466bb6a76cc175923e62/0/full/4326/a0f52ab12eb4466bb6a76cc175923e62_0_full_4326.csv'

  df_totals = pd.read_csv(url,parse_dates=['date'])
  print(url)
  #display(df_totals)
  maxDateCase = df_totals ['date'].max()

  print(maxDateCase, '\n')
  df_tot=df_totals[df_totals['description'] == 'Cumulative counts to date']
  df_tot=(df_tot[df_tot['date']== maxDateCase])
  df_tot['value'].astype(float)
  
  # Find the case Totals & death Totals
  df_tot=df_tot.drop(df_tot.columns[[0,1,2,3,6,7]],axis=1).reset_index() 
  print("CO Totals")
  display(df_tot)
  caseTot=(df_tot[df_tot['metric'] == 'Cases'])
  cases=caseTot['value']
  #print(cases)
  deathTot=(df_tot[df_tot['metric'] == 'Deaths Among Cases'])
  deaths=deathTot['value']
  #print(deaths)

  # find the Case Demographics
  df_caseDems=df_totals[df_totals['description']== 'Percent of Cases by Race and Ethnicity']
  df_caseDems=(df_caseDems[df_caseDems['date'] == maxDateCase])
  df_caseDems=df_caseDems.drop(df_caseDems.columns[[0,2,3,6,7]],axis=1).reset_index() 
  df_caseDems['value'].astype(float)
  df_caseDems['Case Count']=df_caseDems['value']
  display(df_caseDems)
  for i in range(len(df_caseDems)):
    df_caseDems.iloc[i, 4]=round(df_caseDems.iloc[i, 4] * cases.iloc[0]) # PK fix
  display(df_caseDems)

  # find the Death Demographics
  df_deathDems = df_totals[df_totals['description'] == 'Percent of Deaths by Race and Ethnicity']
  df_deathDems= (df_deathDems[df_deathDems['date'] == maxDateCase])
  df_deathDems=df_deathDems.drop(df_deathDems.columns[[0,2,3,6,7]],axis=1).reset_index()
  df_deathDems['value'].astype(float)
  df_deathDems['Death Count']=df_deathDems['value']
  print('6')
  for i in range(len(df_deathDems)):
    df_deathDems.iloc[i, 4]=round(df_deathDems.iloc[i, 4] * deaths.iloc[0]) # PK fix

  print("CO Case Demographic %")
  display(df_caseDems)
  print("CO Death Demographic %")
  display(df_deathDems)

  if(write == 1):
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('F16',dataToWrite)

    # Write Data To Sheet
    #writeTable(df_cases,'Case & Death Totals','G18',ws)
    writeTable(df_tot,'CO Totals','G23',ws)
    writeTable(df_caseDems, 'CO Case Demographics', 'H23', ws)
    writeTable(df_deathDems, 'CO Death Demographics', 'J23', ws)
  
  

#CT

def runCT(ws, write):

  # CT Case & Deaths Totals, Confirmed & Probables"
  url = 'https://data.ct.gov/api/views/rf3k-f8fg/rows.csv?accessType=DOWNLOAD&bom=true&format=true'
  df_totals = pd.read_csv(url,parse_dates=['Date'])

  pd.set_option('chained_assignment',None)
  print('CT Case & Deaths Totals, Confirmed & Probables')

  #Select the latest date
  maxdate2 = df_totals ['Date'].max()
  print(maxdate2,'\n')

  #Select the data from the latest date
  df_tot = df_totals[df_totals['Date'] == maxdate2]
  #Drop the extra columns
  df_tot.drop(df_tot.columns[[0,1,11,12,13,14,15]],axis=1,inplace=True)
  #Replace commas with nothing
  df_totals2=df_tot.replace({',':''}, regex=True)
  #Convert strings to numeric
  df_tot=df_totals2.apply(pd.to_numeric)
  #Fill in Nan with 0
  df_tot=df_tot.fillna(0)
  display(df_tot)

  # CT Race & Ethnicity
  url = 'https://data.ct.gov/api/views/7rne-efic/rows.csv?accessType=DOWNLOAD&bom=true&format=true'
  df_raceeth = pd.read_csv(url,parse_dates=['Date updated'])

  print ('\n CT Race and Ethnicity Totals')
  #select latest date
  maxdateRace2 = df_raceeth ['Date updated'].max()
  print(maxdateRace2,'\n')
  #select rows for latest date
  df_dem = df_raceeth[df_raceeth['Date updated'] == maxdate2]
  #drop extra columns
  df_dem.drop(df_dem.columns[[0,2,4,5,7,8]],axis=1,inplace=True)
  #replace , with nothing
  df_dems2=df_dem.replace({',':''},regex=True)
  #convert individula columns from string to numeric
  df_dems2['Total cases']=pd.to_numeric(df_dems2['Total cases'])
  df_dems2['Total deaths']=pd.to_numeric(df_dems2['Total deaths'])
  #replace NaN with 0
  df_dems2=df_dems2.fillna(0)
  display(df_dems2)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('F19',dataToWrite)

    # Write Data To Sheet
    writeTable(df_tot,'CT Case & Deaths Totals, Confirmed & Probables','G18',ws)
    writeTable(df_dems2,'CT Race and Ethnicity Totals','G22',ws)

# DC
def runDC(ws,write):

  # Function to wrangle data after reading from excel file
  def wrangle(df):
    # mapping of date format, only in date column names
    mapper = lambda x: x.strftime("%d-%b") if isinstance(x, datetime.datetime) else x
    # determine max date by finding maximum of date headers
    max_date = max(d for d in df.columns.values if isinstance(d, datetime.date))
    # Only keep columns with maximum date and the category names
    drop_cols = df.columns.difference([max_date,'Unnamed: 0'])
    df.drop(drop_cols, 1, inplace=True)
    # Remove NaNs, convert columns to integer (removes .0's)
    df = df.fillna(0)
    df.replace(u'\xa0',u'', regex=True, inplace=True)
    df.replace(',','', regex=True, inplace=True)
    df[max_date] = df[max_date].astype('int32')
    # Apply mapper function
    df.columns = df.columns.map(mapper)
    df = df.rename(columns = {'Unnamed: 0':'Race'})
    return df


  # Get DC excel file
  url = 'https://coronavirus.dc.gov/data'
  req = requests.get(url)
  soup = BeautifulSoup(req.text, 'html.parser')
  a = soup.find('a', string=re.compile("Download copy of DC COVID-19 data"))
#  link = "https://coronavirus.dc.gov/{}".format(a['href'])
  if a['href'][0:5] == 'https':
    link = "{}".format(a['href'])
  else:
    link = "https://coronavirus.dc.gov/{}".format(a['href'])
  display(link)
  res = requests.get(link)

  # Read, wrangle race cases tab
  df_cases = pd.read_excel(BytesIO(res.content), sheet_name="Total Cases by Race", skiprows=[0,2], engine='openpyxl')
  df_cases = wrangle(df_cases)
  display(df_cases)

  # Read, wrangle race deaths tab
  df_deaths = pd.read_excel(BytesIO(res.content), sheet_name="Lives Lost by Race", skiprows=[1], engine='openpyxl')
  df_deaths = wrangle(df_deaths)
  display(df_deaths)

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('H17',dataToWrite)

    # Write Data To Sheet
    writeTable(df_cases,'Total Cases by Race','I16',ws)
    writeTable(df_deaths,'Lives Lost by Race','K16',ws)

# DE
def runDE(ws,write):
  def find_val(span):
    val = re.findall(r'\b\d{1,3}(?:,\d{3})*(?!\d)', span.text)
    val = int(val[0].replace(',',''))
    return val
  
  def make_table(script,txt):
    pattern = re.compile(r"App.Charts.SimpleBar\(\n\s*({\"name\".*})", flags=re.MULTILINE)
    scr = re.search(pattern, script)
    dict = json.loads(scr.groups(0)[0])
    keys = ['categories', 'data']
    dict = {x:dict[x] for x in keys}
    dict['data'] = dict['data'][0][1:]
    df = pd.DataFrame(dict)
    df.columns=df.columns.str.capitalize()
    df.rename(columns={'Categories': txt},inplace=True)
    df.Data = df.Data.astype('int')
    return df
  
  confirmed = []
  probable = []
  num_found = [0,0,0,0,0]
  
  url = 'https://myhealthycommunity.dhss.delaware.gov/locations/state/testing'
  req = requests.get(url)
  soup = BeautifulSoup(req.text, 'html5lib')
  
  # Find Total Tests
  span = soup.find_all('span')
  
  for i in range(0,len(span)):
    if ("State of Delaware" in span[i]) & (num_found[4]<2):
      if num_found[4] == 0:
        total_tests = find_val(span[i+1])
      num_found[4] += 1 
  
  all_script = soup.find_all('script')
  
  for scr in all_script:
    if "total_persons_tested_by_race_ethnicity" in scr.text:
      df_tests = make_table(scr.text,'Tests')
  
  url = 'https://myhealthycommunity.dhss.delaware.gov/locations/state/cases'
  req = requests.get(url)
  soup = BeautifulSoup(req.text, 'html5lib')
  
  # Find Total Cases
  span = soup.find_all('span')
  
  for i in range(0,len(span)):
    if ("Total Positive Cases" in span[i]) & (num_found[0]==0):
      total_cases = find_val(span[i+1])
      num_found[0] += 1
    if ("Confirmed" in span[i]) & (num_found[2]<2):
      confirmed.append(find_val(span[i+1]))
      num_found[2] += 1
    if ("Probable" in span[i]) & (num_found[3]<2):
      probable.append(find_val(span[i+1]))
      num_found[3] += 1
  
  all_script = soup.find_all('script')
  
  for scr in all_script:
    if "total_cases_by_race_ethnicity" in scr.text:
      df_cases = make_table(scr.text,'Cases')
  
  url = 'https://myhealthycommunity.dhss.delaware.gov/locations/state/deaths'
  req = requests.get(url)
  soup = BeautifulSoup(req.text, 'html5lib')
  
  # Find Total Deaths
  span = soup.find_all('span')
  
  for i in range(0,len(span)):
    if ("Total Deaths" in span[i]) & (num_found[1]==0):
      total_deaths = find_val(span[i+1])
      num_found[1] += 1
    if ("Confirmed" in span[i]) & (num_found[2]<2):
      confirmed.append(find_val(span[i+1]))
      num_found[2] += 1
    if ("Probable" in span[i]) & (num_found[3]<2):
      probable.append(find_val(span[i+1]))
      num_found[3] += 1
  
  
  all_script = soup.find_all('script')
  
  for scr in all_script:
    if "total_deaths_by_race_ethnicity" in scr.text:
      df_deaths = make_table(scr.text,'Deaths')
  
  df_totals = pd.DataFrame([["Cases",total_cases, confirmed[0],probable[0]],["Deaths",total_deaths, confirmed[1],probable[1]],["Tests",total_tests,0,0]],
                          columns=['Category','Total','Confirmed','Probable'])
  display(df_totals)
  
  display(df_tests)
  display(df_cases)
  display(df_deaths)
  
  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('H14',dataToWrite)

    # Write Data To Sheet
    writeTable(df_totals,'','G15',ws)
    writeTable(df_cases,'','G19',ws)
    writeTable(df_deaths,'','G30',ws)
    writeTable(df_tests,'','G41',ws)

# FL

def runFL(ws, write):
  url = 'http://ww11.doh.state.fl.us/comm/_partners/covid19_report_archive/cases-monitoring-and-pui-information/state-report/state_reports_latest.pdf'
  url2 = 'https://services1.arcgis.com/CY1LXxl9zlJeBuRZ/arcgis/rest/services/Florida_COVID19_Cases/FeatureServer/0/query?where=COUNTY_1%3D%27State%27&outFields=C_HospYes_Res%2C+C_HospYes_NonRes&f=json'

  req = requests.get(url2)
  dict = req.json()['features'][0]['attributes']
  df_hosp_totals = pd.DataFrame({'Metric': list(dict.keys()),'Values':list(dict.values())})
  display(df_hosp_totals)

  req = requests.get(url)
  pdf = BytesIO(req.content)
  first_page = PyPDF2.PdfFileReader(pdf).getPage(0).extractText()

  totals_start = re.search(r"Total cases\n",first_page)
  totals_end = re.search(r"Cases: people with positive PCR ",first_page)

  totals_table = first_page[totals_start.start(0):totals_end.start(0)]

  StringData = StringIO(totals_table)
  df_totals = pd.read_csv(StringData, sep ="\n", header=None)
  df_totals = df_totals.drop([12,45]) #remove headers that screw up order
  df_totals = pd.DataFrame(np.reshape(df_totals.values,(25,2)), columns=['Metric','Value'])
  display(df_totals)

  third_page = PyPDF2.PdfFileReader(pdf).getPage(2).extractText()

  footnote = re.search(r"Hospitalization\n counts include",third_page)
  race_start = re.search(r"Race and ethnicity",third_page)
  race_table = third_page[race_start.start(0):footnote.start(0)]

  body_start = re.search(r"White\n",race_table)
  body_end = re.search(r"Total\n",race_table)

  body = race_table[body_start.start(0):body_end.start(0)]

  StringData = StringIO(body)
  df_race = pd.read_csv(StringData, sep ="\n", header=None)
  df_race = pd.DataFrame(np.reshape(df_race.values,(16,7)), columns=['Race and ethnicity','Cases','','Hospitalizations','','Deaths',''])
  display(df_race)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('L23',dataToWrite)

    # Write Data To Sheet
    writeTable(df_hosp_totals,'Hospitalization Totals','N23',ws)
    writeTable(df_totals,'Overall Totals','P23',ws)
    writeTable(df_race,'Demographics','R23',ws)

# GA
#from io import StringIO, BytesIO
#from datetime import date
#import pandas as pd
#import requests
#import zipfile
#import pprint

def runGA(ws, write):
  print('Run Date:', date.today())

  url = 'https://ga-covid19.ondemand.sas.com/docs/ga_covid_data.zip'
  res = requests.get(url)
  zipdata = BytesIO(res.content)
  zip = zipfile.ZipFile(zipdata, 'r')

   #Case & Death Totals
  df_GA_totals = pd.read_csv(zip.open('summary_totals.csv'))
  print('\nTable for GA Totals')
  display(df_GA_totals)

  df_GA_demo = pd.read_csv(zip.open('demographics_by_race_eth.csv'))
  print('\nTable for Copying to Spreadsheet Area of State Page')
  #display(df_GA_demo)
  df_dems = df_GA_demo[df_GA_demo['county name'] == 'Georgia']
  df_dems = df_dems.groupby(['county name','ethnicity','race'])[['cases','hospitalization','deaths']].sum().reset_index()
  df_dems = df_dems.drop(['county name'],1)
  display(df_dems)

  print('\nRace Table to Match Category Sums')
  df_hisp = df_GA_demo[df_GA_demo['ethnicity'] == "Hispanic/Latino"]
  df_hisp = df_hisp.groupby(['ethnicity'])[['cases','hospitalization','deaths']].sum()
  df_nonhisp = df_GA_demo[df_GA_demo['ethnicity'] == "Non-Hispanic/Latino"]
  df_nonhisp = df_nonhisp.groupby(['race'])[['cases','hospitalization','deaths']].sum()
  df_race = df_nonhisp.append(df_hisp)

  print('\nEthnicity Table to Match Category Sums')
  df_eth = df_GA_demo.groupby(['ethnicity'])[['cases','hospitalization','deaths']].sum()
  display(df_eth)
  df_nonhisp = df_GA_demo[df_GA_demo['ethnicity'] == "Non-Hispanic/Latino"]
  df_nonhisp = df_nonhisp.groupby(['race'])[['cases','hospitalization','deaths']].sum()

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('F20',dataToWrite)

    # Write Data To Sheet
    writeTable(df_dems,'Table for Copying to Spreadsheet Area of State Page','G18',ws)
    writeTable(df_GA_totals,'','G41',ws)

# GU
def runGU(ws,write):

  url = 'https://govguamgis.maps.arcgis.com/apps/opsdashboard/index.html#/304a0edd115b49ff9f98ad52cb66d416'
  wd = init_driver()
  wd.get(url)
  time.sleep(10)
  soup = BeautifulSoup(wd.page_source, 'html.parser')

  # Find Totals
  g_all = soup.find_all('g',{"class": "amcharts-pie-item"})

  pattern = re.compile(r'aria-label=\"(.+) \"')
  demo = []
  for i in range(len(g_all)):
    cat = re.findall(pattern,str(g_all[i]))[0].split()
    if len(cat) == 4:
      cat[0] = cat[0] + '_' + cat[1]
      cat[1:3] = cat[2:4]
    demo.append(cat)

  df_demo = pd.DataFrame(demo).loc[:,[0,2]]


  df_demo_cases = df_demo.loc[2:9,:]
  df_demo_cases.columns = ['Category','Cases']
  df_demo_cases.Cases.replace(',','', regex=True, inplace=True)
  df_demo_cases.Cases = df_demo_cases.Cases.astype('int')
  df_demo_cases.sort_values(by=['Category'],inplace=True)


  df_demo_deaths = df_demo.loc[12:19,:]
  df_demo_deaths.columns = ['Category','Deaths']
  df_demo_deaths.Deaths = df_demo_deaths.Deaths.astype('int')
  df_demo_deaths.sort_values(by=['Category'],inplace=True)

  display(df_demo_cases)
  display(df_demo_deaths)

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('H19',dataToWrite)

    # Write Data To Sheet
    writeTable(df_demo_cases,'','G21',ws)
    writeTable(df_demo_deaths,'','I21',ws)

# HI ************

def runHI(ws,write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC

  #Tableau for Race
  src="https://public.tableau.com/views/HawaiiCOVID-19-RaceChart/Overview?:embed=y&:showVizHome=no&:host_url=https%3A%2F%2Fpublic.tableau.com%2F&:embed_code_version=3&:tabs=no&:toolbar=yes&:animate_transition=yes&:display_static_image=no&:display_spinner=no&:display_overlay=yes&:display_count=yes&null&:loadOrderID=11"

  casesSrc='https://services9.arcgis.com/aKxrz4vDVjfUwBWJ/arcgis/rest/services/HIEMA_COVID_CASES_PUBLIC_LATEST_1203/FeatureServer/0/query?f=json&where=1%3D1&returnGeometry=false&spatialRel=esriSpatialRelIntersects&outFields=*&orderByFields=current_active%20desc&resultOffset=0&resultRecordCount=1&resultType=standard&cacheHint=true'
  deathSrc='https://services9.arcgis.com/aKxrz4vDVjfUwBWJ/arcgis/rest/services/MMWR_Fatality_Latest/FeatureServer/0/query?where=county%3D%27State%27&objectIds=&time=&resultType=none&outFields=*&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  hospSrc='https://services9.arcgis.com/aKxrz4vDVjfUwBWJ/arcgis/rest/services/hospital_reporting_cases/FeatureServer/0/query?where=name%3D%27State%27&returnGeometry=false&spatialRel=esriSpatialRelIntersects&outFields=*&outStatistics=%5B%7B%22statisticType%22%3A%22sum%22%2C%22onStatisticField%22%3A%22num_hospitalized_new_admit%22%2C%22outStatisticFieldName%22%3A%22value%22%7D%5D&resultType=standard&sqlFormat=none&f=pgeojson&token='


  #Summary Table xpath->Tableau Download->Crosstab Button->CSV->Table->Download
  summary_xpath='//*[@id="tabZoneId4"]/div/div/div/span[2]/div/span/span/span[3]/div[2]/div'
  tab_dnld_xpath='//*[@id="download-ToolbarButton"]'
  crosstab_xpath='//*[@id="DownloadDialog-Dialog-Body-Id"]/div/fieldset/button[3]'
  csv_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[2]/div[2]/div/label[2]'
  census_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[2]'
  dnld_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[3]/button'
  bottom_xpath='//*[@id="view3515176980683563860_249615479279001290"]/div[1]/div[2]/canvas[2]'

  #file csvs
  csv_metric = "Table.csv"

  def getCSV(metric_xpath,csv_file):
     #wait = WebDriverWait(wd, 20)
     retry_wait_click_all(wd, 20, 'xpath', metric_xpath)
     #wait.until(EC.element_to_be_clickable((By.XPATH,metric_xpath))).click()
     retry_wait_click_all(wd, 20, 'css', ".tab-icon-download")
     #wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ".tab-icon-download"))).click()
     print("clicked download on tableau frame")
     retry_wait_click_all(wd, 20, 'xpath', "//button[text()='Crosstab']")
     #wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Crosstab']"))).click()
     print("chose crosstab option")
     retry_wait_click_all(wd, 20, 'css', "label[data-tb-test-id='crosstab-options-dialog-radio-csv-Label']")
     #wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "label[data-tb-test-id='crosstab-options-dialog-radio-csv-Label']"))).click()
     print("Chose CSV option")
     retry_wait_click_all(wd, 20, 'css', "div[title='Table']")
     #wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='Table']"))).click()
     print("Chose census file")
     retry_wait_click_all(wd, 20, 'xpath', "//button[text()='Download']")
     #wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Download']"))).click()
     print("Clicked dnld button")
     time.sleep(5)
     return pd.read_csv(csv_file,sep="\t",encoding="utf-16")

  #download view and convert to df
  wd=init_driver()
  wd.get(src)
  time.sleep(10)
  print(src)

  #Go Get the HI Race Table
  print("-" * 10)
  print("HI Race")
  df_race=getCSV(summary_xpath, csv_metric)
  df_race=df_race.fillna('0')
  #convert to integer
  tcols=list(df_race.columns)
  tcols[1]='Case Totals'
  tcols[5]='Death Totals'
  tcols[6]='Hospital Totals'
  df_race.columns=tcols
  #display(df_race)
  df_race['Death Totals'] = df_race['Death Totals'].replace('','4', regex=True)
  df_race['Case Totals'] = df_race['Case Totals'].replace(',','', regex=True).apply(pd.to_numeric)
  df_race['Death Totals'] = df_race['Death Totals'].replace(',','',regex=True).apply(pd.to_numeric)
  df_race['Hospital Totals'] = df_race['Hospital Totals'].replace(',','',regex=True).apply(pd.to_numeric)
  display(df_race)

  #Retrieve HI Total Cases
  req=requests.get(casesSrc)
  df_tot=pd.json_normalize(req.json()['features'])
  df_tot=df_tot.rename(columns=lambda x:re.sub('attributes.','',x))
  df_tots=df_tot.loc[:,['toDate_positiveAndPresumed']]
  df_tots['toDate_positiveAndPresumed'] = df_tots['toDate_positiveAndPresumed'].replace(',', '',regex=True).apply(pd.to_numeric)
  df_tots=df_tots.fillna('0')
  tcols=list(df_tots.columns)
  tcols[0]='Total Cases'
  df_tots.columns=tcols
  display(df_tots)

  #Retrieve HI Hospitalizations
  req=requests.get(hospSrc)
  df_hosp=pd.json_normalize(req.json()['features'])
  df_hosp=df_hosp.rename(columns=lambda x:re.sub('properties.','',x))
  df_hosps=df_hosp.loc[:,['value']]
  df_hosps=df_hosps.fillna('0')
  tcols=list(df_hosps.columns)
  tcols[0]="Hospital"
  df_hosps.columns=tcols
  df_hosps['Hospital'] = df_hosps['Hospital'].replace(',', '',regex=True).apply(pd.to_numeric)
  display(df_hosps)

  #Retrieve HI Deaths
  req=requests.get(deathSrc)
  #{"MMWRyear":2021,"MMWRweek":7,"week_ending":1613779200000,"county":"State","deaths":1,"ObjectId":420,"rolling_deaths":7.42857142857143,"previous_deaths":3,"cumulative_deaths":431,"fatality_rate":1.58607492456024}}]}
  df_death=pd.json_normalize(req.json()['features'])
  df_death=df_death.rename(columns=lambda x:re.sub('properties.','',x))
  df_deaths=df_death.loc[:,['cumulative_deaths']]
  df_deaths=df_deaths.fillna('0')
  tcols=list(df_deaths.columns)
  tcols[0]="Deaths"
  df_deaths.columns=tcols
  df_deaths['Deaths'] = df_deaths['Deaths'].replace(',', '',regex=True).apply(pd.to_numeric)
  display(df_deaths)

  wd.quit()

  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('K13',dataToWrite)

      # Write Data To Sheet
      writeTable(df_tots,'','J14',ws)
      writeTable(df_deaths,'','K14',ws)
      writeTable(df_hosps,'','L14',ws)
      writeTable(df_race,'HI Race','J17',ws)

# ID
#
def runID(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC

  def download_CSV(csv_name):
    retry_wait_click_all(wd, 60, 'css', ".tab-icon-download")
    #wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ".tab-icon-download"))).click()
    print('clicked download')
    
    retry_wait_click_all(wd, 60, 'xpath', "//button[text()='Crosstab']")
    #wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Crosstab']"))).click()
    print('clicked crosstab')

    retry_wait_click_all(wd, 60, 'css', "label[data-tb-test-id='crosstab-options-dialog-radio-csv-Label']")
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "label[data-tb-test-id='crosstab-options-dialog-radio-csv-Label']"))).click()
    print('clicked csv')

    retry_wait_click_all(wd, 60, 'css', "div[title='" + csv_name + "']")
    #wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='" + csv_name + "']"))).click()
    print('clicked file')

    retry_wait_click_all(wd, 60, 'xpath', "//button[text()='Download']")
    #wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Download']"))).click()
    print('clicked download')
    time.sleep(4)

  def colstr2int(df,col):
    df.loc[:,col] = df.loc[:,col].replace(',','', regex=True)
    df[col] = df[col].astype('int')

  def clean_tots(df,cats,dropval):
    df.iloc[:,0] = cats
    df.columns = ['Metric','Value']
    df = df.drop(dropval)
    colstr2int(df,'Value')
    df.sort_values(by='Value',ascending=False,inplace=True)
    return df

  def clean_demo(df,cols,type):
    df = df.iloc[:, cols].copy()
    df.columns = ['Category', type]
    colstr2int(df,type)
    return df

  def get_page(tabZone):
    wait.until(EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, "iframe[title='Data Visualization']")))
    retry_wait_click_all(wd, 60, 'xpath', "//*[@id='" + tabZone + "']/div/div/div/div/div")
    #wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='" + tabZone + "']/div/div/div/div/div"))).click()
    time.sleep(2)

  url = 'https://public.tableau.com/profile/idaho.division.of.public.health#!/vizhome/DPHIdahoCOVID-19Dashboard/Home'

  # Cases Downloads
  wd = init_driver()
  wd.get(url)
  wait = WebDriverWait(wd, 60)
  print('Click on Demo Page')
  get_page('tabZoneId193')
  wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div[tb-test-id='Asymptomatic']")))

  df_cases = {}

  sheets = ['CaseRace', 'CaseEth', 'State Total Cases Display (2)']
  for sheet in sheets:
    print('Download sheet %s' % sheet)
    download_CSV(sheet)
    if sheet == 'State Total Cases Display (2)':
      df_cases[sheet] = pd.read_csv(sheet + '.csv',sep="\t", encoding="utf-16",header=None)
    else:
      df_cases[sheet] = pd.read_csv(sheet + '.csv',sep="\t", encoding="utf-16")

  wd.quit()

  # Deaths Downloads
  wd = init_driver()
  wd.get(url)
  wait = WebDriverWait(wd, 60)
  print('Click on Demo Page')
  get_page('tabZoneId191')
  wait.until(EC.presence_of_all_elements_located((By.XPATH, "//span[text()='Demographics of COVID-19 Related Deaths']")))

  df_deaths = {}

  sheets = ['Race', 'Race Pending', 'Ethnicity', 'Ethnicity Pending','Total Deaths (2)']
  for sheet in sheets:
    print('Download sheet %s' % sheet)
    download_CSV(sheet)
    if (sheet == 'Race') or (sheet == 'Ethnicity'):
      df_deaths[sheet] = pd.read_csv(sheet + '.csv',sep="\t", encoding="utf-16")
    else:
      df_deaths[sheet] = pd.read_csv(sheet + '.csv',sep="\t", encoding="utf-16",header=None)

  wd.quit()

  # Calculations and Display

  df_cases['State Total Cases Display (2)'] = clean_tots(df_cases['State Total Cases Display (2)'],['Total Cases','New','Confirmed Cases','Probable Cases'],1)
  display(df_cases['State Total Cases Display (2)'])

  df_deaths['Total Deaths (2)'] = clean_tots(df_deaths['Total Deaths (2)'],['Confirmed Deaths','Total Deaths','Proabable Deaths',''],3)
  display(df_deaths['Total Deaths (2)'])

  vals = [['Known Cases - Race', df_cases['CaseRace'].iloc[0,2]],
          ['Known Cases - Ethnicity',df_cases['CaseEth'].iloc[0,2]],
          ['Pending Deaths - Race', df_deaths['Race Pending'].iloc[0,0]],
          ['Pending Deaths - Ethnicity', df_deaths['Ethnicity Pending'].iloc[0,0]]]
  df_vals = pd.DataFrame(vals,columns=['Metric','Value'])
  colstr2int(df_vals,'Value')
  display(df_vals)

  df_cases['CaseRace'] = clean_demo(df_cases['CaseRace'],[0,3],'Cases')
  display(df_cases['CaseRace'])

  df_cases['CaseEth'] = clean_demo(df_cases['CaseEth'],[0,3],'Cases')
  display(df_cases['CaseEth'])

  df_deaths['Race'] = clean_demo(df_deaths['Race'],[0,2],'Deaths')
  display(df_deaths['Race'])

  df_deaths['Ethnicity'] = clean_demo(df_deaths['Ethnicity'],[0,2],'Deaths')
  display(df_deaths['Ethnicity'])

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    ##ws.update('L14',dataToWrite)

    # Write Data To Sheet
    writeTable(df_cases['State Total Cases Display (2)'],'','K15',ws)
    writeTable(df_deaths['Total Deaths (2)'],'','M15',ws)
    writeTable(df_vals,'','O15',ws)
    writeTable(df_cases['CaseRace'],'','K21',ws)
    writeTable(df_cases['CaseEth'],'','K31',ws)
    writeTable(df_deaths['Race'],'','M21',ws)
    writeTable(df_deaths['Ethnicity'],'','M31',ws)

#MI
def runMI(ws,write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains

  def colstr2int(df,col):
    df.loc[:,col] = df.loc[:,col].replace(',','', regex=True)
    df[col] = df[col].astype('int')

  url = 'https://www.michigan.gov/coronavirus/0,9753,7-406-98163_98173---,00.html'
  req = requests.get(url)
  soup = BeautifulSoup(req.text, 'html5lib')
  tables = soup.find_all('table')

  rows = tables[0].find_all('tr')
  headers = rows[0].find_all('th') 
  headers = [ele.text.strip() for ele in headers]
  data = []
  for row in rows[1:len(rows)]:
    cols = row.find_all('td')
    cols = [ele.text.strip() for ele in cols]
    data.append([ele for ele in cols if ele]) # Get rid of empty values
  df_eth = pd.DataFrame(data,columns=headers)
  display(df_eth)

  url = 'https://app.powerbigov.us/view?r=eyJrIjoiNDY0ZGVlMDItMzUzNC00ZGE5LWFjYzQtNzliOGJkZWQ4YTgzIiwidCI6ImQ1ZmI3MDg3LTM3NzctNDJhZC05NjZhLTg5MmVmNDcyMjVkMSJ9'
  wd = init_driver()
  wd.get(url)
  wait = WebDriverWait(wd, 20)

  retry_wait_click_all(wd, 20, 'xpath', "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-group[3]/transform/div/div[2]/visual-container-modern[4]/transform/div/div[3]/div/visual-modern/div/button")
  #demo_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-group[3]/transform/div/div[2]/visual-container-modern[4]/transform/div/div[3]/div/visual-modern/div/button")))
  #demo_button.click()
  print('clicked Demo button')

  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[3]/transform/div")))
  ActionChains(wd).context_click(elements[0]).perform()
  #  time.sleep(10)
  retry_wait_click_all(wd, 20, 'css', "div[title='Show as a table']")
  #wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='Show as a table']"))).click()
  print('clicked Show Table')

  cats = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive ']")))
  for element in elements:
    print(element.text)
    cats.append(element.text)

  vals = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive tablixAlignRight ']")))
  for element in elements:
    vals.append(element.text)

  df_conf_cases = pd.DataFrame([cats[-7:],vals]).T
  df_conf_cases.columns = ['Category', 'Confirmed Cases']
  colstr2int(df_conf_cases,'Confirmed Cases')
  df_conf_cases.sort_values('Category',ascending=True,inplace=True)
  display(df_conf_cases)

  retry_wait_click_all(wd, 20, 'xpath', "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[3]/transform/div/div[3]/visual-container-pop-out-bar/div/div[1]/button")
  #back_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[3]/transform/div/div[3]/visual-container-pop-out-bar/div/div[1]/button")))
  #back_button.click()
  print('clicked Back button')

  retry_wait_click_all(wd, 20, 'xpath', "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-group/transform/div/div[2]/visual-container-modern[3]/transform/div/div[3]/div/visual-modern/div/div/div[2]/div/div[2]/div/div[1]/div/div/div[2]")

  #prob_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-group/transform/div/div[2]/visual-container-modern[3]/transform/div/div[3]/div/visual-modern/div/div/div[2]/div/div[2]/div/div[1]/div/div/div[2]")))
  #prob_button.click()
  print('clicked Probable case status')

  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[3]/transform/div")))
  ActionChains(wd).context_click(elements[0]).perform()
  #  time.sleep(10)
  retry_wait_click_all(wd, 20, 'css', "div[title='Show as a table']")
  #wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='Show as a table']"))).click()
  print('clicked Show Table')

  cats = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive ']")))
  for element in elements:
    cats.append(element.text)

  vals = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive tablixAlignRight ']")))
  for element in elements:
    vals.append(element.text)

  df_prob_cases = pd.DataFrame([cats[-7:],vals]).T
  df_prob_cases.columns = ['Category', 'Probable Cases']
  colstr2int(df_prob_cases,'Probable Cases')
  df_prob_cases.sort_values('Category',ascending=True,inplace=True)
  display(df_prob_cases)

  retry_wait_click_all(wd, 20, 'xpath', "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[3]/transform/div/div[3]/visual-container-pop-out-bar/div/div[1]/button")
  #back_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[3]/transform/div/div[3]/visual-container-pop-out-bar/div/div[1]/button")))
  #back_button.click()
  print('clicked Back button')

  retry_wait_click_all(wd, 20, 'xpath', "//*[@aria-label=' Bookmark Button Click to view demographic characteristics of deaths.. Click here to follow link']")
  #deaths_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@aria-label=' Bookmark Button Click to view demographic characteristics of deaths.. Click here to follow link']")))
  #deaths_button.click()
  print('clicked Deaths button')
  time.sleep(5)

  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[3]/transform/div")))
  ActionChains(wd).context_click(elements[0]).perform()
  #  time.sleep(10)
  retry_wait_click_all(wd, 20, 'css', "div[title='Show as a table']")
  #wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='Show as a table']"))).click()
  print('clicked Show Table')

  cats = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive ']")))
  for element in elements:
    cats.append(element.text)

  vals = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive tablixAlignRight ']")))
  for element in elements:
    vals.append(element.text)

  df_conf_deaths = pd.DataFrame([cats[-7:],vals]).T
  df_conf_deaths.columns = ['Category', 'Confirmed Deaths']
  colstr2int(df_conf_deaths,'Confirmed Deaths')
  df_conf_deaths.sort_values('Category',ascending=True,inplace=True)
  display(df_conf_deaths)

  retry_wait_click_all(wd, 20, 'xpath', "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[19]/transform/div/div[3]/visual-container-pop-out-bar/div/div[1]/button")
  #back_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[19]/transform/div/div[3]/visual-container-pop-out-bar/div/div[1]/button")))
  #back_button.click()
  print('clicked Back button')

  retry_wait_click_all(wd, 20, 'xpath', "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-group/transform/div/div[2]/visual-container-modern[3]/transform/div/div[3]/div/visual-modern/div/div/div[2]/div/div[2]/div/div[1]/div/div/div[2]")
  #prob_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-group/transform/div/div[2]/visual-container-modern[3]/transform/div/div[3]/div/visual-modern/div/div/div[2]/div/div[2]/div/div[1]/div/div/div[2]")))
  #prob_button.click()
  print('clicked Probable case status')

  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[3]/transform/div")))
  ActionChains(wd).context_click(elements[0]).perform()
  #  time.sleep(10)
  retry_wait_click_all(wd, 20, 'css', "div[title='Show as a table']")
  #wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='Show as a table']"))).click()
  print('clicked Show Table')

  cats = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive ']")))
  for element in elements:
    cats.append(element.text)

  vals = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive tablixAlignRight ']")))
  for element in elements:
    vals.append(element.text)

  df_prob_deaths = pd.DataFrame([cats[-7:],vals]).T
  df_prob_deaths.columns = ['Category', 'Probable Deaths']
  colstr2int(df_prob_deaths,'Probable Deaths')
  df_prob_deaths.sort_values('Category',ascending=True,inplace=True)
  display(df_prob_deaths)

  df = df_conf_cases.merge(df_prob_cases,how='left',on='Category')
  df = df.merge(df_conf_deaths,how='left',on='Category')
  df = df.merge(df_prob_deaths,how='left',on='Category')
  display(df)

  wd.quit()

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('M19',dataToWrite)

    # Write Data To Sheet
    writeTable(df,'','L20',ws)
    writeTable(df_eth,'','L20',ws)

def runIL(ws,write):

  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC

  def getvals(html,title):
    cats = []
    vals = []
    for e in html:
      tspan = e.find_all('tspan')
      cats.append(tspan[0].get_text())
      vals.append(tspan[1].get_text())
    df = pd.DataFrame([cats[:-3],vals[:-3]]).T
    df.columns = ['Category',title]
    return df

  url = 'http://www.dph.illinois.gov/covid19/covid19-statistics'

  #Open Webpage
  wd = init_driver()
  #wait = WebDriverWait(wd, 20)
  wd.get(url)
  time.sleep(10)

  # Cases
  soup = BeautifulSoup(wd.page_source, 'html.parser')
  prob_deaths = soup.find('h3',{"id": "covid19probabledeaths"})
  prob_deaths = int(prob_deaths.text.replace(',',''))
  div_race = soup.find('div',{"id": "pieRace"})
  cases_race = div_race.find_all_next('text',{"class": "slicetext"})

  # Deaths
  retry_wait_click_all(wd, 20, 'xpath', "//*[@id='liRaceChartDeaths']/a")
  #wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='liRaceChartDeaths']/a"))).click()
  time.sleep(10)
  soup = BeautifulSoup(wd.page_source, 'html.parser')
  div_race = soup.find('div',{"id": "pieRace"})
  deaths_race = div_race.find_all_next('text',{"class": "slicetext"})

  # Tests
  retry_wait_click_all(wd, 20, 'xpath', "//*[@id='racePagination']/li[2]/a")
  #wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='racePagination']/li[2]/a"))).click()
  time.sleep(10)
  soup = BeautifulSoup(wd.page_source, 'html.parser')
  div_race = soup.find('div',{"id": "pieRace"})
  tests_race = div_race.find_all_next('text',{"class": "slicetext"})

  df_cases = getvals(cases_race,'Cases')
  df_deaths = getvals(deaths_race,'Deaths')
  df_tests = getvals(tests_race,'Tests')

  df = df_cases.merge(df_deaths,how='left',on='Category')
  df = df.merge(df_tests,how='left',on='Category')
  cols = df.columns[1:4]
  df.loc[:,cols] = df.loc[:,cols].replace(',','', regex=True)
  df[cols] = df[cols].astype('int')
  df['Category'] = df['Category'].replace('\*','', regex=True)
  df = df.sort_values(by='Category').reset_index(drop=True)
  df.loc[len(df.index)] = ['Probable',0,prob_deaths,0]
  display(df)

  wd.quit()

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('G33',dataToWrite)

    # Write Data To Sheet
    writeTable(df,'','F34',ws)

# IN
#import pandas as pd
#import requests

def runIN(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains

  url = 'https://hub.mph.in.gov/dataset/62ddcb15-bbe8-477b-bb2e-175ee5af8629/resource/2538d7f1-391b-4733-90b3-9e95cd5f3ea6/download/covid_report_demographics.xlsx'
  df_IN_casesRace = pd.read_excel(url, sheet_name='Race', skiprows=0, engine='openpyxl')
  print("Cases by Race")
  display(df_IN_casesRace)
  df_IN_casesEthnicity = pd.read_excel(url, sheet_name='Ethnicity', skiprows=0, engine='openpyxl')
  print("\nCases by Ethnicity")
  display(df_IN_casesEthnicity)

  url = 'https://www.coronavirus.in.gov/2393.htm'

  wd = init_driver()
  wd.get(url)
  wd.maximize_window()
  wait = WebDriverWait(wd, 20)
  height = wd.execute_script("return Math.max( document.body.scrollHeight, document.body.offsetHeight, document.documentElement.clientHeight, document.documentElement.scrollHeight, document.documentElement.offsetHeight )")
  wd.set_window_size(1920, height)
  wait.until(EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, "iframe[src='./map/test.htm']")))
  wd.save_screenshot("in_probable.png");
  soup = BeautifulSoup(wd.page_source, 'html.parser')
  p_tag = soup.find('p',attrs={'class':'h5'})
  h2_tag = p_tag.findNext('h2')
  df_prob = pd.DataFrame([['Probable',h2_tag.text]],columns=['Category','Deaths'])
  display(df_prob)
  wd.quit()

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('G21',dataToWrite)

    # Write Data To Sheet
    writeTable(df_IN_casesRace,'Cases by Race','H22',ws)
    writeTable(df_IN_casesEthnicity,'Cases by Ethnicity','H29',ws)
    writeTable(df_prob,'Cases by Ethnicity','H29',ws)

# KS
def runKS(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains 


  url='https://public.tableau.com/views/COVID-19TableauVersion2/COVID-19Overview?:embed=y&:showVizHome=no&:host_url=https%3A%2F%2Fpublic.tableau.com%2F&:embed_code_version=3&:tabs=no&:toolbar=yes&:animate_transition=yes&:display_static_image=no&:display_spinner=no&:display_overlay=yes&:display_count=yes&:language=en&publish=yes&:loadOrderID=0'
  case_dems_xpath='//*[@id="tabZoneId434"]/div/div/div/div/div/div'
  death_dems_xpath='//*[@id="tabZoneId439"]/div/div/div/div/div/div'
  hosp_dems_xpath='//*[@id="tabZoneId440"]/div/div/div/div/div/div'
  test_dems_xpath='//*[@id="tabZoneId437"]/div/div/div/div/div/div'
  dnld_xpath='//*[@id="download-ToolbarButton"]/span[1]'
  pdf_xpath='//*[@id="DownloadDialog-Dialog-Body-Id"]/div/fieldset/button[4]'
  download_xpath='//*[@id="export-pdf-dialog-Dialog-Body-Id"]/div/div[4]/button'

  Race_Eth=['Total', 'White', 'Black', 'AIAN', 'Asian', 'Other', 'Race Missing', 'Hispanic', 'Not Hispanic', 'Eth Missing']
 
  def getPDF(metric_xpath,pdf_file,interval):
     retry_wait_click_all(wd, interval, 'xpath',metric_xpath)
     #wait.until(EC.element_to_be_clickable((By.XPATH,metric_xpath))).click()
     print("Metric - case, hosp or death dems")
     retry_wait_click_all(wd, interval,  'xpath',dnld_xpath)
     #wait.until(EC.element_to_be_clickable((By.XPATH, dnld_xpath))).click()
     print("clicked download on tableau frame")
     retry_wait_click_all(wd, interval,  'xpath',pdf_xpath)
     #wait.until(EC.element_to_be_clickable((By.XPATH, pdf_xpath))).click()
     print("chose PDF option")
     retry_wait_click_all(wd, interval,  'xpath',download_xpath)
     #wait.until(EC.element_to_be_clickable((By.XPATH, download_xpath))).click()
     print("Chose Download as is option")
     time.sleep(10)
     tables=tabula.read_pdf(pdf_file, pages="all", multiple_tables=True, lattice=False, encoding='utf-8', guess=False)

     return tables

  ########
  #Tests
  #######
  #initialize the driver
  wd=init_driver()
  wd.get(url)
  print("Getting Test Demographics\n")

  test_tables=getPDF(test_dems_xpath, 'Testing Rates.pdf', 60)
  test_info=test_tables[0]
  #display(test_info)
  test_info.columns=['Race/Ethnicity', 'TestNums', 'Drop1', 'EthNums']
  test_info=test_info.drop(['Drop1'], axis=1)
  #test_info0=test_info
  print("deleted 1 columns")
 
  #Code with incorrect totals - here for reference on the rows to cut
  #test_info1=test_info.iloc[14:-102].reset_index(drop=True) 
  #print("remove big span 14:-102")
  #display(test_info1)

  #Code with corrected totals
  test_info1=test_info.iloc[4:-101].reset_index(drop=True) 
  #display(test_info1)
  test_info1=test_info1.drop(test_info1.index[1:15]).reset_index(drop=True)
  print("remove big span of rows - but keep in totals row")
  #display(test_info1)

  #Old totals code
  #test_info1=test_info1.drop(test_info1.index[1:4]).reset_index(drop=True)
  #print("removed 3 rows")
  #display(test_info1)

  #Remove commas & Split the Test Column Between Numbers and Ethnicity Labels by space
  
  print("split for total search")
  #test_info1[['tdrop1', 'tdrop2', 'tdrop3', 'tdrop4', 'tdrop5', 'tdrop6', 'tdrop7']]=test_info1['Race/Ethnicity'].str.split(' ', expand=True)
  test_info1[['tdrop1', 'tdrop2', 'tdrop3', 'tdrop4', 'tdrop5']]=test_info1['Race/Ethnicity'].str.split(' ', expand=True)
  #display(test_info1)

  print("split for Race Test Numbers")
  test_info1[['Tests', 'Drop1', 'EthName', 'Drop2', 'Drop3','Drop4']]=test_info1['TestNums'].str.split(' ',expand=True)
  print("about to assign totals values")
  #assign first value in tests to the first value in tdrop1 for totals
  test_info1.loc[0, 'Tests'] = test_info1.loc[0, 'tdrop1']

  print("split for Ethnicity Test Numbers")
  test_info1[['Test_Eth', 'EDrop1', 'EDrop2']]=test_info1['EthNums'].str.split(' ', expand=True)
  #Take out what we don't need
  test_info1=test_info1.drop(['tdrop1', 'tdrop2', 'tdrop3', 'tdrop4', 'tdrop5'], axis=1)
  test_info1=test_info1.drop(['TestNums', 'EthNums', 'Drop1', 'Drop2', 'Drop3', 'Drop4', 'EDrop1', 'EDrop2'], axis=1)
  #display (test_info1)

  print("finished split- now drop columns getting ready for dataframe merge")
  test_eth=test_info1
  test_eth=test_eth.drop(['Race/Ethnicity', 'Tests'], axis=1) #drop these columns for ethnicity
  test_eth.columns=['Race/Ethnicity', 'Tests'] #rename remaining columns for ethnicity
  #display(test_eth)

  test_info1=test_info1.drop(['EthName', 'Test_Eth'], axis=1)
  #display(test_info1)

  #reorder Asian and AIAN values to match Cases/Hosp/Deaths by storing Asian values in an empty row of test_eth
  test_eth.iloc[0]=test_info1.iloc[3]

  #put AIAN values where Asian was
  test_info1.iloc[3]=test_info1.iloc[4] 
  test_info1.iloc[4]=test_eth.iloc[0]
  #display(test_info1)

  #remove 4 empty rows of test_eth
  test_eth=test_eth.iloc[4:].reset_index(drop=True)
  #display(test_eth)

  
  print("Append dataframes")
  test_info1=test_info1.append(test_eth, ignore_index=True)
  #display(test_info1)
  test_final=test_info1
  test_final['Tests']=test_final['Tests'].str.replace(r",","").astype(int)

  test_final['Race/Ethnicity']=Race_Eth
  display(test_final)

  wd.quit()
  

  ########
  #Cases
  #######
  #initialize the driver
  wd=init_driver()
  wd.get(url)
  print(url)



  #Cases
  print("Getting Case Demographics")
  case_tables=getPDF(case_dems_xpath, 'Case Characteristics.pdf', 60)
  case_info=case_tables[0]
  case_info.columns=['Drop 0', 'Race/Ethnicity', 'Cases', 'Drop 1', 'Drop 2']
  case_info0=case_info.drop(['Drop 0', 'Drop 1', 'Drop 2'], axis=1)
  #print("deleted 2 columns")

  case_info1=case_info0.iloc[23:-24].reset_index(drop=True) #from 24 to 11
  #print("remove big span 23:-23")
  case_info1=case_info1.drop(case_info0.index[7:10]).reset_index(drop=True)
  #print("removed 3 rows")

  #Going after Total number of Cases
  case_info0=case_info0.iloc[10:-48].reset_index(drop=True)
  case_info0_label=case_info0
  #print("about to split Race/Ethnicity Cases by space")
  case_info0_label[['Word1','Word2','Word3','Word4']]=case_info0_label['Race/Ethnicity'].str.split(' ',expand=True)
  print("finished split")

  case_info0_label.columns=['Total', 'Drop0', 'Cases', 'Word2', 'Word3', 'Word4']
  case_info0_label=case_info0_label.drop(['Drop0', 'Word2', 'Word3', 'Word4'], axis=1)
  #display(case_info0_label)

  #Reassemble Dataframe with Totals on top
  case_final=case_info1
  case_final.iloc[0]=case_info0_label.iloc[1]
  #display(case_final)
  case_final['Cases']=case_final['Cases'].str.replace(r",","").astype(float)
  case_final['Race/Ethnicity']=Race_Eth
  display(case_final)

  wd.quit()


  ########
  #Hospitalizations
  #######
  #initialize the driver
  wd=init_driver()
  wd.get(url)

  #Hospitalizations
  print("Getting Hospitalization Demographics")
  hosp_tables=getPDF(hosp_dems_xpath, 'Hospital Summary.pdf', 60)
  hosp_tables[0].fillna('0')
  hosp_info=hosp_tables[0]
  hosp_info.columns=['Race/Ethnicity', 'Drop 0', 'Drop 1', 'Drop 2', 'Drop 3']
  hosp_info=hosp_info.drop(['Drop 1', 'Drop 2','Drop 3'], axis=1)
  #print("dropped 3 columns")

  hosp_info0=hosp_info.iloc[8:-20].reset_index(drop=True) 
  hosp_prefinal=hosp_info0.drop(hosp_info0.index[7:14]).reset_index(drop=True)
  print("reset indices")
  hosp_prefinal.columns=['Race/Ethnicity', 'Drop 0']
  #print("adjust columns")

  #remove commas from the mega Race/Ethnicity column
  hosp_prefinal['Race/Ethnicity']=hosp_prefinal['Race/Ethnicity'].str.replace(r",","")
  #print("remove commas in Race/Ethnicity")

  #explode out the Race/Ethnicity column - split by space
  hosp_final_label=hosp_prefinal
  hosp_final_label[['Word1','Word2','Word3','Word4','Word5', 'Word6', 'Word7', 'Word8', 'Word9']]=hosp_final_label['Race/Ethnicity'].str.split(' ',expand=True)
  print("Exploding columns, removing columns and rearranging")
  
  #Try to convert the data frame to numeric
  hosp_final_label2=hosp_final_label.apply(pd.to_numeric, errors='coerce')
  hosp_final_label2=hosp_final_label2.fillna('0')
  #print("trying to convert df to numeric")
  
  #See what got converted
  #display(hosp_final_label2['Word2'].apply(type))

  hosp_final_label2.columns=['Race/Ethnicity', 'Hospitalizations', 'Word1','Word2','Word3','Word4','Word5', 'Word6', 'Word7', 'Word8', 'Word9']
  #print("Renamed Hosp")

  #Iterate over the hosp data frame because tabula is not clean and adds numbers to the race/ethnicity column  
  print("starting iterating")
  for i in range(len(hosp_final_label2)):
      if isinstance(hosp_final_label2.loc[i, 'Word1'], float):
                #print("Label had a number Word1")
                hosp_final_label2.loc[i, 'Hospitalizations'] = hosp_final_label2.loc[i, 'Word1']
      elif isinstance(hosp_final_label2.loc[i, 'Word2'], float):
                #print("Label had a number Word2")
                hosp_final_label2.loc[i, 'Hospitalizations'] = hosp_final_label2.loc[i, 'Word2']
      elif isinstance(hosp_final_label2.loc[i, 'Word3'], float):
                #print("Label had a number Word3")
                hosp_final_label2.loc[i, 'Hospitalizations'] = hosp_final_label2.loc[i, 'Word3']
      elif isinstance(hosp_final_label2.loc[i, 'Word4'], float):
                #print("Label had a number Word4")
                hosp_final_label2.loc[i, 'Hospitalizations'] = hosp_final_label2.loc[i, 'Word4']
      elif isinstance(hosp_final_label2.loc[i, 'Word5'], float):
                #print("Label had a number Word5")
                hosp_final_label2.loc[i, 'Hospitalizations'] = hosp_final_label2.loc[i, 'Word5']
      elif isinstance(hosp_final_label2.loc[i, 'Word6'], float):
                #print("Label had a number Word6")
                hosp_final_label2.loc[i, 'Hospitalizations'] = hosp_final_label2.loc[i, 'Word6']
      elif isinstance(hosp_final_label2.loc[i, 'Word7'], float):
                #print("Label had a number Word7")
                hosp_final_label2.loc[i, 'Hospitalizations'] = hosp_final_label2.loc[i, 'Word7']
      elif isinstance(hosp_final_label2.loc[i, 'Word8'], float):
                #print("Label had a number Word8")
                hosp_final_label2.loc[i, 'Hospitalizations'] = hosp_final_label2.loc[i, 'Word8']
      elif isinstance(hosp_final_label2.loc[i, 'Word9'], float):
                #print("Label had a number Word9")
                hosp_final_label2.loc[i, 'Hospitalizations'] = hosp_final_label2.loc[i, 'Word9']

  print("stopping iterating")

  #Drop extra columns & move Totals to the top
  hosp_final_label2=hosp_final_label2.drop(['Word1', 'Word2', 'Word3', 'Word4', 'Word5', 'Word6', 'Word7', 'Word8', 'Word9'], axis=1) 
  hosp_final_label2.iloc[0]=hosp_final_label2.iloc[10]
  hosp_final=hosp_final_label2.drop(hosp_final_label2.index[10]).reset_index(drop=True)
  hosp_final['Race/Ethnicity']=Race_Eth
  display(hosp_final)

  wd.quit()


  ########
  #Deaths
  #######
  #initialize the driver
  wd=init_driver()
  wd.get(url)
  print("Getting Death Demographics")

  #Deaths
  death_tables=getPDF(death_dems_xpath, 'Death Summary.pdf', 60)
  death_tables[0].fillna('0')
  death_info=death_tables[0]
  #display(death_info)

  #print("about to drop deaths extra columns")
  death_info.columns=['Race/Ethnicity', 'Deaths', 'Drop 1', 'Drop 2']
  death_info1=death_info.drop(['Drop 1', 'Drop 2'], axis=1)

  #Going after Total number of Deaths
  death_tot0=death_info1.iloc[5:-24].reset_index(drop=True)

  #print("dropping extra rows")
  death_final=death_tot0.drop(death_tot0.index[1:18]).reset_index(drop=True)
  death_final=death_final.drop(death_final.index[7:9]).reset_index(drop=True)
  #display(death_final)

  death_final_label=death_final
  #Race/Ethnicity label sometimes has numbers in it - so split the strings by spaces
  death_final_label['Race/Ethnicity']=death_final_label['Race/Ethnicity'].str.replace(r",","")
  death_final_label[['Word1','Word2','Word3','Word4','Word5']]=death_final_label['Race/Ethnicity'].str.split(' ',expand=True)

  #split out any extra numbers from Deaths column
  death_final_label['Deaths']=death_final_label['Deaths'].str.replace(r",","")
  death_final_label[['Deaths','Extra']]=death_final_label['Deaths'].str.split(' ',expand=True)

  #Try to convert the dataframe to numeric
  death_final_label2=death_final_label.apply(pd.to_numeric, errors='coerce').fillna('0')
  #display(death_final_label2)

  #See what got converted
  #display(death_final_label2['Word2'].apply(type))

  #Iterate over the deaths data frame because tabula is not clean and adds a number to the race/ethnicity column  
  print("starting iterating")
  for i in range(len(death_final_label2)):
      if isinstance(death_final_label2.loc[i, 'Word1'], float):
                #print("Label had a number Word1")
                death_final_label2.loc[i, 'Deaths'] = death_final_label2.loc[i, 'Word1']
      elif isinstance(death_final_label2.loc[i, 'Word2'], float):
                #print("Label had a number Word2")
                death_final_label2.loc[i, 'Deaths'] = death_final_label2.loc[i, 'Word2']  
      elif isinstance(death_final_label2.loc[i, 'Word3'], float):
                #print("Label had a number Word3")
                death_final_label2.loc[i, 'Deaths'] = death_final_label2.loc[i, 'Word3']
      elif isinstance(death_final_label2.loc[i, 'Word4'], float):
                #print("Label had a number Word4")
                death_final_label2.loc[i, 'Deaths'] = death_final_label2.loc[i, 'Word4']
      elif isinstance(death_final_label2.loc[i, 'Word5'], float):
                #print("Label had a number Word5")
                death_final_label2.loc[i, 'Deaths'] = death_final_label2.loc[i, 'Word5']

  print("stopping iterating")
  death_final_label2=death_final_label2.drop(['Extra', 'Word1', 'Word2', 'Word3', 'Word4', 'Word5'], axis=1)
  death_final_label2['Race/Ethnicity']=Race_Eth
  display(death_final_label2)

  wd.quit()


  #Combine 3 dataframes to get a total dataframe
  print("combining data frames")
  df_tot=pd.concat([case_final,hosp_final,death_final_label2, test_final],axis=1)
  df_tot.columns=['Race/Ethnicity', 'Cases', 'Drop0', 'Hospitalizations', 'Drop1', 'Deaths', 'Drop2', 'Tests']
  df_tot=df_tot.drop(['Drop0', 'Drop1', 'Drop2'], axis=1)
  display(df_tot)

  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('G21',dataToWrite)

      # Write Data To Sheet
      #writeTable(case_final,'Cases by Race & Ethnicity','H22',ws)
      #writeTable(hosp_final,'Hosp by Race & Ethnicity', 'H32', ws)
      #writeTable(death_final_label2,'Deaths by Race & Ethnicity','H42',ws)
      writeTable(df_tot,'KS Demographics','K22',ws)


# KY
#from io import StringIO, BytesIO
#from bs4 import BeautifulSoup
#import pandas as pd
#import tabula
#import PyPDF2
#import re
#import requests

import ssl
ssl._create_default_https_context = ssl._create_unverified_context # To fix bad certificate issue

def runKY(ws, write):
  url = 'https://chfs.ky.gov/agencies/dph/covid19/COVID19DailyReport.pdf'

  req = requests.get(url, verify=False)
  pdf = BytesIO(req.content)
  first_page = PyPDF2.PdfFileReader(pdf).getPage(0).extractText()

  # Known race & ethnicity for cases
  match = re.search(r'Report(\d+\w+\d+)Daily',first_page)
  print('\nDate of KY Report:',match.group(1))
  match = re.search(r'Race known for[ ]*(\d+.\d)%[ ]*of cases and[ ]*(\d+.\d)%[ ]*of deaths',first_page)
  knownRaceCases = match.group(1)
  knownRaceDeaths = match.group(2)
  print('\n% known race for cases = ' + knownRaceCases + '%','\n% known race for deaths = ' + knownRaceDeaths + '%')
  match = re.search(r'Ethnicity known for[ ]*(\d+.\d)%[ ]*of cases and[ ]*(\d+.\d)%[ ]*of deaths',first_page)
  knownEthCases = match.group(1)
  knownEthDeaths = match.group(2)
  print('% known ethnicity for cases = ' + knownEthCases + '%','\n% known ethnicity for deaths = ' + knownEthDeaths + '%\n')

  # Totals
  tables = tabula.read_pdf(url,pages=1,multiple_tables = True)
  totals = tables[0].drop(['Measure'],axis=1)
  totals = totals.iloc[0:2]

  #Remove "," from Case numbers
  totals['Total']=totals['Total'].str.replace(r",","")
  totals['Confirmed']=totals['Confirmed'].str.replace(r",","")
  totals['Probable']=totals['Probable'].str.replace(r",","")
  #Convert Str to Numeric
  totals['Total']=pd.to_numeric(totals['Total'])
  totals['Confirmed']=pd.to_numeric(totals['Confirmed'])
  totals['Probable']=pd.to_numeric(totals['Probable'])
  totals=totals.fillna('')
  print("\nCase and Death Totals")
  display(totals)

  #Race Tables
  race = tables[2]
  race = race.loc[(race.index%2==1) & (race.index<13),['Cases','Race','Deaths']]
  race = race.reset_index(drop=True)
  #Remove "," from Case Numbers
  race['Cases']=race['Cases'].str.replace(r",","")
  race['Deaths']=race['Deaths'].str.replace(r",","")
  #Convert Str to Numeric
  race['Cases']=pd.to_numeric(race['Cases'])
  race['Deaths']=pd.to_numeric(race['Deaths'])
  print('\nRace Table')
  display(race)

  #Ethnicity Tables
  eth = tables[2]
  eth = eth.loc[(eth.index%2==1) & (eth.index>13),['Cases','Race','Deaths']]
  eth.rename(columns={'Race':'Ethnicity'}, inplace=True)
  eth = eth.reset_index(drop=True)
  #Remove "," from Case Numbers
  eth['Cases']=eth['Cases'].str.replace(r",","")
  eth['Deaths']=eth['Deaths'].str.replace(r",","")
  #Convert Str to Numeric
  eth['Cases']=pd.to_numeric(eth['Cases'])
  eth['Deaths']=pd.to_numeric(eth['Deaths'])
  print('\nEthnicity Table')
  display(eth)


  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('L37',dataToWrite)

    dataToWrite = [[knownRaceCases, 'Known %', knownRaceDeaths]]
    #ws.update('J26',dataToWrite)

    dataToWrite = [[knownEthCases, 'Known %', knownEthDeaths]]
    #ws.update('J31',dataToWrite)


    # Write Data To Sheet
    writeTable(race,'Race Table','J18',ws)
    writeTable(eth,'Ethnicity Table','J27',ws)
    writeTable(totals,'Totals Table','J32',ws)

# LA
def runLA(ws,write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC

  url = 'https://services5.arcgis.com/O5K6bb5dZVZcTo5M/ArcGIS/rest/services/Case_Deaths_Race_Region_new/FeatureServer/0/query?where=1%3D1&outFields=LDH_region%2C+Race%2C+Deaths%2C+Cases&returnGeometry=false&f=json'

  req = requests.get(url)
  dict = req.json()['features']

  for i in range(0,len(dict)):
    cols = list(dict[i]['attributes'].keys())
    vals = list(dict[i]['attributes'].values())
    df_tmp = pd.DataFrame([vals],columns=cols)
    if i == 0:
      df_demo = df_tmp
    else:
      df_demo = df_demo.append(df_tmp)


  df_demo = df_demo.sort_values(by=['LDH_Region','Race'])

  display(df_demo)

  url = 'https://public.tableau.com/views/COVID19demog/DataonCOVIN-19RelatedDeathsToDate?:embed=y&:showVizHome=no&:host_url=https%3A%2F%2Fpublic.tableau.com%2F&:embed_code_version=3&:tabs=no&:toolbar=no&:showAppBanner=false&:display_spinner=no&:loadOrderID=0'
  wd = init_driver()
  wd.get(url)

  #wait = WebDriverWait(wd, 20)
  retry_wait_click_all(wd, 20, 'css', ".tab-icon-download")
  #wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ".tab-icon-download"))).click()
  print('Clicked download')
  retry_wait_click_all(wd, 20, 'xpath', "//button[text()='PDF']")
  #wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='PDF']"))).click()
  print('Clicked PDF')
  retry_wait_click_all(wd, 20, 'xpath', "//button[text()='Download']")
  #wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Download']"))).click()
  print('Clicked download')
  time.sleep(5)

  wd.quit()
  

  tables = tabula.read_pdf('Data on COVIN-19 Related Deaths To Date.pdf',pages=1,multiple_tables=True,pandas_options={'header': None})
  df_eth = tables[1]
  df_eth.columns = ['Category','Deaths']
  display(df_eth)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('H26',dataToWrite)

    # Write Data To Sheet
    writeTable(df_demo,'','J27',ws)
    writeTable(df_eth,'','J27',ws)

# MA

#from io import StringIO, BytesIO
#from bs4 import BeautifulSoup
#import pandas as pd
#import re
#import requests
#import zipfile
def runMA(ws, write):
  url = 'https://www.mass.gov/info-details/covid-19-response-reporting#covid-19-interactive-data-dashboard-'
  req = requests.get(url)
  soup = BeautifulSoup(req.text, 'html.parser')
  a = soup.find('a', string=re.compile("COVID-19 Raw Data"))
  link = "https://www.mass.gov{}".format(a['href'])
  res = requests.get(link)

  #Cases -
  df_cases = pd.read_excel(BytesIO(res.content), sheet_name="Cases (Report Date)", engine='openpyxl')
  maxdate = df_cases['Date'].max()
  print("Case Totals")
  print(maxdate,'\n')
  df_cases = df_cases[df_cases['Date'] == maxdate]
  display(df_cases)

  #Deaths -
  df_deaths = pd.read_excel(BytesIO(res.content), sheet_name="DeathsReported (Report Date)", engine='openpyxl')
  maxdate = df_deaths['Date'].max()
  print("\nDeath Totals")
  print(maxdate,'\n')
  df_deaths = df_deaths[df_deaths['Date'] == maxdate]
  display(df_deaths)

  #Demographics
  df_dems = pd.read_excel(BytesIO(res.content), sheet_name='RaceEthnicityLast2Weeks', engine='openpyxl')
  maxdate = df_dems['Date'].max()
  print("\nMA Demographics")
  print(maxdate,'\n')
  df_dems = df_dems[df_dems['Date'] == maxdate]
  display(df_dems)

  #Demographic Totals for DC
  print ('\nMA Race and Ethnicity Totals')
  print(maxdate,'\n')
  df_dem_tot = df_dems.groupby(['Date'])[['All Cases','Ever Hospitaltized','Deaths']].sum()
  display(df_dem_tot)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('F26',dataToWrite)

    # Write Data To Sheet
    writeTable(df_cases,'Case Totals','G23',ws)
    writeTable(df_deaths,'Death Totals','G27',ws)
    writeTable(df_dems,'MA Demographics','G31',ws)
    writeTable(df_dem_tot,'MA Race and Ethnicity Totals','G42',ws)

# MD
def runMD(ws,write):
    
  url = 'https://coronavirus.maryland.gov/'
  wd = init_driver()
  wait = WebDriverWait(wd, 20)
  wd.maximize_window()
  wd.get(url)
  time.sleep(5)
  height = wd.execute_script("return Math.max( document.body.scrollHeight, document.body.offsetHeight, document.documentElement.clientHeight, document.documentElement.scrollHeight, document.documentElement.offsetHeight )")
  wd.set_window_size(1920, height)
  wd.execute_script("window.scrollTo(0, document.body.scrollHeight);")
#  time.sleep(5)
  wait.until(EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, "iframe[src='https://state-of-maryland.github.io/COVID19_Cases_DashboardBlackBox/CasesBlackboxStats2.html']")))
  wd.save_screenshot("md_init.png");
  print('switched')
  soup = BeautifulSoup(wd.page_source, 'html5lib')
  for el in soup(text='By Race and Ethnicity'):
    table = el.parent.parent.findNext('table')
  # Clean data, move first row to column names
  df = pd.read_html(str(table))[0]
  df = df.fillna('')
  df.columns = df.iloc[0]
  df_demo = df.drop([0])
  if len(df_demo['Cases']) == 7:
      df_demo = df_demo.drop([7])
  # Convert 1st column (cases) to int (other columns fine as strings)
  df_demo['Cases']=df_demo['Cases'].astype('int')
  display(df_demo)

  wd.switch_to.default_content()
  wait.until(EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, "iframe[src='https://state-of-maryland.github.io/COVID19_Cases_DashboardBlackBox/StatisticsSummary.html']")))
  soup = BeautifulSoup(wd.page_source, 'html5lib')
  p_text = soup.find('span',{"id": "NumberofConfirmedCases"}).parent.text
  df_totals = pd.read_csv(StringIO(p_text), sep=':', header=None)
  df_totals.columns = ["Metric","Value"]
  display(df_totals)
  wd.quit()


  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('F33',dataToWrite)

    # Write Data To Sheet
    writeTable(df_demo,'','A33',ws)
    writeTable(df_totals,'','G29',ws)

# ME #Race only
def runME(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains

  #URLs
  raceethsrc='https://analytics.maine.gov/t/CDCExternal/views/covid-19-maine-cdc-dashboard/6_COVID-19databyrace?%3Aembed=y&amp%3B%3AshowVizHome=no&amp%3B%3Ahost_url=https%3A%2F%2Fanalytics.maine.gov%2F&amp%3B%3Aembed_code_version=3&amp%3B%3Atabs=no&amp%3B%3Atoolbar=yes&amp%3B%3AshowAppBanner=false&amp%3B%3Adisplay_spinner=no&amp%3B%3AloadOrderID=0'
  #xpaths
  #Race & Ethnicity Toggle
  race_toggle_xpath='//*[@id="tableau_base_widget_ParameterControl_0"]/div/div[2]/span/div[2]/span/svg'

  #Select hospitalizations
  #race_hosp_xpath='//*[@id="tabZoneId8"]/div/div/div/div[1]/div[5]/div[1]/canvas'
  race_hosp_xpath='//*[@class="tabCanvas tab-widget"]'
  #Download Toolbar Button
  tab_btn_xpath = '//*[@id="download-ToolbarButton"]/span[1]'
  #download form options
  data_btn_xpath='//*[@data-tb-test-id="DownloadData-Button"]'
  #data_btn_xpath='//*[@id="DownloadDialog-Dialog-Body-Id"]/div/fieldset/button[2]'
  #downloaded csv paths
  csv_Race = "case-by-race-bars_data.csv"
  csv_Eth = "case-by-race-bars_data.csv"


  def getCSV(interval, metric_csv):

    retry_wait_click_all(wd, interval, 'xpath',tab_btn_xpath)
    #wait.until(EC.element_to_be_clickable((By.XPATH,tab_btn_xpath))).click()
    time.sleep(10)
    print("clicked download button")
    print('record the windows that are open')
    window_before = wd.window_handles[0]
    print(window_before)
    #switch to the new window that opens up
    retry_wait_click_all(wd, interval, 'xpath',data_btn_xpath)
    #wait.until(EC.element_to_be_clickable((By.XPATH,data_btn_xpath))).click()
    print("clicked data button")
    window_after = wd.window_handles[1]
    wd.switch_to_window(window_after)
    print("window after")
    retry_wait_click_all(wd, interval,  'xpath',"//*[@class='csvLink_summary']")
    #wait.until(EC.element_to_be_clickable((By.XPATH,"//*[@class='csvLink_summary']"))).click()
    print("clicked text file option")
    time.sleep(5)
    #make df
    df = pd.read_csv(metric_csv, sep="\t", encoding="utf-16") #dumb tableau encoding
    print("-" *10)
    return df

 #New Eth
  print("\nDemographics by Ethnicity")
  wd=init_driver()
  wd.get(raceethsrc)
  #time.sleep(10)
  #race_eth_toggle = wait.until(EC.element_to_be_clickable((By.XPATH,"//span[@aria-label='Toggle race / ethnicity Race']")))
  #race_eth_toggle.click()
  retry_wait_click_all(wd, 160, 'xpath',"//span[@aria-label='Toggle race / ethnicity Race']")
  retry_wait_click_all(wd, 160, 'xpath',"//span[text()='Ethnicity']")
  retry_wait_click_all(wd, 160, 'xpath',race_hosp_xpath)
  #wait.until(EC.element_to_be_clickable((By.XPATH,"//span[text()='Ethnicity']"))).click()
  #wait.until(EC.element_to_be_clickable((By.XPATH,race_hosp_xpath))).click()
  df_casesEth = getCSV(160, csv_Eth)
  df_casesEth=df_casesEth.drop(['Population'],1).fillna('0')
  display(df_casesEth)
  wd.quit()

 #Race
  wd=init_driver()
  wd.get(raceethsrc)
  wait = WebDriverWait(wd, 160)
  print(raceethsrc)
  #Demographics by Race
  print("\nDemographics by Race")
  #time.sleep(10)
  #wait.until(EC.element_to_be_clickable((By.XPATH,race_hosp_xpath))).click()
  retry_wait_click_all(wd, 160,  'xpath',race_hosp_xpath)
  print("selected Hospitalizations")
  df_casesRace = getCSV(160, csv_Race)
  df_casesRace.fillna('0')
  #Get rid of extra rows
  df_casesRace=df_casesRace.drop(['Population'],1).fillna('0')
  display(df_casesRace)
  wd.quit()

  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('G26',dataToWrite)
      #Write Demographic Data
      #writeTable(df_totals,'Confirmed Case & Death Totals','L15')
      writeTable(df_casesRace,'','H27',ws)
      writeTable(df_casesEth,'Demographics by Ethnicity','L23',ws)

#MN
#ARCGIS
def runMN(ws, write):
  url='https://services2.arcgis.com/V12PKGiMAH7dktkU/ArcGIS/rest/services/COVIDSUMMARY032920/FeatureServer/0'
  urlRaceCase='https://services2.arcgis.com/V12PKGiMAH7dktkU/arcgis/rest/services/MyMapService/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=EvrHospYes%2C+TotalCases%2C+RaceWht%2C+RaceBlk%2C+RaceAsian%2C+RaceAmerIn%2C+RacePacifi%2C+RaceMultip%2C+RaceOther%2C+RaceHisp%2C++RaceUnk&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urlRaceCase)
  df_case=pd.json_normalize(req.json()['features'])
  df_case=df_case.rename(columns=lambda x: re.sub('properties.','',x))
  df_caseR=df_case.transpose()
  df_caseR=df_caseR.reset_index()
  df_caseR=df_caseR[2:]
  display(df_caseR)

  urlRaceDeath='https://services2.arcgis.com/V12PKGiMAH7dktkU/arcgis/rest/services/MyMapService/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=EvrHospYes%2C++OutcmDied%2C+DeathWht%2C+DeathBlk%2C+DeathAsian%2C+DeathNativ%2C+DeathPacif%2C++DeathRaceM%2C+DeathOther%2C+DeathHisp%2C++DeadUnknow&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urlRaceDeath)
  df_death=pd.json_normalize(req.json()['features'])
  df_death=df_death.rename(columns=lambda x: re.sub('properties.','',x))
  df_deathR=df_death.transpose()
  df_deathR=df_deathR[2:]
  display(df_deathR)

  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('C48',dataToWrite)

      # Write Data To Sheet
      writeTable(df_caseR,'','B51',ws)
      writeTable(df_deathR,'','D51',ws)
     # writeTable(df_tot,'','I19')

#MO
def runMO(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains

  #Sumary Dashboard
  url="https://showmestrong.mo.gov/data-download/"

  #iframe src
  src="https://results.mo.gov/t/COVID19/views/COVID-19DataforDownload/MetricsbyCounty?:embed=y&amp;:showVizHome=no&amp;:host_url=https%3A%2F%2Fresults.mo.gov%2F&amp;:embed_code_version=3&amp;:tabs=yes&amp;:toolbar=yes&amp;:showAppBanner=false&amp;:display_spinner=no&amp;:loadOrderID=0"

  #xpaths for race/ethnicity/totals
  race_xpath="(//span[contains(@id, 'tableauTabbedNavigation_tab_3')])"
  ethnicity_xpath="(//span[contains(@id, 'tableauTabbedNavigation_tab_4')])"

  #xpaths for all of the buttons
  demos_dnld_xpath= '//*[@id="download-ToolbarButton"]'
  crosstab_xpath='//*[@id="DownloadDialog-Dialog-Body-Id"]/div/button[3]'
  csv_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[2]/div[2]/div/label[2]'
  dnld_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[3]/button'

  #csv names
  race_csv='Metrics by Race.csv'
  ethnicity_csv='Metrics by Ethnicity.csv'

  #Order for downloads:
  #Go to url
  #Go to race or ethnicity or totals tab
  #Click the demos_dnld_xpath
  #Click crosstab
  #Click csv
  #Click metric file - NOT NEEDED
  #Click dnld button

  #define getCSV MO style
  def getCSV(metric_xpath,metric_csv):

      retry_wait_click_all(wd,240,'xpath',metric_xpath)
      #wait.until(EC.element_to_be_clickable((By.XPATH,metric_xpath))).click()
      #print("clicked metric on tableau frame")
      time.sleep(10)

      retry_wait_click_all(wd,240,'xpath',demos_dnld_xpath)
      #wait.until(EC.element_to_be_clickable((By.XPATH,demos_dnld_xpath))).click()
      #print("clicked download on tableau frame")
  
      retry_wait_click_all(wd,240,'xpath',crosstab_xpath)
      #wait.until(EC.element_to_be_clickable((By.XPATH,crosstab_xpath))).click()
      #print("clicked crosstab")

      retry_wait_click_all(wd,240,'xpath',csv_xpath)
      #wait.until(EC.element_to_be_clickable((By.XPATH,csv_xpath))).click()
      #print("clicked csv option")

      retry_wait_click_all(wd,240,'xpath',dnld_xpath)
      #wait.until(EC.element_to_be_clickable((By.XPATH,dnld_xpath))).click()
      #print('clicked download button')
      time.sleep(10)
      #make df
      df=pd.read_csv(metric_csv,sep="\t", encoding="utf-16")

      df=df.fillna('0')
      return df

  #initialize the driver, get the url and take a nap
  wd=init_driver()
  wd.get(src)
  #wait = WebDriverWait(wd, 240)
  print("Demographics Tableau")
  print(src)
  time.sleep(15)

  #get demographic counts
  race = getCSV(race_xpath, race_csv)
  time.sleep(5)
  ethnicity = getCSV(ethnicity_xpath,ethnicity_csv)
  wd.quit()

  #Fix column names
  tcols=list(race.columns)
  tcols[0]='Race'
  tcols[1]='Confirmed Cases'
  tcols[2]='Probable Cases'
  tcols[3]='Confirmed Deaths'
  tcols[4]='PCR Tests'
  tcols[5]='Antigen Tests'
  tcols[6]='Serology Tests'
  race.columns=tcols
  tcols=list(ethnicity.columns)
  tcols[0]='Ethnicity'
  tcols[1]='Confirmed Cases'
  tcols[2]='Probable Cases'
  tcols[3]='Confirmed Deaths'
  tcols[4]='PCR Tests'
  tcols[5]='Antigen Tests'
  tcols[6]='Serology Tests'
  ethnicity.columns=tcols
  
  #Remove ',' and convert to int for Race
  race['Confirmed Cases']=race['Confirmed Cases'].str.replace(r",","").astype(int)
  race['Probable Cases']=race['Probable Cases'].str.replace(r",","").astype(int)
  race['Confirmed Deaths']=race['Confirmed Deaths'].str.replace(r",","").astype(int)
  race['PCR Tests']=race['PCR Tests'].str.replace(r",","").astype(int)
  race['Antigen Tests']=race['Antigen Tests'].str.replace(r",","").astype(int)
  race['Serology Tests']=race['Serology Tests'].str.replace(r",","").astype(int)
  #For Ethnicity
  ethnicity['Confirmed Cases']=ethnicity['Confirmed Cases'].str.replace(r",","").astype(int)
  ethnicity['Probable Cases']=ethnicity['Probable Cases'].str.replace(r",","").astype(int)
  ethnicity['Confirmed Deaths']=ethnicity['Confirmed Deaths'].str.replace(r",","").astype(int)
  ethnicity['PCR Tests']=ethnicity['PCR Tests'].str.replace(r",","").astype(int)
  ethnicity['Antigen Tests']=ethnicity['Antigen Tests'].str.replace(r",","").astype(int)
  ethnicity['Serology Tests']=ethnicity['Serology Tests'].str.replace(r",","").astype(int)
 
  display(race)
  print("\n")
  display(ethnicity)


  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('J16',dataToWrite)

      # Write Data To Sheet
      writeTable(race,'Race Totals','J17',ws)
      writeTable(ethnicity,'Ethnicity Totals','J28',ws)

#MS
def runMS(ws, write):

  #Extract the latest file from the URL by finding the biggest number
  big=0
  html_page = urllib.request.urlopen("https://msdh.ms.gov/msdhsite/_static/14,0,420,884.html")
  soup = BeautifulSoup(html_page, "html.parser")
  for link in soup.findAll('a', attrs={'href': re.compile(".pdf")}):
    filename = link.get('href')
    segments = filename.rpartition('/')
    index = segments[2].split(".")
    index = int(index[0])
    #print("\nindex")
    #print(index)
    if (index > big) and ('Cases and Deaths' in link['title']):
        big = index
  #print(big)

  #Put the filename back together
  url=('https://msdh.ms.gov/msdhsite/_static/resources/{}.pdf'.format(big))
  print(url)

  print("\nMS Cases by Race & Ethnicity")
  #Grab first half & second havles of tables - strip off first two rows and then rename columns
  casesTable1 = tabula.read_pdf(url,pages=1 ,multiple_tables=True, lattice=True)
  print("table2")
  casesTable2 = tabula.read_pdf(url,pages=2 ,multiple_tables=True, lattice=True)
  deathsTable1 = tabula.read_pdf(url,pages=3 ,multiple_tables=True, lattice=True)
  deathsTable2 = tabula.read_pdf(url,pages=4 ,multiple_tables=True, lattice=True)
  #Read in the url and on Page 4, find the Race & Ethnicity at the bottom of the page
  case1=casesTable1[0]
  case2=casesTable2[0]
  death1=deathsTable1[0]
  death2=deathsTable2[0]
  #Slice off header rows for CASES & DEATHS
  case1=case1[2:]
  case2=case2[2:]
  death1=death1[2:]
  death2=death2[2:]
  #Concatenate Death tables
  death=pd.concat([death1,death2], axis=0, ignore_index=True)

  def column_namer(in_df,out_df):
     #Rename Case Columns - done because of funky behavoir when slicing off header
     cols = list(in_df.columns)
     cols[0] = 'County'
     cols[1] = 'Tot Cases'
     cols[2] = 'Blk NH'
     cols[3] = 'White NH'
     cols[4] = 'AIAN NH'
     cols[5] = 'Asian NH'
     cols[6] = 'Other NH'
     cols[7] = 'Unk NH'
     cols[8] = 'Junk'
     cols[9] = 'Blk Hisp'
     cols[10] = 'White Hisp'
     cols[11] = 'AIAN Hisp'
     cols[12] = 'Asian Hisp'
     cols[13] = 'Other Hisp'
     cols[14] = 'Unk Hisp'
     cols[15] = 'Blk Unk'
     cols[16] = 'White Unk'
     cols[17] = 'AIAN Unk'
     cols[18] = 'Asian Unk'
     cols[19] = 'Other Unk'
     cols[20] = 'Unk Unk'
     out_df.columns = cols
     return out_df

  #Concatenate the dataframes
  column_namer(case1, case1)
  column_namer(case2, case2)
  case=pd.concat([case1,case2], axis=0,ignore_index=True)  #ignore index fixes index wonkiness when concatenating
  #create the checker dataframe (casetotCheck)
  casetotCheck=case.drop('Junk',1).fillna('0')
  casetot=casetotCheck.drop('County',1).astype(int)
  #display(casetotCheck.iloc[11])

  #display(casetot)
  case_tot=casetot.tail(1)  #Select just the last row as the totals to write to google sheet
  case_county=casetot.drop(case.tail(1).index) #Select every row to compare totals
  case_bycounty=case_county.sum(axis=0)

  #display(case_bycounty)
  display(case_tot)

  #Rename Death Columns
  cols = list(death.columns)
  cols[0] = 'County'
  cols[1] = 'Tot Deaths'
  cols[2] = 'Blk NH'
  cols[3] = 'White NH'
  cols[4] = 'AIAN NH'
  cols[5] = 'Asian NH'
  cols[6] = 'Other NH'
  cols[7] = 'Unk NH'
  cols[8] = 'Blk Hisp'
  cols[9] = 'White Hisp'
  cols[10] = 'AIAN Hisp'
  cols[11] = 'Asian Hisp'
  cols[12] = 'Other Hisp'
  cols[13] = 'Unk Hisp'
  cols[14] = 'Blk Unk'
  cols[15] = 'White Unk'
  cols[16] = 'AIAN Unk'
  cols[17] = 'Asian Unk'
  cols[18] = 'Other Unk'
  cols[19] = 'Unk Unk'
  death.columns = cols

  deathtot=death.drop('County',1).fillna('0').astype(int)
  death_tot=deathtot.tail(1) #Last row has the totals
  death_county=deathtot.drop(deathtot.tail(1).index) #All rows BUT the last
  death_county=death_county.sum(axis=0)
  #display(death_county)
  display(death_tot)

  #Do counties match tot?
  print("\nTotal vs. Counties ok if True")
  display(case_bycounty.eq(case_tot))
  display(death_county.eq(death_tot))

  #Totals in progress

  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('B36',dataToWrite)

      # Write Data To Sheet
      writeTable(case_tot,'','A37',ws)
      writeTable(death_tot,'','A40',ws)

#MT
def runMT(ws, write):
 
  #Setup Dates
  todaysDate=date.today()
  latest= todaysDate
  latest = latest.strftime('%Y%m%d')

  #From the link - find the link with DemographicsTable in the filename
  link = 'https://dphhs.mt.gov/publichealth/cdepi/diseases/coronavirusmt/demographics'
  req = requests.get(link)
  soup = BeautifulSoup(req.text, 'html.parser')
  for link in soup.findAll('a', attrs={'href': re.compile(".pdf")}):
    filename = link.get('href')
    segments = filename.rpartition('/')
    namedate = segments[2].split('.')
    name=namedate[0]
    #print(name)
    if 'DemographicTables' in name:
      #print('found')
      url = "https://dphhs.mt.gov{}".format(filename)
      print(url)

 
  #Read in the url, page 1 and select the first table & display the values for the Totals   
  tables = tabula.read_pdf(url,pages=1,multiple_tables=True)
  print("read tables")
  totals = tables[0]
  tcols = list(totals.columns)
  tcols[0] = 'MT'
  tcols[1] = 'Totals'
  totals.columns = tcols
  print("setup totals")

  hosp = tables[2]
  tcols = list(hosp.columns)
#  tcols[0] = 'Hospitalization Status'
#  tcols[1] = 'Number of Cases'
  tcols[0] = 'Status'
  tcols[1] = 'Cases'
  hosp.columns = tcols
  print("setup hosp columns")

  #Replace NaN with ''
  totals=totals.fillna('0').drop(totals.index[0])
  #totals=totals.fillna('0')
  #print("did fillna on totals and lopped the top row")
  #display(totals)

  if len(hosp) == 4:
    hosp=hosp.fillna('0').drop(hosp.index[0])
  else:
    hosp=hosp.fillna('0')
  #print("did fillna on hosp and lopped the top row")
  #display(hosp)

  #Remove parens from numbers
  display(totals['Totals'].dtypes)
  if (totals['Totals'].dtypes == 'str'):
    totals['Totals']=totals['Totals'].str.replace(r"1E5","100000",regex=True)
    totals['Totals']=totals['Totals'].str.replace(r"\(.*\)","",regex=True).astype(int)
  #print("totals regex")
  #hosp['Number of Cases']=hosp['Number of Cases'].replace("\(.*\)","",regex=True).astype(int)
  hosp['Cases']=hosp['Cases'].str.replace(r"\(.*\)","",regex=True).astype(int)
  print('MT Totals Table\n')
  display(totals)
  print('MT Hospitalized\n')
  display(hosp)

  #Read in the url and on Page 4, find the Race & Ethnicity at the bottom of the page
  raceeth = tabula.read_pdf(url,pages=5,multiple_tables=True, lattice=True)
  race=raceeth[0]
  #print("\n Race from page 5")
  #print(type(race))
  #print(race.columns)

  #Rename columns
  cols = list(race.columns)
  cols[0] = 'Race'
  cols[1] = 'Cases'
  cols[2] = 'Deaths'
  race.columns = cols
  #Remove "(contents)" from Case numbers
  race['Cases']=race['Cases'].replace("\(.*\)","",regex=True)
  race['Deaths']=race['Deaths'].replace("\(.*\)","",regex=True)
  #Replace < 5 with 1
  race['Deaths']=race['Deaths'].replace("< 5","1",regex=True)
  raceeth = race.drop(race.index[0])
  raceeth=raceeth.fillna('0')

  #Convert Str to Numeric
  raceeth = raceeth.astype({"Cases": int, "Deaths": int})
  print("\nRace and Ethnicity Tables")
  display(raceeth)

  if write == True:
    # Write Paste Date To Sheet
     dataToWrite = [[date.today().strftime('%m/%d')]]
     #ws.update('J18',dataToWrite)

     # Write Data To Sheet
     writeTable(raceeth,'Race Table','H19',ws)
     writeTable(totals,'Totals  Table','H33',ws)
     writeTable(hosp,'Ever Hospitalized','H39',ws) 


#NC
def runNC(ws, write):

  #totals ARGCIS
  url='https://services.arcgis.com/iFBq2AW9XO0jYYF7/ArcGIS/rest/services/NCCovid19/FeatureServer/0/query?outStatistics=%5B%7B%27onStatisticField%27%3A+%27Hosp%27%2C+%27statisticType%27%3A+%27max%27%7D%2C+%7B%27onStatisticField%27%3A+%27Total%27%2C+%27statisticType%27%3A+%27sum%27%7D%2C+%7B%27onStatisticField%27%3A+%27Deaths%27%2C+%27statisticType%27%3A+%27sum%27%7D%5D&where=1%3D1'
  #iframe src
  src ='https://public.tableau.com/views/NCDHHS_COVID-19_DataDownload/Demographics'

  #for Totals
  urltot='https://services.arcgis.com/iFBq2AW9XO0jYYF7/ArcGIS/rest/services/NCCovid19/FeatureServer/0/query?where=1%3D1&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&returnGeodetic=false&outFields=&returnGeometry=true&returnCentroid=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=%5B%7B%27onStatisticField%27%3A+%27Hosp%27%2C+%27statisticType%27%3A+%27max%27%7D%2C+%7B%27onStatisticField%27%3A+%27Total%27%2C+%27statisticType%27%3A+%27sum%27%7D%2C+%7B%27onStatisticField%27%3A+%27Deaths%27%2C+%27statisticType%27%3A+%27sum%27%7D%5D&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urltot)
  df_tot=pd.json_normalize(req.json()['features'])
  df_tot=df_tot.rename(columns=lambda x: re.sub('properties.','',x))
  display(df_tot)

  #xpaths for all of the buttons
  demos_dnld_xpath= '//*[@id="download-ToolbarButton"]'
  crosstab_xpath='//*[@id="DownloadDialog-Dialog-Body-Id"]/div/fieldset/button[3]'
  csv_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[2]/div[2]/div/label[2]'
  #race_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[4]/div/div/div/div'
  race_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[4]/div/div'
  #eth_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[2]/div/div/div/div'
  eth_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[2]/div/div'
  dnld_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[3]/button'

  #csv names
  race_csv='TABLE_RACE.csv'
  ethnicity_csv='TABLE_ETHNICITY.csv'

  #Order for downloads
  #Go to url->demos_dnld_xpath->crosstab_xpath->csv_xpath->metric_xpath->dnld_xpath

  #define getCSV NC style
  def getCSV(metric_xpath,metric_csv):
    wait = WebDriverWait(wd, 20)

    retry_wait_click_all(wd, 20, 'css', ".tab-icon-download")
    #wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ".tab-icon-download"))).click()
    print('clicked download')
    retry_wait_click_all(wd, 20, 'xpath', "//button[text()='Crosstab']")
    #wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Crosstab']"))).click()
    print('clicked crosstab')
    retry_wait_click_all(wd, 20, 'css', "label[data-tb-test-id='crosstab-options-dialog-radio-csv-Label']")
    #wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "label[data-tb-test-id='crosstab-options-dialog-radio-csv-Label']"))).click()

    print('clicked csv')
    metric = wait.until(EC.element_to_be_clickable((By.XPATH, metric_xpath)))
    if len(metric.find_elements_by_tag_name('div')) == 1:
      metric.click()

    print('clicked file'), 
    retry_wait_click_all(wd, 20, 'xpath', "//button[text()='Download']")
    #wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Download']"))).click()
    print('clicked download')
    time.sleep(5)

    df=pd.read_csv(metric_csv,sep="\t", encoding="utf-16")
    df=df.fillna('0')
    return df

  #inititalize the driver, get the url and take a nap
  wd=init_driver()
  wd.get(src)
  time.sleep(5)
  #print("Demographics Tableau")
  print(src)
  time.sleep(5)

  #get total counts
  race = getCSV(race_xpath, race_csv)
  time.sleep(5)
  ethnicity = getCSV(eth_xpath,ethnicity_csv)
  time.sleep(5)
  wd.quit()

  #Remove , and convert to int for Race
  race['Cases']=race['Cases'].str.replace(r",","").astype(int)
  race['Deaths']=race['Deaths'].str.replace(r",","").astype(int)
  #For Ethnicity
  ethnicity['Cases']=ethnicity['Cases'].str.replace(r",","").astype(int)
  ethnicity['Deaths']=ethnicity['Deaths'].str.replace(r",","").astype(int)

  display(race)
  display(ethnicity)

  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('H19',dataToWrite)

      # Write Data To Sheet
      writeTable(race,'Race Totals','G20',ws)
      writeTable(ethnicity,'Ethnicity Totals','G28',ws)

#ND
def runND(ws,write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains

  url = 'https://app.powerbigov.us/view?r=eyJrIjoiYTExNDg5YjctOTQxZi00YmU2LWI5Y2YtNWVkMDFmNDUyN2FlIiwidCI6IjJkZWEwNDY0LWRhNTEtNGE4OC1iYWUyLWIzZGI5NGJjMGM1NCJ9'

  def colstr2int(df,col):
    df.loc[:,col] = df.loc[:,col].replace(',','', regex=True)
    df[col] = df[col].astype('int')

  wd = init_driver()
  wd.get(url)
  wait = WebDriverWait(wd, 20)
  #retry_wait_click_all(wd, 20, 'css', "i[title='Next Page']")
  wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "i[title='Next Page']"))).click()
  print('clicked Next Page')
  time.sleep(1)
  #retry_wait_click_all(wd, 20, 'css', "i[title='Next Page']")
  wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "i[title='Next Page']"))).click()
  print('clicked Next Page')

  #retry_wait_click_all(wd, 20, 'xpath', "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[12]/transform/div/div[3]/div/visual-modern/div/button")
  cum_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[12]/transform/div/div[3]/div/visual-modern/div/button")))
  cum_button.click()
  print('clicked Cumulative')

  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[13]/transform/div/div[3]/div/visual-modern")))
  ActionChains(wd).context_click(elements[0]).perform()
  #  time.sleep(10)
  wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='Show as a table']"))).click()
  print('clicked Show Table')

  cats = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive ']")))
  for element in elements:
    cats.append(element.text)

  vals = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive tablixAlignRight ']")))
  for element in elements:
    vals.append(element.text)

  df = pd.DataFrame([cats[-7:],vals]).T
  df.columns = ['Category', 'Cases']
  colstr2int(df,'Cases')
  df.sort_values('Category',ascending=True,inplace=True)
  df = df.reset_index(drop=True)
  display(df)

  url =  'https://www.health.nd.gov/diseases-conditions/coronavirus/north-dakota-coronavirus-cases'
  wd.get(url)
  deaths=wait.until(EC.presence_of_element_located((By.XPATH,"//tbody/tr[5]/td[2]")))
  df_deaths = pd.DataFrame([['Total',int(deaths.text)]],columns=['Category','Deaths'])
  display(df_deaths)
  wd.quit()

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('D22',dataToWrite)

    # Write Data To Sheet
    writeTable(df,'','C23',ws)
    writeTable(df_deaths,'','',ws)

#NE
def runNE(ws, write):
    #totals, case dems, death dems
  print("NE Dems")
  #NE Deaths  FIXED
  urldeath='https://gis.ne.gov/Enterprise/rest/services/Covid19MapV6/MapServer/4/query?f=json&where=1%3D1&returnGeometry=false&spatialRel=esriSpatialRelIntersects&outFields=*&outSR=102100&resultOffset=0&resultRecordCount=50'
  #headers = {"User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:79.0) Gecko/20100101 Firefox/79.0"}
  req = requests.get(urldeath)
  #Read in from ArcGIS->replace NaN
  df_death=pd.json_normalize(req.json()['features'])
  df_death=df_death.fillna('0').astype('int64')
  df_death= df_death.rename(columns={'attributes.DEATH_TOTAL':'Deaths'})
  df_death=df_death['Deaths'].astype('int64')
  #print("Deaths")
  #display(df_death)

  #NE Hospitalizations FIXED
  urlhosp='https://gis.ne.gov/Enterprise/rest/services/Covid19MapV6/MapServer/5/query?f=json&where=1%3D1&returnGeometry=false&spatialRel=esriSpatialRelIntersects&outFields=*&outStatistics=%5B%7B%22statisticType%22%3A%22avg%22%2C%22onStatisticField%22%3A%22HOSP_0_19%22%2C%22outStatisticFieldName%22%3A%22HOSP_0_19%22%7D%2C%7B%22statisticType%22%3A%22avg%22%2C%22onStatisticField%22%3A%22HOSP_20_34%22%2C%22outStatisticFieldName%22%3A%22HOSP_20_34%22%7D%2C%7B%22statisticType%22%3A%22avg%22%2C%22onStatisticField%22%3A%22HOSP_35_44%22%2C%22outStatisticFieldName%22%3A%22HOSP_35_44%22%7D%2C%7B%22statisticType%22%3A%22avg%22%2C%22onStatisticField%22%3A%22HOSP_45_54%22%2C%22outStatisticFieldName%22%3A%22HOSP_45_54%22%7D%2C%7B%22statisticType%22%3A%22avg%22%2C%22onStatisticField%22%3A%22HOSP_55_64%22%2C%22outStatisticFieldName%22%3A%22HOSP_55_64%22%7D%2C%7B%22statisticType%22%3A%22avg%22%2C%22onStatisticField%22%3A%22HOSP_65_74%22%2C%22outStatisticFieldName%22%3A%22HOSP_65_74%22%7D%2C%7B%22statisticType%22%3A%22avg%22%2C%22onStatisticField%22%3A%22HOSP_75_84%22%2C%22outStatisticFieldName%22%3A%22HOSP_75_84%22%7D%2C%7B%22statisticType%22%3A%22avg%22%2C%22onStatisticField%22%3A%22HOSP_85UP%22%2C%22outStatisticFieldName%22%3A%22HOSP_85UP%22%7D%5D&outSR=102100'
  req = requests.get(urlhosp)
  #Read in from ArcGIS->replace NaN
  df_hosp=pd.json_normalize(req.json()['features'])
  df_hosp=df_hosp.fillna('0')
  df_hosp['Hospitalized']=df_hosp.sum(axis=1)
  df_hosp=df_hosp['Hospitalized'].astype('int64')
  #display(df_hosp)

  #NE Case Totals FIXED
  urlpos='https://gis.ne.gov/Enterprise/rest/services/Covid19MapV6/MapServer/1/query?f=json&where=1%3D1&returnGeometry=false&spatialRel=esriSpatialRelIntersects&outFields=*&orderByFields=AllTestsAsOfThisDate%20desc&outSR=102100&resultOffset=0&resultRecordCount=1'
  req = requests.get(urlpos)
  #Read in from ArcGIS & replace NaN
  df_pos=pd.json_normalize(req.json()['features'])
  df_pos=df_pos.fillna('0')
  df_pos= df_pos.rename(columns={'attributes.TotalPositiveAsOfThisDate':'Cases'})
  df_pos=df_pos['Cases'].astype('int64')
  #print("Positive Cases")
  #display(df_pos)

  #Combine 3 dataframes to get a total dataframe
  df_tot=pd.concat([df_pos,df_hosp,df_death],axis=1)
  display(df_tot)

  #NE Demographics
  urldems='https://gis.ne.gov/enterprise/rest/services/Covid19MapV6/MapServer/6/query?f=json&where=1%3D1&returnGeometry=false&spatialRel=esriSpatialRelIntersects&outFields=*&orderByFields=Category%20desc&resultOffset=0&resultRecordCount=500'
  req = requests.get(urldems)
  #Read in from ArcGIS->replace NaN->slice unneeded columns->rotate->slice number indices->
  #remove unneeded new columns-> remove attributes. from race & ethnicity categories
  df_cases=pd.json_normalize(req.json()['features'])
  df_cases=df_cases.fillna('0')
  df_casesF=df_cases.transpose().reset_index()
  df_casesF=df_casesF[1:12]
  df_casesF=df_casesF.reset_index()
  df_casesF=df_casesF.drop(['level_0',0,4],1)
  df_casesF['index']=df_casesF['index'].str.replace(r"attributes.","")
  display(df_casesF)

  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('B38',dataToWrite)

      # Write Data To Sheet
      writeTable(df_tot,'','C39',ws)
      writeTable(df_casesF,'Case Totals','C41',ws)

# NH
def runNH(ws, write):

  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains

  #interactive equity dash
  #iframe
  url="https://www.nh.gov/t/DHHS/views/COVID19InteractiveEquityDashboard/COVID19InteractiveEquityDashboard?:isGuestRedirectFromVizportal=y&amp;:embed=y"
  #Cases Button xpaths
  cases_xpath='//*[@id="[Parameters].[Parameter 3]_0"]'
  deaths_xpath='//*[@id="[Parameters].[Parameter 3]_1"]/div[2]/input'

  #compare rates by
  raceeth_xpath = "//*[@id='[Parameters].[Parameter 1]_0']"

  #compare numbers by sex "No"
  sex_xpath = "//*[@id='[Parameters].[Parameter 5]_0']"

  #download form options
  data_btn_xpath = "//*[@id='view1917354692013186876_13459133749204884095']/div[1]/div[2]/canvas[2]"

  #downloaded csv paths
  csv_Cases = "CrudeCount-Cases.csv"
  csv_Deaths = "CrudeCount-Deaths.csv"

  def getCSV(demo_xpath, demo_csv):
        display(demo_xpath, demo_csv)
        #Demographics for Cases
        print("Demographics for Cases/Deaths")
        #retry_wait_click_all(wd, 20,'xpath',demo_xpath)
        demo_element = wait.until(EC.presence_of_element_located((By.XPATH, demo_xpath)))
        #demo_element = wait.until(EC.element_to_be_clickable((By.XPATH, demo_xpath)))
        display(demo_element.get_attribute('outerHTML'))
        demo_element.click()
        print("clicked Cases/Deaths")

        #choose race or ethnicity vs. Age
        retry_wait_click_all(wd, 20,'xpath',raceeth_xpath)
        #wait.until(EC.element_to_be_clickable((By.XPATH, raceeth_xpath))).click()
        print("clicked race or ethnicity vs age")

        #choose sex yes or no view
        retry_wait_click_all(wd, 20,'xpath',sex_xpath)
        #wait.until(EC.element_to_be_clickable((By.XPATH, sex_xpath))).click()
        print("chose sex=no")

        #click data option
        retry_wait_click_all(wd, 20,'xpath',data_btn_xpath)
        #wait.until(EC.element_to_be_clickable((By.XPATH, data_btn_xpath))).click()
        print("clicked download button")   

        wd.save_screenshot("nc_demo.png");
        timer = 0
        display(os.path.exists(demo_csv))
        while (not os.path.exists(demo_csv)) & (timer < 60):
          time.sleep(1)
          timer+=1

        #download view and convert to df
        df = pd.read_csv(demo_csv, sep=",", encoding="utf-8", na_values=['']) #dumb tableau encoding
        df = df.fillna('0')
        os.remove(demo_csv)

        return df

  #cases
  wd=init_driver()
  wd.get(url)
  wd.maximize_window()
  wait = WebDriverWait(wd, 20)
  time.sleep(5)
  print("\nTableau URL")
  print(url)

  #make df_casesRace
  df_casesRace = getCSV(cases_xpath, csv_Cases)
  tcols = list(df_casesRace.columns)
  tcols[0] = 'Demographic'
  tcols[1] = 'Sex'
  tcols[2] = 'Cases'
  tcols[3] = 'Suppression'
  tcols[4] = 'Date'
  df_casesRace.columns = tcols
  df_casesRace['Cases'] = df_casesRace['Cases'].astype('int')
  print("-" *10)
  display(df_casesRace)
  wd.quit()


  #deaths
  wd=init_driver()
  wd.get(url)
  wd.maximize_window()
  wait = WebDriverWait(wd, 20)
  time.sleep(5)
  print("\nTableau URL")
  print(url)

  #make df_deathsRace
  df_deathsRace = getCSV(deaths_xpath, csv_Deaths)
  tcols = list(df_deathsRace.columns)
  tcols[0] = 'Demographic'
  tcols[1] = 'Sex'
  tcols[2] = 'Suppression'
  tcols[3] = 'Deaths'
  tcols[4] = 'Date'
  df_deathsRace.columns = tcols
  df_deathsRace['Deaths'] = df_deathsRace['Deaths'].astype(int)
  print("-" *10)
  print("\nDeaths")
  display(df_deathsRace)

  #Summary Dash for Totals
  #urlSum = "https://www.nh.gov/covid19/dashboard/case-summary.htm"
  urlSumT="https://www.nh.gov/t/DHHS/views/COVID-19CaseDashboard/Summary?:iid=1&amp;:isGuestRedirectFromVizportal=y&amp;:embed=y"
  wd.quit()
  wd=init_driver()
  wd.get(urlSumT)
  wait = WebDriverWait(wd, 20)
  time.sleep(5)

  #xpaths
  summary_xpath ='//*[@id="tableauTabbedNavigation_tab_0"]'
  race_xpath='//*[@id="tabZoneId125"]/div/div/div/div[1]/div[5]/div[1]/canvas'
  metrics_xpath='//*[@id="view2676828700685879255_11683864725981880803"]/div[1]/div[2]/canvas[2]'
  totals_xpath="//*[@id='view2676828700685879255_12402419230462992794']/div[1]/div[2]/canvas[2]"
  casepercent_xpath="//*[@id='title2676828700685879255_18320527963907039571']/div[1]/div/span/div/span[2]"
  hosppercent_xpath="//*[@id='title2676828700685879255_18320527963907039571']/div[1]/div/span/div/span[4]"
  deathpercent_xpath="//*[@id='title2676828700685879255_18320527963907039571']/div[1]/div/span/div/span[6]"
  
  #press summary tab
  print("\nSummary Tab")
  retry_wait_click_all(wd, 20,'xpath',summary_xpath)
  #wait.until(EC.element_to_be_clickable((By.XPATH, summary_xpath))).click()

  print("\nPercents of Known Demographics")
  casepercent=wd.find_element_by_xpath(casepercent_xpath)

  #print("\nCase Percent")
  casepercent=casepercent.text
  casepercent=casepercent.replace('%','', 1)
  casepercent=float(casepercent)
  #print(casepercent)

  #print("\nHospPercent")
  hosppercent=wd.find_element_by_xpath(hosppercent_xpath)
  hosppercent=hosppercent.text
  hosppercent=float(hosppercent.replace('%','', 1))
  #print(hosppercent)

  #print("\nDeathPercent")
  deathpercent=wd.find_element_by_xpath(deathpercent_xpath)
  deathpercent=deathpercent.text
  deathpercent=float(deathpercent.replace('%','', 1))
  #print(deathpercent)

  # initialize list of lists
  dataPercent = {'Percent Known':['Case %','Hosp %','Deaths %'],'Value':[casepercent, hosppercent, deathpercent]}
  ##Put the reads into one row of a matrix, then convert to dataframe
  knownpercent = pd.DataFrame(dataPercent)
  display(knownpercent)

  wd.quit()

  #Hospitalizations & Totals
  wd=init_driver()

  #Sumary Dashboard
  urlSum="https://www.nh.gov/t/DHHS/views/COVID-19CaseDashboard/Summary?:iid=1&amp;:isGuestRedirectFromVizportal=y&amp;:embed=y"
  wd.get(urlSum)
  wait = WebDriverWait(wd, 20)
  time.sleep(5)
  print("\nTableau URL")
  print(urlSum)

  #xpaths
  summary_xpath ='//*[@id="tableauTabbedNavigation_tab_0"]'
  dnld_xpath = '//*[@id="download-ToolbarButton"]/span[1]'
  sumpdf_xpath = '//*[@id="DownloadDialog-Dialog-Body-Id"]/div/button[4]'
  view_xpath='//*[@id="tableau-ui-1612745976402"]/span'
  dnldpdf_xpath='//*[@id="PdfDialog-Dialog-Body-Id"]/div/div[2]/div[4]/button'

  #press summary tab
  print("Choose Summary Tab")
  retry_wait_click_all(wd, 20,'xpath',summary_xpath)
  #wait.until(EC.element_to_be_clickable((By.XPATH, summary_xpath))).click()

  #press download button
  print("Choose Download Tab")
  retry_wait_click_all(wd, 20,'xpath',dnld_xpath)
  #wait.until(EC.element_to_be_clickable((By.XPATH, dnld_xpath))).click()

  #Choose pdf
  print("Choose PDF")
  retry_wait_click_all(wd, 20,'xpath',sumpdf_xpath)
  #wait.until(EC.element_to_be_clickable((By.XPATH, sumpdf_xpath))).click()

  #Choose Download
  print("Download PDF")
  retry_wait_click_all(wd, 20,'xpath',dnldpdf_xpath)
  #wait.until(EC.element_to_be_clickable((By.XPATH, dnldpdf_xpath))).click()

  summaryTable = tabula.read_pdf('Summary.pdf',lattice=True, multiple_tables=True, pages=1)
  #display(summaryTable)

  #Totals Table
  totals = summaryTable[0].iloc[3:]
  tcols=list(totals.columns)
  tcols[2] = 'Infections'
  tcols[4] = 'Ever Hospitalized'
  tcols[8] = 'Deaths'
  totals.columns = tcols
  cols=['Infections','Ever Hospitalized','Deaths']
  totals=totals[cols]
  totals['Infections']=totals['Infections'].str.replace(r",",'',1).astype(int)
  totals['Ever Hospitalized']=totals['Ever Hospitalized'].str.replace(r",",'',1).astype(int)
  totals['Deaths']=totals['Deaths'].str.replace(r",",'',1).astype(int)

  totals=totals.fillna('0')
  display(totals)

  #Hospitalizations
  hosp=summaryTable[8].iloc[1:]
  tcols=list(hosp.columns)
  tcols[0] = 'Race/Ethnicity'
  tcols[3] = 'Hosp'
  hosp.columns = tcols
  cols=['Race/Ethnicity','Hosp']
  hosp.index =['White','Hispanic or Latino','Black or African American','Other','Asian','Total']
  hosp=hosp[cols]
  hosp['Race/Ethnicity']=['White','Hispanic or Latino','Black or African American','Other','Asian','Total']
  hosp['Hosp']=hosp['Hosp'].str.replace(r",",'',1).astype(int)
  display(hosp)

  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('F31',dataToWrite)

      # Write Data To Sheet
      #writeTable(df_totals,'Race & Ethnicity Totals','G32',ws)
      writeTable(df_casesRace,'Confirmed Cases by Race & Ethnicity','G31',ws)
      writeTable(df_deathsRace,'Confirmed Deaths by Race & Ethnicity','S31',ws)
      writeTable(knownpercent,'Percent of Known Demographics','G43',ws)
      writeTable(totals,'Race & Ethnicity Totals','G48',ws)
      writeTable(hosp,'Hosp by Race & Ethnicity','M31',ws)

# NM
def runNM(ws, write):

  #totals, case dems, death dems
  print("NM Out")
  urlAll='https://e7p503ngy5.execute-api.us-west-2.amazonaws.com/prod/GetPublicStatewideData'
  req=requests.get(urlAll)
  df_cases=pd.json_normalize(req.json()['data'])
  df_cases=df_cases.drop(['created','cvDataId','updated','archived','currentHospitalizations','recovered','male','female','genderNR','0-9','10-19','20-29','30-39','40-49','50-59','60-69','70-79','80-89','90+','ageNR'],1)
  df_casesR=df_cases.transpose().reset_index()

  display(df_casesR)

  #Death Demographics
  url = 'https://cv.nmhealth.org/epidemiology-reports/'
  req = requests.get(url)
  soup = BeautifulSoup(req.text, 'html.parser')
  a = soup.find('a', string=re.compile("Download The Latest COVID-19 Mortality Report"))
  url_mort = a['href']
  print(url_mort)
  #tables = tabula.read_pdf(url_mort,pages=6,multiple_tables=False,stream=True,pandas_options={'header': 1})
  tables = tabula.read_pdf(url_mort,pages=6,multiple_tables=False,stream=True)
  display(tables)
  death_table=tables[0].iloc[:,[0,-1]].copy()
  display("Death table:")
  death_table.columns = ['Total' if 'Unnamed' in x else x for x in death_table.columns]
  i = 0
  for bool in death_table.loc[:,'Total'].isna():
    if bool:
      death_table.loc[i,'Total']=death_table.loc[i+1,'Total']
    i+=1
  death_table = death_table.drop(death_table[death_table['Race/Ethnicity'].isna()].index)
  display(death_table)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('B36',dataToWrite)

    # Write Data To Sheet
    writeTable(df_casesR,'','B37',ws)
    writeTable(death_table,'','E36',ws)

# NV
def runNV(ws,write):

  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC

  def get_df(col2):
    cats = []
    elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='expandableContent pivotTableCellWrap ']")))
    for element in elements:
      if not element.text.isupper():
        cats.append(element.text)
    vals = []
    elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellNoWrap tablixAlignLeft ']")))
    for element in elements:
        vals.append(element.text)
    df = pd.DataFrame([cats, vals]).T
    df.columns=['Category',col2]
    return df

  url = 'https://app.powerbigov.us/view?r=eyJrIjoiMjA2ZThiOWUtM2FlNS00MGY5LWFmYjUtNmQwNTQ3Nzg5N2I2IiwidCI6ImU0YTM0MGU2LWI4OWUtNGU2OC04ZWFhLTE1NDRkMjcwMzk4MCJ9'


  wd = init_driver()
  wd.get(url)
  wd.maximize_window()
  wait = WebDriverWait(wd, 20)

#  wait.until(EC.element_to_be_clickable((By.XPATH, "//span[text()='Current Status']/parent::*"))).click()
  wait.until(EC.element_to_be_clickable((By.XPATH, "//span[text()='Confirmed Cases']/parent::*"))).click()

  cases = wait.until(EC.visibility_of_element_located((By.XPATH,"//span[text()='Cumulative cases']/parent::*/span[3]"))).text
  wait.until(EC.element_to_be_clickable((By.XPATH, "//span[text()='Deaths']/parent::*"))).click()
  deaths = wait.until(EC.visibility_of_element_located((By.XPATH,"//span[text()='Cumulative Deaths  ']/parent::*/span[2]"))).text
  wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[aria-label=' Page navigation Button. Trends']"))).click()
  wait.until(EC.element_to_be_clickable((By.XPATH, "//span[text()='Testing']/parent::*"))).click()
  tests  = wait.until(EC.visibility_of_element_located((By.XPATH,"//span[text()='People tested']/parent::*/span[2]"))).text
  df_tots = pd.DataFrame([['Totals',cases,deaths,tests]],columns=['Category','Cases','Deaths','Tests'])
  display(df_tots)

  wait.until(EC.element_to_be_clickable((By.XPATH, "//span[text()='Confirmed Cases']/parent::*"))).click()
#  time.sleep(5)

  wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[aria-label=' Page navigation Button. Demographics']"))).click()
  df_cases = get_df('Cases')

  wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-group[3]/transform/div/div[2]/visual-container-modern[3]/transform/div/div[3]/div/visual-modern/div/button"))).click()
  df_deaths = get_df('Deaths')

  wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-group[3]/transform/div/div[2]/visual-container-modern[2]/transform/div/div[3]/div/visual-modern/div/button"))).click()
  df_tests = get_df('Tests')

  df = df_cases.merge(df_deaths,how='left',on='Category')
  df = df.merge(df_tests,how='left',on='Category')
  df.drop(df.index[0:10],inplace=True)
  display(df)

  wd.quit()

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('B37',dataToWrite)

    # Write Data To Sheet
    writeTable(df,'','A38',ws)
    writeTable(df_tots,'','A38',ws)

# NY
#import pandas as pd
#import requests

def runNY(ws, write):
  url = 'https://raw.githubusercontent.com/nychealth/coronavirus-data/master/totals/probable-confirmed-by-race.csv'
  df_nyc_deaths = pd.read_csv(url)
  print('NYC Deaths by Race')
  display(df_nyc_deaths)


  url = 'https://raw.githubusercontent.com/nychealth/coronavirus-data/master/totals/by-race.csv'
  df_nyc_cases_hosp = pd.read_csv(url)
  print('\nNYC Cases and Hospitalizations by Race\n')
  display(df_nyc_cases_hosp)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('J25',dataToWrite)

    # Write Data To Sheet
    writeTable(df_nyc_deaths,'','J17',ws)
    writeTable(df_nyc_cases_hosp,'','O17',ws)

#OK
def runOK(ws, write):
  url = 'https://looker-dashboards.oklahoma.gov/embed/dashboards/75'
  url_deaths = 'https://looker-dashboards.oklahoma.gov/embed/dashboards/76'
  data = []
  ind = []

  wd = init_driver()
  wd.get(url)
  wd.maximize_window()
  wait = WebDriverWait(wd, 20)

  element = wait.until(EC.presence_of_element_located((By.CLASS_NAME, "highcharts-series-group")))
  print('Charts present')

  elements = wait.until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, "path.highcharts-point.highcharts-color-0")))
  print('Moving to first element')
  ActionChains(wd).move_to_element(elements[0]).move_by_offset(25,0).perform()
  time.sleep(1)

  values = wait.until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, "div[class='value']")))
  names  = wait.until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, "div[class='section value']")))
  ind.append(names[0].text)
  data.append(values[0].text)

  for i in range(1,6):
    elements = wait.until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, "path.highcharts-point.highcharts-color-"+str(i))))
    print('Moving to next element')
    hover = ActionChains(wd).move_to_element(elements[0]).perform()
    time.sleep(1)
  
    values = wait.until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, "div[class='value']")))
    names  = wait.until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, "div[class='section value']")))
    ind.append(names[0].text)
    data.append(values[0].text)

  df_cases = pd.DataFrame(data,index=ind,columns=['Cases']).reset_index()
  df_cases = df_cases.rename(columns={"index": "Category"})
  display(df_cases)

  data = []
  ind = []
  wd.get(url_deaths)
  wd.maximize_window()
  
  element = wait.until(EC.presence_of_element_located((By.CLASS_NAME, "highcharts-series-group")))
  print('Charts present')
  
  elements = wait.until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, "path.highcharts-point.highcharts-color-0")))
  print('Moving to first element')
  ActionChains(wd).move_to_element(elements[0]).move_by_offset(35,0).perform()
  time.sleep(1)
  
  values = wait.until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, "div[class='value']")))
  names  = wait.until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, "div[class='section value']")))
  ind.append(names[0].text)
  data.append(values[0].text)
  
  for i in range(1,6):
    elements = wait.until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, "path.highcharts-point.highcharts-color-"+str(i))))
    print('Moving to next element')
    hover = ActionChains(wd).move_to_element(elements[0]).perform()
    time.sleep(1)
  
    values = wait.until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, "div[class='value']")))
    names  = wait.until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, "div[class='section value']")))
    ind.append(names[0].text)
    data.append(values[0].text)
  
  df_deaths = pd.DataFrame(data,index=ind,columns=['Deaths']).reset_index()
  df_deaths = df_deaths.rename(columns={"index": "Category"})
  display(df_deaths)

  wd.quit()

  #Ethnicity
  url = 'https://oklahoma.gov/covid19/newsroom/weekly-epidemiology-and-surveillance-report.html'
  req = requests.get(url)
  soup = BeautifulSoup(req.text, 'html.parser')
  pdfs = soup.findAll('a', attrs={'href': re.compile('Report.pdf')})
  pdf_url = 'https://oklahoma.gov' + pdfs[0].get('href')
  display(pdf_url)
  
  tables = tabula.read_pdf(pdf_url,pages=12,multiple_tables = False)
  df_table = tables[0]
  for i in range(1,9,2):
    df_table = df_table.drop('Unnamed: '+str(i),axis=1)
  df_table = df_table[df_table.columns.drop(list(df_table.filter(regex='Cumulative')))]
  df_table.columns = ['Category','Cases','Deaths']
  df_table = df_table.loc[25:28].reset_index(drop=True)
  df_table = df_table.replace('\s+\([^)]*\)','',regex=True)
  display(df_table)

  #Template for writing to state page
  if write == True:
    # Write Data To Sheet
    writeTable(df_cases,'','',ws)
    writeTable(df_deaths,'','',ws)
    writeTable(df_table,'','',ws)

# OR
def runOR(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC

  def getCSV_OR(csv_file,first=1,contains='',header='infer'):
    wait = WebDriverWait(wd, 20)
    if first == 1:
      print('Waiting on frame...')
      wait.until(EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, "iframe[title='Data Visualization']")))
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ".tab-icon-download"))).click()
    print('Clicked download')
    wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Crosstab']"))).click()
    print('Clicked crosstab')
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "label[data-tb-test-id='crosstab-options-dialog-radio-csv-Label']"))).click()
    print('Clicked csv button')
    if contains == '':
      wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='" + csv_file + "']"))).click()
    else:
      wait.until(EC.element_to_be_clickable((By.XPATH, "//div[contains(@title," + contains + ")]")))
      #wait.until(EC.element_to_be_clickable((By.XPATH, "//div[contains(@title," + contains + ")]"))).click()
    print('Clicked sheet')
    wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Download']"))).click()
    print('Clicked download')
    time.sleep(5)
    return pd.read_csv(csv_file + '.csv',skiprows=1,sep="\t", encoding="utf-16",header=header)

  url = 'https://public.tableau.com/profile/oregon.health.authority.covid.19#!/vizhome/OregonCOVID-19CaseDemographicsandDiseaseSeverityStatewide-SummaryTable/DemographicDataSummaryTable'
  url_hosp_total = 'https://public.tableau.com/profile/oregon.health.authority.covid.19#!/vizhome/OregonHealthAuthorityCOVID-19SummaryTable_15889676399110/OregonsEpiCurveSummaryTable'

  deaths_csv = 'Demographic Data - Death Status'
  hosp_csv = 'Demographic Data - Hospitalizaton Status'
  epi_csv = "Oregon's Epi Curve Summary Table"

  wd = init_driver()
  wd.get(url)

  df = getCSV_OR(deaths_csv)
  df_cases_deaths = df[~df['Demographic'].str.contains('Group')]
  df_cases_deaths = df_cases_deaths.fillna(0)

  cols = df_cases_deaths.columns[2:5]
  df_cases_deaths.loc[:,cols] = df_cases_deaths.loc[:,cols].replace(',','', regex=True)
  df_cases_deaths[cols] = df_cases_deaths[cols].astype('int')

  df_cases_deaths.loc[24,cols] = df_cases_deaths[cols][7:9].sum()
  df_cases_deaths.drop(23,inplace=True)
  df_cases_deaths = df_cases_deaths.drop(df_cases_deaths.columns[[2]],axis=1)
  df_cases_deaths.rename(columns={"Total": "Cases"},inplace=True)
  display(df_cases_deaths)

  df = getCSV_OR(hosp_csv,0)

  df_hosp = df[~df['Demographic'].str.contains('Group')]
  df_hosp = df_hosp.drop(df_hosp.columns[[3,4]],axis=1)
  df_hosp.loc[:,'Hospitalized'] = df_hosp.loc[:,'Hospitalized'].replace(',','', regex=True)
  df_hosp['Hospitalized'] = df_hosp['Hospitalized'].astype('int')
  display(df_hosp)

  wd.quit()

  wd=init_driver()
  wd.get(url_hosp_total)
  df = getCSV_OR(epi_csv,1,"'Epi Curve Summary Table'",None)
  hosp_total = df.iloc[0,3]
  hosp_total = int(hosp_total.replace(',',''))
  print('hosp_total = ',hosp_total)

  wd.quit()

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('G21',dataToWrite)

    # Write Paste Date To Sheet
    dataToWrite = [[hosp_total]]
    #ws.update('J48',dataToWrite)

    # Write Data To Sheet
    writeTable(df_cases_deaths,'','H21',ws)
    writeTable(df_hosp,'','H36',ws)

# PA
def runPA(ws, write):

  #Totals
  urlTot='https://services1.arcgis.com/Nifc7wlHaBPig3Q3/arcgis/rest/services/COVID_PA_Counties/FeatureServer/0//query?where=1%3D1&objectIds=68&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&returnGeodetic=false&outFields=County%2CCases%2C+Confirmed%2C+Probable%2C+Deaths&returnGeometry=true&returnCentroid=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urlTot)
  df_tot=pd.json_normalize(req.json()['features'])
  df_tot=df_tot.rename(columns=lambda x:re.sub('properties.','',x))
  df_tots=df_tot.loc[:,['Cases','Confirmed','Probable','Deaths']]
  display(df_tots)

  #PA Cases
  print("\nPA Cases")
  urlRaceCases='https://services1.arcgis.com/Nifc7wlHaBPig3Q3/ArcGIS/rest/services/racedata/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=Positive_Cases%2C+Race&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urlRaceCases)
  df_Rcases=pd.json_normalize(req.json()['features'])
  df_Rcases=df_Rcases.rename(columns=lambda x: re.sub('properties.','',x))
  df_casesR=df_Rcases.loc[:,['Race','Positive_Cases']]
  display(df_casesR)

  urlEthCases='https://services1.arcgis.com/Nifc7wlHaBPig3Q3/ArcGIS/rest/services/ethniccases/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=Ethnicity%2C+Cases&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urlEthCases)
  df_Ecases=pd.json_normalize(req.json()['features'])
  df_Ecases=df_Ecases.rename(columns=lambda x: re.sub('properties.','',x))
  df_casesE=df_Ecases.loc[:,['Ethnicity','Cases']]
  print("\nBy Ethnicity")
  display(df_casesE)

  #PA Deaths
  print("PA Deaths")
  urlEthDeath='https://services1.arcgis.com/Nifc7wlHaBPig3Q3/ArcGIS/rest/services/Death_Ethnicity/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=Ethnicity+%2CF__of_Deaths&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  urlRaceDeath='https://services1.arcgis.com/Nifc7wlHaBPig3Q3/ArcGIS/rest/services/Death_Race/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=Race%2C+Deaths&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urlRaceDeath)
  df_Rdeath=pd.json_normalize(req.json()['features'])
  df_Rdeath=df_Rdeath.rename(columns=lambda x: re.sub('properties.','',x))
  df_deathR=df_Rdeath.loc[:,['Race','Deaths']]
  print("\nby Race")
  display(df_deathR)

  req=requests.get(urlEthDeath)
  df_Edeath=pd.json_normalize(req.json()['features'])
  df_Edeath=df_Edeath.rename(columns=lambda x: re.sub('properties.','',x))
  df_deathE=df_Edeath.loc[:,['Ethnicity','F__of_Deaths']]

  print("\nby Ethnicity")
  display(df_deathE)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('H16',dataToWrite)

    # Write Data To Sheet
    writeTable(df_casesR, 'Cases by Race', 'H25',ws)
    writeTable(df_casesE,'Cases by Ethnicity','H34',ws)
    writeTable(df_deathR,'Deaths by Race','K25',ws)
    writeTable(df_deathE,'Deaths by Ethnicity','K34',ws)
    writeTable(df_tots,'Totals', 'H17',ws)

# RI

def runRI(ws,write):

#  key_ri = '1c2QrNMz8pIbYEKzMJL7Uh2dtThOJa2j1sSMwiDo5Gz4'
#  try:
#    wb_ri = gc.open_by_key(key_ri)
#  except NameError:
#    auth.authenticate_user()
#    gc = gspread.oauth()
#    gc = gspread.authorize(GoogleCredentials.get_application_default())
#    wb_ri = gc.open_by_key(key_ri)

  # Get Totals from Summary tab, discard unused rows
  summary_url = 'https://docs.google.com/spreadsheets/d/1c2QrNMz8pIbYEKzMJL7Uh2dtThOJa2j1sSMwiDo5Gz4/gviz/tq?tqx=out:csv&sheet=Summary'
  df_totals = pd.read_csv(summary_url)
  df_totals = df_totals.iloc[[9,12,14,24],:]
  display(df_totals)

  # Get race info from Demographics tab, wrangle data
  demo_url = 'https://docs.google.com/spreadsheets/d/1c2QrNMz8pIbYEKzMJL7Uh2dtThOJa2j1sSMwiDo5Gz4/gviz/tq?tqx=out:csv&sheet=Demographics'
  df_demo= pd.read_csv(demo_url,header=None)
  df_demo.drop(df_demo.columns[[2, 4, 6, 8,9,10,11,12,13,14]], axis = 1, inplace = True)
  header = df_demo.T[0].str.split(' N=', expand=True)
  df_demo = header.T.append(df_demo.iloc[1:,:])
  df_demo.reset_index(drop=True,inplace=True)
  df_demo.iloc[0:2,0]=['','Totals']
  df_demo.columns = df_demo.iloc[0,:].replace({'\n': ''}, regex=True)
  totals = df_demo.iloc[[1]]
  df_demo = totals.append(df_demo.iloc[16:27,:])
  df_demo = df_demo.replace({'<5': '1'}, regex=True)
  df_demo = df_demo.fillna('')

  cols = list(df_demo.columns)
  cols = [s for s in cols if s != '']
  df_demo[cols] = df_demo[cols].replace(',','',regex=True)
  df_demo[cols] = df_demo[cols].apply(pd.to_numeric,errors='coerce')
  df_demo = df_demo.fillna('0')
  df_demo[cols] = df_demo[cols].astype('int')
  cols = list(df_demo.columns)
  cols[0] = 'Category'
  df_demo.columns = cols

  # Add excess to Unknowns
  unknown_row = df_demo.index[df_demo['Category'] == 'Unknown or pending further information']
  tot_row = df_demo.index[df_demo['Category'] == 'Totals']
  tot = df_demo['Deaths'][tot_row[0]]
  subtot = df_demo['Deaths'][(tot_row[0]+1):(len(df_demo['Deaths'])-1)].sum()
  df_demo.loc[unknown_row[0],'Deaths'] = tot - subtot
  df_demo = df_demo.iloc[:,0:5]
  display(df_demo)

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('R32',dataToWrite)

    # Write Data To Sheet
    writeTable(df_totals,'Summary','R19',ws)
    writeTable(df_demo,'Demographics','T19',ws)

# SD
def runSD(ws,write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains

  url = 'https://app.powerbigov.us/view?r=eyJrIjoiMWJkMzQ2ZmUtNmExNC00MTJiLWIzOTktNDViNDY4MzVhODU1IiwidCI6IjcwYWY1NDdjLTY5YWItNDE2ZC1iNGE2LTU0M2I1Y2U1MmI5OSJ9'
#  url = 'https://app.powerbigov.us/view?r=eyJrIjoiZGJjZWYwZmEtMWVjMy00OTUwLThkMzgtZDhkNzAwOWQ3YzNlIiwidCI6IjcwYWY1NDdjLTY5YWItNDE2ZC1iNGE2LTU0M2I1Y2U1MmI5OSJ9'
#  url = 'https://app.powerbigov.us/view?r=eyJrIjoiZDUwODIyNGEtODdkZC00MmI4LWFmOTctZWJjOWRkYmIzNzhhIiwidCI6IjcwYWY1NDdjLTY5YWItNDE2ZC1iNGE2LTU0M2I1Y2U1MmI5OSJ9'

  def getdf_SD(category):
    wd = init_driver()
    wd.get(url)
    wait = WebDriverWait(wd, 60)
    print('Got Website')

    time.sleep(6)
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "i[title='Next Page']"))).click()
    print('Clicked Next Page')
    time.sleep(4)
    if category != 'Cases':
      wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@aria-label='" + category + "']/div"))).click()
      print('Clicked metric')
    else:
      time.sleep(1)
  #  time.sleep(10)
    elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "g[class='axisGraphicsContext columnChart']")))
    ActionChains(wd).context_click(elements[0]).perform()
  #  time.sleep(10)
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='Show as a table']"))).click()
    print('Show table')

    cats = []
    elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive ']")))
    for element in elements:
      cats.append(element.text)

    vals = []
    elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive tablixAlignRight ']")))
    for element in elements:
      vals.append(element.text)

    df = pd.DataFrame([cats[-7:],vals[-7*5:-7*4],vals[-7*3:-7*2],vals[-7:]]).T
    df.columns = ['Category', 'Hispanic_' + category, 'Non-Hispanic_' + category, 'Unknown_' + category]
    df = df.set_index('Category')
    df = df.replace(',','', regex=True)
    df = df.replace(r'^\s*$','0', regex=True)
    df = df.astype('int').reset_index()
    display(df)
    #colstr2int(df,category)

    wd.quit()

    return df

  df_cases = getdf_SD('Cases')
  df_deaths = getdf_SD('Deceased Among Cases')
  df_hosp = getdf_SD('Ever Hospitalized')

  df = df_cases.merge(df_deaths,how='left',on='Category')
  df = df.merge(df_hosp,how='left',on='Category')
  df.sort_values(by=['Category'], ascending=True, inplace=True)
  display(df)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    ##ws.update('J19',dataToWrite)

    # Write Data To Sheet
    writeTable(df,'','I20',ws)

# TN
def runTN(ws, write):
  url = 'https://apps.health.tn.gov/AEM_embed/TDH-2019-Novel-Coronavirus-Epi-and-Surveillance.pdf#toolbar=0'
  req = requests.get(url)
  pdf = BytesIO(req.content)
  first_page = PyPDF2.PdfFileReader(pdf)

  tables = tabula.read_pdf(url,pages=1,multiple_tables=True)
  display(tables)

  df = pd.DataFrame(tables[1].iloc[18:29,0:4])
  display(df)
  temp = df.iloc[:,0].str.split(r'^(.*) (\d+,*\d+)$', expand = True)
  temp.columns = ['','Race','Cases','']
  temp = temp.drop(columns='')
  temp2 = df.iloc[:,3].str.split(r'(^\d*,*\d+)', expand = True)
  temp2.columns = ['','Deaths','']
  temp2 = temp2.drop(columns='')
  temp2.iloc[-3:,0] = df.iloc[-3:,2]
  df = temp.join(temp2)
  df = df.replace([None],'---')
  df.iloc[7,0] = 'Ethnicity'
  df.replace(',','', regex=True, inplace=True)
  df.iloc[np.r_[0:7, 8:11],[1,2]] = df.iloc[np.r_[0:7, 8:11],[1,2]].astype('int')
  display(df)


  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('K20',dataToWrite)

    # Write Data To Sheet
    writeTable(df,'','J21',ws)

# TX
#import pandas as pd
#runTX(ws)
def runTX(ws,write):

  url_tots = 'https://dshs.texas.gov/coronavirus/TexasCOVID19CaseCountData.xlsx'
  #url = 'https://dshs.texas.gov/coronavirus/TexasCOVID19Demographics.xlsx.asp'
  url = 'https://dshs.texas.gov/coronavirus/TexasCOVID19Demographics.xlsx'
  df_trends = pd.read_excel(url_tots, sheet_name='Trends', skiprows=3, engine='openpyxl', parse_dates=['Date'])
  today = df_trends['Date'].max()
  yesterday = today - timedelta(1)
  conf_cases = df_trends.loc[df_trends['Date']==today]['Cumulative Confirmed Cases'].values[0]
  prob_cases = df_trends.loc[df_trends['Date']==today]['Cumulative Probable Cases'].values[0]
  deaths = df_trends.loc[df_trends['Date']==yesterday]['Cumulative Fatalities'].values[0]
  cases = int(conf_cases) + int(prob_cases)
  deaths = int(deaths)

  df_cases = pd.read_excel(url, sheet_name='Cases by RaceEthnicity', skiprows=0, engine='openpyxl')
  df_cases.loc[len(df_cases.index)]=['Overall Total',cases,0]
  print("Cases by Race")
  display(df_cases)

  df_deaths = pd.read_excel(url, sheet_name='Fatalities by Race-Ethnicity', skiprows=0, engine='openpyxl')
  df_deaths.loc[len(df_deaths.index)]=['Overall Total',deaths,0]
  print("\nDeaths by Race")
  display(df_deaths)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('F31',dataToWrite)

    # Write Data To Sheet
    writeTable(df_cases,'Cases by Race','H20',ws)
    writeTable(df_deaths,'Deaths by Race','H29',ws)

# UT
def runUT(ws, write):
  url = 'https://coronavirus-dashboard.utah.gov/demographics.html'
  wd = init_driver()
  wd.get(url)
  #time.sleep(20)
  soup = BeautifulSoup(wd.page_source, 'html.parser')

  # Find Table
  t_1 = soup.find_all('table',{"id": "DataTables_Table_1"})
  t_2 = soup.find_all('table',{"id": "DataTables_Table_2"})

  table_body = t_1[0].find('tbody')
  rows = table_body.find_all('tr')
  data = []
  col_names = ['Category','','Cases','','','Hospitalizations','','Deaths','','']
  for row in rows:
      cols = row.find_all('td')
      cols = [ele.text.strip() for ele in cols]
      data.append([ele for ele in cols if ele]) # Get rid of empty values
  df = pd.DataFrame(data, columns=col_names)
  df.drop(df[''], axis = 1, inplace = True)
  df = df.astype({'Cases': 'int64', 'Hospitalizations': 'int64', 'Deaths': 'int64'})

  table_body = t_2[0].find('tbody')
  rows = table_body.find_all('tr')
  data = []
  col_names = ['Category','','Persons Tested','','','','','']
  for row in rows:
      cols = row.find_all('td')
      cols = [ele.text.strip() for ele in cols]
      data.append([ele for ele in cols if ele]) # Get rid of empty values

  df_tests = pd.DataFrame(data, columns=col_names)
  df_tests.drop(df_tests[''], axis = 1, inplace = True)
  df_tests.iloc[:,1] = df_tests.iloc[:,1].astype('int')

  display(df)
  display(df_tests)

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('G34',dataToWrite)

    # Write Data To Sheet
    writeTable(df,'','H17',ws)
    writeTable(df_tests,'','H34',ws)

# VA
#import pandas as pd
# import requests

def runVA(ws, write):
  # VA Totals
  url = 'https://data.virginia.gov/api/views/bre9-aqqr/rows.csv?accessType=DOWNLOAD'
  df_totals = pd.read_csv(url,parse_dates=['Report Date'])
  maxdateTot = df_totals ['Report Date'].max()
  print('\nVA Case, Death and Hospitalizations Totals')
  print(maxdateTot,'\n')
  #df_cases = df_totals.groupby(['Report Date'])[['totalcountconfirmed','totalcountdeaths']].sum()
  df_tots = df_totals[df_totals['Report Date'] == maxdateTot]
  df_tots = df_tots.groupby(['Report Date'])[['Total Cases','Hospitalizations','Deaths']].sum()
  df_tots = df_tots.replace(',','', regex=True).apply(pd.to_numeric)
  display(df_tots)


  # VA Case, Deaths, Hosptializations Confirmed & Probables"
  url = 'https://data.virginia.gov/api/views/uqs3-x7zh/rows.csv?accessType=DOWNLOAD'
  df_totals = pd.read_csv(url,parse_dates=['Report Date'])
  maxdateTot = df_totals ['Report Date'].max()
  print('\nVA Case, Death and Hospitalizations Totals')
  print(maxdateTot,'\n')
  df_confProb = df_totals[df_totals['Report Date'] == maxdateTot]
  df_confProb = df_confProb.fillna('0')
  df_confProb['Number of Cases'] = df_confProb['Number of Cases'].replace(',','', regex=True).apply(pd.to_numeric)
  df_confProb['Number of Hospitalizations'] = df_confProb['Number of Hospitalizations'].replace(',','', regex=True).apply(pd.to_numeric)
  df_confProb['Number of Deaths'] = df_confProb['Number of Deaths'].replace(',','', regex=True).apply(pd.to_numeric)

  display(df_confProb)

  # VA Race & Ethnicity
  url='https://data.virginia.gov/api/views/9sba-m86n/rows.csv?accessType=DOWNLOAD'
  df_raceeth = pd.read_csv(url,parse_dates=['Report Date'])

  print ('\nVA Race and Ethnicity Totals')
  maxdateRace = df_raceeth ['Report Date'].max()
  print(maxdateRace,'\n')
  df_dem = df_raceeth[df_raceeth['Report Date'] == maxdateRace]
  df_dem = df_dem.groupby('Race and Ethnicity').sum()
  df_dem = df_dem.fillna('0')
  df_dem['Race/Ethnicity']=['Asian or Pacific Islander', 'Black', 'Latino', 'Native American',
       'Not Reported', 'Other Race', 'Two or more races', 'White']
  df_dem['Number of Cases'] = df_dem['Number of Cases'].replace(',','', regex=True).apply(pd.to_numeric)
  df_dem['Number of Hospitalizations'] = df_dem['Number of Hospitalizations'].replace(',','', regex=True).apply(pd.to_numeric)
  df_dem['Number of Deaths'] = df_dem['Number of Deaths'].replace(',','', regex=True).apply(pd.to_numeric)

  display(df_dem)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('B33',dataToWrite)
    #ws.update('J14',dataToWrite)

    # Write Data To Sheet
    writeTable(df_confProb,'','K15',ws)
    writeTable(df_tots,'','K19',ws)
    writeTable(df_dem,'','C33',ws)

# VT
def runVT(ws, write):
  # Internet Explorer version of the dashboard
  url = 'https://vcgi.maps.arcgis.com/apps/opsdashboard/index.html#/25e8ef0453e347a8b216ddf98a25a040'
  wd = init_driver()
  wd.get(url)
  time.sleep(20)
  soup = BeautifulSoup(wd.page_source, 'html.parser')
  texts = soup.find_all('text',style=re.compile('^fill'))
  cases = texts[1].text
  deaths = int(texts[9].text)
  cases = int(cases.replace(' Total','').replace(',',''))

  # Find Totals
  g_all = soup.find_all('g',{"class": "amcharts-pie-item"})

  pattern = re.compile(r'aria-label=\"(.+) \"')
  demo = []
  for i in range(len(g_all)):
    cat = re.findall(pattern,str(g_all[i]))[0].split()
    lcat = len(cat)
    if lcat > 3:
      for j in range(1,lcat-2):
        cat[0] = cat[0] + '_' + cat[j]
      cat[1:3] = cat[lcat-2:lcat]
    demo.append(cat)

  df_demo = pd.DataFrame(demo).loc[:,[0,2]]
  df_demo.iloc[:,1].replace(',','', regex=True, inplace=True)
  df_demo.iloc[:,1] = df_demo.iloc[:,1].astype('int')

  df_demo_cases = df_demo.loc[5:11,:].copy().reset_index(drop=True)
  df_demo_cases.columns = ['Category','Cases']
  df_demo_cases.loc[len(df_demo_cases.index)]=['Total',cases]

  df_demo_deaths = df_demo.loc[15:21,:].copy().reset_index(drop=True)
#  df_demo.loc[14] = ['Hispanic:', 0]
  df_demo_deaths.columns = ['Category','Deaths']
  df_demo_deaths.loc[len(df_demo_deaths.index)]=['Total',deaths]

  display(df_demo_cases)
  display(df_demo_deaths)

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('I19',dataToWrite)

    # Write Data To Sheet
    writeTable(df_demo_cases,'','H20',ws)
    writeTable(df_demo_deaths,'','K20',ws)

# WA
def runWA(ws,write):
  def parse_table(t,col_names):
    table_body = t.find('tbody')
    rows = table_body.find_all('tr')
    data = []
    col_names.extend(['',''])
    for row in rows:
        cols = row.find_all('td')
        cols = [ele.text.strip() for ele in cols]
        data.append([ele for ele in cols if ele]) # Get rid of empty values
    df = pd.DataFrame(data, columns=col_names)
    df.drop(df.columns[[2,3]], axis = 1, inplace = True)
    df.iloc[:,1].replace(',','', regex=True, inplace=True)
    df.iloc[:,1] = df.iloc[:,1].astype('int')
    display(df)
    return df

  url = 'https://www.doh.wa.gov/Emergencies/COVID19/DataDashboard#tables'
  wd = init_driver()
  wd.get(url)
  time.sleep(10)
  soup = BeautifulSoup(wd.page_source, 'html.parser')

  # Find Tables
  t_all = soup.find_all('table',{"class": "table table-bordered table-striped"})

  for t in t_all:
    if 'Cumulative COVID-19 Cases by Race/Ethnicity' in t.text:
      cases_table = t
    if 'Cumulative COVID-19 Hospitalizations by Race/Ethnicity' in t.text:
      hosp_table = t
    if 'Cumulative COVID-19 Deaths by Race/Ethnicity' in t.text:
      deaths_table = t

  df_cases = parse_table(cases_table,['Category','Cases'])
  df_hosp = parse_table(hosp_table,['Category','Hospitalizations'])
  df_deaths = parse_table(deaths_table,['Category','Deaths'])

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('B33',dataToWrite)

    # Write Data To Sheet
    writeTable(df_cases,'','A35',ws)
    writeTable(df_hosp,'','G35',ws)
    writeTable(df_deaths,'','D35',ws)

#WI
#from io import StringIO, BytesIO
#from bs4 import BeautifulSoup
#import pandas as pd
def runWI(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC

  url = 'https://opendata.arcgis.com/datasets/859602b5d427456f821de3830f89301c_11.csv?outSR=%7B%22latestWkid%22%3A3857%2C%22wkid%22%3A102100%7D'

  #TOTALS
  df_cases = pd.read_csv(url,encoding='utf-8')
  maxdate = df_cases['DATE'].max()
  print("Confirmed Case & Death Totals")
  print(maxdate,'\n')
  df_cases = df_cases[df_cases['DATE'] == maxdate]
  #display(df_cases)
  df_totals = df_cases.groupby(['DATE'])[['POSITIVE','DEATHS']].sum()
  display(df_totals)

  #DEMOGRAPHICS
  #urls
  deaths_url="https://www.dhs.wisconsin.gov/covid-19/deaths.htm"
  cases_url="https://www.dhs.wisconsin.gov/covid-19/cases.htm"

  #target strings to tableau view
  cases_string = "casesby-group"
  deaths_string = "deathsby-group"

  #xpaths
  #options on chart
  confirm_radio_xpath = "//*[@id='[Parameters].[Case status Parameter]_0']/div[2]/input"
  prob_radio_xpath = "//*[@id='[Parameters].[Case status Parameter]_1']/div[2]/input"
  race_radio_xpath = "//*[@id='[Parameters].[Select a subgroup (copy)]_2']/div[2]/input"
  eth_radio_xpath = "//*[@id='[Parameters].[Select a subgroup (copy)]_3']/div[2]/input"
  main_download_btn_xpath = "//*[@id='download-ToolbarButton']"
  #download form options
  #crosstab_btn_xpath = "//*[@id='DownloadDialog-Dialog-Body-Id']/div/button[3]"
  crosstab_btn_xpath = "//button[text()='Crosstab']"
  csv_radio_xpath = "//*[@id='export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id']/div/div[2]/div[2]/div/label[2]"
  fin_dwnld_xpath = "//*[@id='export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id']/div/div[3]/button"

  #downloaded csv paths
  #csv_cases = "/content/Case chart.csv"
  csv_cases = "Case chart.csv"
  #csv_deaths = "/content/Death chart.csv"
  csv_deaths = "Death chart.csv"

  #visit metric url and go to tableau src page
  def visitNextURL(url, target_string):
    wd.get(url)
    wait = WebDriverWait(wd, 60)
    print('waiting')
    time.sleep(20)
    soup = BeautifulSoup(wd.page_source, "html.parser")
    iframes = soup.find_all("iframe")
    #cases_src = [tag["src"] for tag in iframes if target_string in tag["src"]]
    cases_src = []
    for tag in iframes:
        try:
            if target_string in tag["src"]:
                cases_src.append(tag["src"])
        except Exception as e:
            print('Got Exception')
    target_src = cases_src[0]
    print("Loading new URL")
    wd.get(target_src)
    #time.sleep(3)

  #download view and convert to df
  def getCSV(status_choice, demo_choice, metric_csv):
    wait = WebDriverWait(wd, 20)
    #choose confirmed/probable on chart
    wait.until(EC.presence_of_element_located((By.XPATH, status_choice))).click()
    print("clicked case status")
    time.sleep(1)
    #choose demographic on chart
    wait.until(EC.presence_of_element_located((By.XPATH, demo_choice))).click()
    print("clicked demographic")
    time.sleep(1)

    #click download button at button of tableau to open download dialog box
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ".tab-icon-download"))).click()
    print("clicked download button")

    #click crosstab option
    wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Crosstab']"))).click()
    print("clicked crosstab button")

    #click csv option
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "label[data-tb-test-id='crosstab-options-dialog-radio-csv-Label']"))).click()
    print("clicked csv option")

    #download csv
    wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Download']"))).click()
    print("clicked download button")
    time.sleep(2) #wait for download

    #make df
    df = pd.read_csv(metric_csv, sep="\t", encoding="utf-16") #dumb tableau encoding
    return df

  #start webdriver
  wd = init_driver()
  visitNextURL(cases_url, cases_string)

  #CONFIRMED CASES
  #by race
  print("-" * 10)
  print("CONFIRMED CASES:")
  df_casesConfirmRace = getCSV(confirm_radio_xpath, race_radio_xpath, csv_cases)
  df_casesConfirmRace['Number of cases'] = df_casesConfirmRace['Number of cases'].str.replace(',', '').astype('int')
  df_casesConfirmRace.rename(columns={list(df_casesConfirmRace)[0]:'Demographic'}, inplace=True)
  df_casesConfirmRace = df_casesConfirmRace[['Demographic','Number of cases']]
  display(df_casesConfirmRace)
  #by ethnicity
  df_casesConfirmEth = getCSV(confirm_radio_xpath,eth_radio_xpath, csv_cases)
  df_casesConfirmEth['Number of cases'] = df_casesConfirmEth['Number of cases'].str.replace(',', '').astype('int')
  df_casesConfirmEth.rename(columns={list(df_casesConfirmEth)[0]:'Demographic'}, inplace=True)
  df_casesConfirmEth = df_casesConfirmEth[['Demographic','Number of cases']]
  display(df_casesConfirmEth)
  print("-" * 10)
  print("PROBABLE CASES:")
  #PROBABLE CASES
  #by race
  df_casesProbsRace = getCSV(prob_radio_xpath, race_radio_xpath, csv_cases)
  df_casesProbsRace['Number of cases'] = df_casesProbsRace['Number of cases'].str.replace(',', '').astype('int')
  df_casesProbsRace.rename(columns={list(df_casesProbsRace)[0]:'Demographic'}, inplace=True)
  df_casesProbsRace = df_casesProbsRace[['Demographic','Number of cases']]
  display(df_casesProbsRace)
  #by ethnicity
  df_casesProbsEth = getCSV(prob_radio_xpath, eth_radio_xpath, csv_cases)
  df_casesProbsEth['Number of cases'] = df_casesProbsEth['Number of cases'].str.replace(',', '').astype('int')
  df_casesProbsEth.rename(columns={list(df_casesProbsEth)[0]:'Demographic'}, inplace=True)
  df_casesProbsEth = df_casesProbsEth[['Demographic','Number of cases']]
  display(df_casesProbsEth)

  #reset driver
  wd.quit()

  #start webdriver
  wd = init_driver()
  visitNextURL(deaths_url, deaths_string)

  print("-" * 10)
  print("CONFIRMED DEATHS:")
  #CONFIRMED DEATHS
  #by race
  df_deathsConfirmRace = getCSV(confirm_radio_xpath, race_radio_xpath, csv_deaths)
  df_deathsConfirmRace['Deaths'] = df_deathsConfirmRace['Deaths'].str.replace(',', '').astype('int')
  df_deathsConfirmRace.rename(columns={list(df_deathsConfirmRace)[0]:'Demographic'}, inplace=True)
  df_deathsConfirmRace = df_deathsConfirmRace[['Demographic','Deaths']]
  display(df_deathsConfirmRace)
  #by ethnicity
  df_deathsConfirmEth = getCSV(confirm_radio_xpath, eth_radio_xpath, csv_deaths)
  df_deathsConfirmEth['Deaths'] = df_deathsConfirmEth['Deaths'].str.replace(',', '').astype('int')
  df_deathsConfirmEth.rename(columns={list(df_deathsConfirmEth)[0]:'Demographic'}, inplace=True)
  df_deathsConfirmEth = df_deathsConfirmEth[['Demographic','Deaths']]
  display(df_deathsConfirmEth)

  print("-" * 10)
  print("PROBABLE DEATHS:")
  #PROBABLE DEATHS
  #by race
  df_deathsProbsRace = getCSV(prob_radio_xpath, race_radio_xpath, csv_deaths)
  df_deathsProbsRace.rename(columns={list(df_deathsProbsRace)[0]:'Demographic'}, inplace=True)
  df_deathsProbsRace = df_deathsProbsRace[['Demographic','Deaths']]
  display(df_deathsProbsRace)
  #by ethnicity
  df_deathsProbsEth = getCSV(prob_radio_xpath, eth_radio_xpath, csv_deaths)
  df_deathsProbsEth.rename(columns={list(df_deathsProbsEth)[0]:'Demographic'}, inplace=True)
  df_deathsProbsEth = df_deathsProbsEth[['Demographic','Deaths']]
  display(df_deathsProbsEth)

  #reset driver
  wd.quit()

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('J15',dataToWrite)

    # Write Data To Sheet
    writeTable(df_totals,'Confirmed Case & Death Totals','L15',ws)

    writeTable(df_casesConfirmRace, 'Confirmed Cases by Race', 'K19',ws)
    writeTable(df_casesConfirmEth, 'Confirmed Cases by Ethnicity', 'K28',ws)
    writeTable(df_casesProbsRace, 'Probable Cases by Race', 'N19',ws)
    writeTable(df_casesProbsEth, 'Probable Cases by Ethnicity', 'N28',ws)

    writeTable(df_deathsConfirmRace, 'Confirmed Deaths by Race', 'K34',ws)
    writeTable(df_deathsConfirmEth, 'Confirmed Deaths by Ethnicity', 'K43',ws)
    writeTable(df_deathsProbsRace, 'Probable Deaths by Race', 'N34',ws)
    writeTable(df_deathsProbsEth, 'Probable Deaths by Ethnicity', 'N43',ws)


# WV
def runWV(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains

  def get_elements():
    elements = []
    while len(elements)<7:
      elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "tspan[x='0'],[y='0']")))
      time.sleep(1)
    return elements


  #url = 'https://app.powerbigov.us/view?r=eyJrIjoiNDAwZjU3ZTAtMWM3OS00M2NjLWFiMGMtOTYwYjdmYTAwMGZjIiwidCI6IjhhMjZjZjAyLTQzNGEtNDMxZS04Y2FkLTdlYWVmOTdlZjQ4NCJ9'
  url = 'https://dhhr.wv.gov/COVID-19/Pages/default.aspx'

  wd=init_driver()
  wd.get(url)
  soup = BeautifulSoup(wd.page_source, "html.parser")
  url = soup.find("iframe",src=re.compile("app.powerbigov.us"))['src']
  wd.get(url)
  wd.maximize_window()
  wait = WebDriverWait(wd, 160)
  data = []
  cats = ['White','Unknown','Black','Other']
  time.sleep(10)
  print('Clicking Cumulative Summary')
  wait.until(EC.element_to_be_clickable((By.XPATH, "//*[text()[contains(.,'Cumulative Summary')]]/parent::*"))).click()
#  time.sleep(5)
  print('Getting Totals')
  elements = get_elements()
  data_row = ['Total',elements[0].text,elements[1].text,elements[3].text]
  data.append(data_row)
  for i in range(1,5):
    print('Getting',cats[i-1])
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "g > rect:nth-child("+str(i)+")"))).click()
    time.sleep(5)
    elements = get_elements()
    data_row = [cats[i-1],elements[0].text,elements[1].text,elements[3].text]
    #display(data_row)
    data.append(data_row)
  wd.quit()

  df = pd.DataFrame(data,columns=['Category','Confirmed','Probable','Deaths'])

  display(df)

  if write == 1:
    # Write Data To Sheet
    writeTable(df,'','',ws)

# WY ************

def runWY(ws,write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  #url = 'https://public.tableau.com/vizql/w/EpiCOVIDtest/v/Dashboard/tempfile/sessions/324E4F03F0184305838EC00E7A01CB86-0:0/?key=4098009697&keepfile=yes&attachment=yes'
  srcCases="https://public.tableau.com/views/EpiCOVIDtest/Dashboard?:embed=y&amp;:showVizHome=no&amp;:host_url=https%3A%2F%2Fpublic.tableau.com%2F&amp;:embed_code_version=3&amp;:tabs=no&amp;:toolbar=no&amp;:animate_transition=yes&amp;:display_static_image=no&amp;:display_spinner=no&amp;:display_overlay=yes&amp;:display_count=yes&amp;publish=yes&amp;:loadOrderID=0"
  srcDeaths="https://public.tableau.com/views/EpiCOVIDtest/COVID-19RelatedDeaths?%3Aembed=y&amp%3B%3AshowVizHome=no&amp%3B%3Ahost_url=https%3A%2F%2Fpublic.tableau.com%2F&amp%3B%3Aembed_code_version=3&amp%3B%3Atabs=no&amp%3B%3Atoolbar=no&amp%3B%3Aanimate_transition=yes&amp%3B%3Adisplay_static_image=no&amp%3B%3Adisplay_spinner=no&amp%3B%3Adisplay_overlay=yes&amp%3B%3Adisplay_count=yes&amp%3Bpublish=yes&amp%3B%3AloadOrderID=0"

  #download form options
  data_btn_xpath ='//*[@id="download-ToolbarButton"]/span[1]'

  #xpaths from tableau
  crosstab_xpath='//*[@id="DownloadDialog-Dialog-Body-Id"]/div/fieldset/button[3]'
  csv_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[2]/div[2]/div/label[2]'
  cases_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[4]/div/div/div[1]/div'
  deaths_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[5]/div/div/div'
  probables_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[10]/div/div/div'
  raceth_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[11]/div/div/div'
  racethdeath_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[5]/div/div/div'
  #download button for getting the file
  dnld_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[3]/button'

  #file csvs
  csv_cases = "cases.csv"
  csv_deaths = "death.csv"
  csv_probables = 'probable cases.csv'
  csv_raceth = 'raceth.csv'
  csv_racethdeath = 'raceth (death).csv'

  #press download button
  def getCSV(metric_xpath,csv_metric):

      #wait = WebDriverWait(wd, 20)
      interval=30

      retry_wait_click_all(wd, interval, 'css', ".tab-icon-download")
      #click download button at button of tableau to open download dialog box 
      #wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ".tab-icon-download"))).click()
      print("clicked download button")

      #click crosstab option
      retry_wait_click_all(wd, interval, 'xpath',"//button[text()='Crosstab']")
      #wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Crosstab']"))).click()
      print("clicked crosstab button")

      #click csv option
      retry_wait_click_all(wd, interval, 'css', "label[data-tb-test-id='crosstab-options-dialog-radio-csv-Label']")
      #wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "label[data-tb-test-id='crosstab-options-dialog-radio-csv-Label']"))).click()
      print("clicked csv option")

      #select metric
      retry_wait_click_all(wd, interval, 'xpath',metric_xpath)
      #wait.until(EC.element_to_be_clickable((By.XPATH, metric_xpath))).click()
      print("clicked metric")

      #download csv
      retry_wait_click_all(wd, interval, 'xpath',"//button[text()='Download']")
      #wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Download']"))).click()
      print("clicked download button")
      time.sleep(2) #wait for download

      #make df
      df=pd.read_csv(csv_metric,sep="\t", encoding="utf-16")
      df=df.fillna('0')
      return df

  #download view and convert to df
  #cases
  wd=init_driver()
  wd.get(srcCases)
  time.sleep(3)
  print("\nCases, Deaths, Probables, Raceth")
  print(srcCases)
  #Cases
  print("-" * 10)
  print("Cases")
  df_cases=getCSV(cases_xpath,csv_cases)
  #convert to integer
  df_cases['Laboratory Confirmed Cases'] = df_cases['Laboratory Confirmed Cases'].str.replace(r",",'', 1)
  df_cases = df_cases.astype({"Laboratory Confirmed Cases": int})
  display(df_cases)
  time.sleep(3)
  #Deaths
  print("-" * 10)
  print("Deaths")
  df_deaths=getCSV(deaths_xpath,csv_deaths)
  display(df_deaths)
  time.sleep(3)
  #Probables
  print("-" * 10)
  print("Probables")
  df_probables=getCSV(probables_xpath,csv_probables)
  df_probables['Probable Cases'] = df_probables['Probable Cases'].str.replace(r",",'', 1).astype('float')
  display(df_probables)
  time.sleep(3)
  #Race Ethnicity for Cases
  print("-" * 10)
  print("Race Ethnicity for Cases")
  df_raceth=getCSV(raceth_xpath,csv_raceth)
  tcols=list(df_raceth.columns)
  tcols=['Race','Count','%']
  df_raceth.columns=tcols
  df_raceth['Count']= df_raceth['Count'].str.replace(r",",'',1)
  df_raceth['Count'] = df_raceth['Count'].astype(int)
  display(df_raceth)
  wd.quit()

  wd=init_driver()
  wd.get(srcDeaths)
  print("\nCases, Deaths, Probables, Raceth")
  print(srcDeaths)
  time.sleep(3)
  #Race Ethnicity for Deaths
  print("-" * 10)
  print("Race Ethnicity for Deaths")
  df_racethdeath=getCSV(racethdeath_xpath,csv_racethdeath)
  tcols=list(df_racethdeath.columns)
  tcols=['Race','Count','%']
  df_racethdeath.columns=tcols
  display(df_racethdeath)
  display(df_probables)
  wd.quit()

  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('G16',dataToWrite)

      # Write Data To Sheet
      writeTable(df_cases,'Case Totals','M17',ws)
      writeTable(df_probables,'Probable Totals','M20',ws)
      writeTable(df_deaths,'Death Totals','M32',ws)
      writeTable(df_raceth,'Cases by Race & Ethnicity','H17',ws)
      writeTable(df_racethdeath,'Deaths by Race & Ethnicity','H32',ws)

