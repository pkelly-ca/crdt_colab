# -*- coding: utf-8 -*-
"""[CTP] CRDT Long Calculations (Official)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CExjy3Af3guUTKub0mVIVdCZKQgj3h4L
"""

# Necessary Libraries
import pandas as pd
import requests
import numpy as np
import urllib
import inspect
from urllib.parse import unquote
from io import StringIO, BytesIO
from bs4 import BeautifulSoup
import re
import zipfile
import datetime
from datetime import date, timedelta
import PyPDF2
import tabula
import auth
import gspread
from IPython.display import display
import os, time
os.environ['TZ'] = 'America/New_York'
time.tzset()
import json
from selenium import webdriver
import inspect

# for S3 backup
import boto3


# Functions

#start webdriver #to use just write wd = init_driver() in the state function
def init_driver():
  options = webdriver.ChromeOptions()
  options.add_argument('--headless')
  options.add_argument('--no-sandbox')
  options.add_argument('--disable-dev-shm-usage')
  wd = webdriver.Chrome('chromedriver',options=options)
  return wd

def writeTable(df,title,startCell,ws):
  i = 0
  maindir = 'crdt_' +  date.today().strftime('%m%d%y')
  func = inspect.stack()[1][3]
  state = func[-2:]
  statedir = maindir + "/" + state
  if not os.path.exists(maindir):
    os.mkdir(maindir)
  if not os.path.exists(statedir):
    os.mkdir(statedir)
  highest_num = 0
  for f in os.listdir(statedir):
    if os.path.isfile(os.path.join(statedir, f)):
        file = os.path.splitext(f)[0]
        file_num = int(file[-1])
        if file_num > highest_num:
            highest_num = file_num
  i = highest_num + 1

  # save this to a filename like: "AK-20210304-1.csv"
  date_str = datetime.datetime.now().strftime("%Y%m%d")
  path = '%s-%s-%d.csv' % (state, date_str, i)
  full_path = os.path.join(statedir, path)
  df.to_csv(full_path)
  if ws == True:
      gd_path = "/Users/User/Google Drive (patkellyatx@gmail.com)/CRDT"
      state_path = gd_path + "/" + state
      if not os.path.exists(state_path):
          os.mkdir(state_path)
      df.to_csv(state_path + "/" + path)
  else:
  # attempt to write to S3
    s3 = boto3.resource('s3')
    bucket_name = 'covid-tracking-project-data'
    s3_path = os.path.join('CRDT', state, path)
    try:
        s3.meta.client.upload_file(full_path, bucket_name, s3_path)
        display('S3 upload successful to %s' % s3_path)
    except Exception as e:
        display('Skipping S3 upload for %s' % state)
    
    


# -*- coding: utf-8 -*-
"""[CTP] CRDT Long Calculations (Official)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CExjy3Af3guUTKub0mVIVdCZKQgj3h4L
"""

# AK
#basic dependencies
#from bs4 import BeautifulSoup
#import pandas as pd
#import time

def runAK(ws, write):
  link ="https://coronavirus-response-alaska-dhss.hub.arcgis.com/datasets/table-3-demographic-distribution-of-confirmed-cases/data"
  wd = init_driver()
  wd.get(link) #visit site
  time.sleep(5) #ensure enough time for page to load
  soup = BeautifulSoup(wd.page_source, "html.parser")
  csv_tag = soup.find("a", class_="csv")
  csv = csv_tag["href"]
  data = pd.read_csv(csv)[13:]
  df_dem = data[["Demographic", "Hospitalizations", "Deaths"]]
  display(df_dem)
  wd.quit()

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('J17',dataToWrite)

    # Write Data To Sheet
    writeTable(df_dem,'AK Hospitalization & Death Demographics (Residents Only)','J19',ws)

# AL
def runAL(ws, write):
  url = 'https://services7.arcgis.com/4RQmZZ0yaZkGR1zy/ArcGIS/rest/services/DIED_FROM_COVID19_STWD_DEMO_PUBLIC/FeatureServer/0/query?where=1%3D1&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&returnGeodetic=false&outFields=EthnicityCat%2C+DiedFromCovid19&returnGeometry=false&returnCentroid=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pjson&token='
  req = requests.get(url)
  df_deaths_eth = pd.json_normalize(req.json()['features'], sep='_')
  df_deaths_eth = df_deaths_eth.rename(columns=lambda x: re.sub('attributes_','',x))
  display(df_deaths_eth)

  url = 'https://services7.arcgis.com/4RQmZZ0yaZkGR1zy/ArcGIS/rest/services/DIED_FROM_COVID19_STWD_DEMO_PUBLIC/FeatureServer/1/query?where=1%3D1&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&returnGeodetic=false&outFields=RaceCat%2C+DiedFromCovid19&returnGeometry=false&returnCentroid=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pjson&token='
  req = requests.get(url)
  df_deaths_race = pd.json_normalize(req.json()['features'], sep='_')
  df_deaths_race = df_deaths_race.rename(columns=lambda x: re.sub('attributes_','',x))
  display(df_deaths_race)

  url = 'https://services7.arcgis.com/4RQmZZ0yaZkGR1zy/ArcGIS/rest/services/Statewide_COVID19_CONFIRMED_DEMOG_PUBLIC/FeatureServer/1/query?where=1%3D1&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&returnGeodetic=false&outFields=Ethnicity%2C+Ethnicity_counts&returnGeometry=false&returnCentroid=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pjson&token='
  req = requests.get(url)
  df_cases_eth = pd.json_normalize(req.json()['features'], sep='_')
  df_cases_eth = df_cases_eth.rename(columns=lambda x: re.sub('attributes_','',x))
  display(df_cases_eth)

  url = 'https://services7.arcgis.com/4RQmZZ0yaZkGR1zy/ArcGIS/rest/services/Statewide_COVID19_CONFIRMED_DEMOG_PUBLIC/FeatureServer/3/query?where=1%3D1&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&returnGeodetic=false&outFields=Racecat%2C+Race_counts&returnGeometry=false&returnCentroid=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pjson&token='
  req = requests.get(url)
  df_cases_race = pd.json_normalize(req.json()['features'], sep='_')
  df_cases_race = df_cases_race.rename(columns=lambda x: re.sub('attributes_','',x))
  display(df_cases_race)



  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('B36',dataToWrite)

    # Write Data To Sheet
    writeTable(df_cases_eth,'Ethnicity Cases','A38',ws)
    writeTable(df_cases_race,'Race Cases','A43',ws)
    writeTable(df_deaths_eth,'Ethnicity Deaths','E38',ws)
    writeTable(df_deaths_race,'Race Deaths','E43',ws)

# AR
def runAR(ws, write):

  #totals, case dems, death dems
  print("AR Cases")
  urlCases='https://services.arcgis.com/PwY9ZuZRDiI5nXUB/ArcGIS/rest/services/UPDATED_ADH_COVID19_STATE_METRICS/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=positives%2C+black%2C+white%2C+na%2C+asian%2C+pi%2C+other_race%2C+unk_race%2C+multi_race%2C+nonhispanic%2C+hispanic&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urlCases)
  df_cases=pd.json_normalize(req.json()['features'])
  df_cases=df_cases.rename(columns=lambda x: re.sub('properties.','',x))
  display(df_cases)

  print("\nAR Deaths")
  urlDeaths='https://services.arcgis.com/PwY9ZuZRDiI5nXUB/ArcGIS/rest/services/UPDATED_ADH_COVID19_STATE_METRICS/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=deaths%2C+d_black%2C+d_white%2C+d_na%2C+d_asian%2C+d_pi%2C+d_other_race%2Cd_unk_race%2Cd_multi_race&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urlDeaths)
  df_deaths=pd.json_normalize(req.json()['features'])
  df_deaths=df_deaths.rename(columns=lambda x: re.sub('properties.','',x))
  display(df_deaths)
  #split into two tables

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('B33',dataToWrite)

    # Write Data To Sheet
    writeTable(df_cases,'Case Demographics','A34',ws)
    writeTable(df_deaths,'Death Demographics','A37',ws)

# CA
#import pandas as pd
# import requests

def runCA(ws, write):
  # CA Case & Deaths Totals, Confirmed & Probables"
  #url = 'https://data.ca.gov/dataset/590188d5-8545-4c93-a9a0-e230f0db7290/resource/926fd08f-cc91-4828-af38-bd45de97f8c3/download/statewide_cases.csv'
  #url = 'https://data.chhs.ca.gov/dataset/f333528b-4d38-4814-bebb-12db1f10f535/resource/046cdd2b-31e5-4d34-9ed3-b48cdbc4be7a/download/covid19cases_test.csv'
  #df_totals = pd.read_csv(url,parse_dates=['date'])
  #maxdateTot = df_totals ['date'].max()
  #print('\nCase & Death Totals')
  #print(maxdateTot,'\n')
  #display(df_totals)
  #df_cases = df_totals.groupby(['date'])[['totalcountconfirmed','totalcountdeaths']].sum()
  #df_cases = df_totals[df_totals['date'] == maxdateTot]
  #df_cases = df_cases.groupby(['date'])[['totalcountconfirmed','totalcountdeaths']].sum()
  #df_cases = df_cases.astype('int')
  #display(df_cases)

  # CA Race & Ethnicity
  #url = 'https://data.ca.gov/dataset/590188d5-8545-4c93-a9a0-e230f0db7290/resource/7e477adb-d7ab-4d4b-a198-dc4c6dc634c9/download/case_demographics_ethnicity.csv'
  url = 'https://data.chhs.ca.gov/dataset/f333528b-4d38-4814-bebb-12db1f10f535/resource/e2c6a86b-d269-4ce1-b484-570353265183/download/covid19casesdemographics.csv'
  #df_raceeth = pd.read_csv(url,parse_dates=['date'])
  df_raceeth = pd.read_csv(url,parse_dates=['report_date'])

  print ('\nCA Race and Ethnicity Totals')
  maxdateRace = df_raceeth ['report_date'].max()
  #maxdateRace = df_raceeth ['date'].max()
  print(maxdateRace,'\n')
  df_dem = df_raceeth[df_raceeth['report_date'] == maxdateRace]
  df_dem = df_dem[df_dem['demographic_category'] == 'Race Ethnicity']
  df_dem = df_dem.drop(df_dem.columns[[0,3,5,6]],axis=1)
  display(df_dem)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('F16',dataToWrite)

    # Write Data To Sheet
    #writeTable(df_cases,'Case & Death Totals','G18',ws)
    writeTable(df_dem,'CA Race and Ethnicity Totals','G23',ws)

#CO

def runCO(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC

  # CO Total Cases, Total Deaths, Percent of Cases by Race, Percent of Cases by Ethnicity
  #url='https://data-cdphe.opendata.arcgis.com/datasets/cdphe-covid19-state-level-expanded-case-data/data'
  #dnld_xpath='//*[@class="btn dropdown-toggle btn-default hub-download"]'
  #dnld_xpath_caret='//*[@class="caret"]'
  url='https://opendata.arcgis.com/datasets/15883575464d46f686044d2c1aa84ef9_0.csv'

  df_totals = pd.read_csv(url,parse_dates=['date'])
  print(url)
  #display(df_totals)
  maxDateCase = df_totals ['date'].max()

  print(maxDateCase, '\n')
  df_tot=df_totals[df_totals['description'] == 'Cumulative counts to date']
  df_tot=(df_tot[df_tot['date']== maxDateCase])
  df_tot['value'].astype(float)
  
  # Find the case Totals & death Totals
  df_tot=df_tot.drop(df_tot.columns[[0,1,2,3,6,7]],axis=1).reset_index() 
  print("CO Totals")
  display(df_tot)
  caseTot=(df_tot[df_tot['metric'] == 'Cases'])
  cases=caseTot['value']
  #print(cases)
  deathTot=(df_tot[df_tot['metric'] == 'Deaths Among Cases'])
  deaths=deathTot['value']
  #print(deaths)

  # find the Case Demographics
  df_caseDems=df_totals[df_totals['description']== 'Percent of Cases by Race and Ethnicity']
  df_caseDems=(df_caseDems[df_caseDems['date'] == maxDateCase])
  df_caseDems=df_caseDems.drop(df_caseDems.columns[[0,2,3,6,7]],axis=1).reset_index() 
  df_caseDems['value'].astype(float)
  df_caseDems['Case Count']=df_caseDems['value']
  display(df_caseDems)
  for i in range(len(df_caseDems)):
    df_caseDems.iloc[i, 4]=round(df_caseDems.iloc[i, 4] * cases.iloc[0]) # PK fix
  display(df_caseDems)

  # find the Death Demographics
  df_deathDems = df_totals[df_totals['description'] == 'Percent of Deaths by Race and Ethnicity']
  df_deathDems= (df_deathDems[df_deathDems['date'] == maxDateCase])
  df_deathDems=df_deathDems.drop(df_deathDems.columns[[0,2,3,6,7]],axis=1).reset_index()
  df_deathDems['value'].astype(float)
  df_deathDems['Death Count']=df_deathDems['value']
  print('6')
  for i in range(len(df_deathDems)):
    df_deathDems.iloc[i, 4]=round(df_deathDems.iloc[i, 4] * deaths.iloc[0]) # PK fix

  print("CO Case Demographic %")
  display(df_caseDems)
  print("CO Death Demographic %")
  display(df_deathDems)

  if(write == 1):
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('F16',dataToWrite)

    # Write Data To Sheet
    #writeTable(df_cases,'Case & Death Totals','G18',ws)
    writeTable(df_tot,'CO Totals','G23',ws)
    writeTable(df_caseDems, 'CO Case Demographics', 'H23', ws)
    writeTable(df_deathDems, 'CO Death Demographics', 'J23', ws)
  
  

#CT

def runCT(ws, write):

  # CT Case & Deaths Totals, Confirmed & Probables"
  url = 'https://data.ct.gov/api/views/rf3k-f8fg/rows.csv?accessType=DOWNLOAD&bom=true&format=true'
  df_totals = pd.read_csv(url,parse_dates=['Date'])

  pd.set_option('chained_assignment',None)
  print('CT Case & Deaths Totals, Confirmed & Probables')

  #Select the latest date
  maxdate2 = df_totals ['Date'].max()
  print(maxdate2,'\n')

  #Select the data from the latest date
  df_tot = df_totals[df_totals['Date'] == maxdate2]
  #Drop the extra columns
  df_tot.drop(df_tot.columns[[0,1,11,12,13,14,15]],axis=1,inplace=True)
  #Replace commas with nothing
  df_totals2=df_tot.replace({',':''}, regex=True)
  #Convert strings to numeric
  df_tot=df_totals2.apply(pd.to_numeric)
  #Fill in Nan with 0
  df_tot=df_tot.fillna(0)
  display(df_tot)

  # CT Race & Ethnicity
  url = 'https://data.ct.gov/api/views/7rne-efic/rows.csv?accessType=DOWNLOAD&bom=true&format=true'
  df_raceeth = pd.read_csv(url,parse_dates=['Date updated'])

  print ('\n CT Race and Ethnicity Totals')
  #select latest date
  maxdateRace2 = df_raceeth ['Date updated'].max()
  print(maxdateRace2,'\n')
  #select rows for latest date
  df_dem = df_raceeth[df_raceeth['Date updated'] == maxdate2]
  #drop extra columns
  df_dem.drop(df_dem.columns[[0,2,4,5,7,8]],axis=1,inplace=True)
  #replace , with nothing
  df_dems2=df_dem.replace({',':''},regex=True)
  #convert individula columns from string to numeric
  df_dems2['Total cases']=pd.to_numeric(df_dems2['Total cases'])
  df_dems2['Total deaths']=pd.to_numeric(df_dems2['Total deaths'])
  #replace NaN with 0
  df_dems2=df_dems2.fillna(0)
  display(df_dems2)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('F19',dataToWrite)

    # Write Data To Sheet
    writeTable(df_tot,'CT Case & Deaths Totals, Confirmed & Probables','G18',ws)
    writeTable(df_dems2,'CT Race and Ethnicity Totals','G22',ws)

# DC
def runDC(ws,write):

  # Function to wrangle data after reading from excel file
  def wrangle(df):
    # mapping of date format, only in date column names
    mapper = lambda x: x.strftime("%d-%b") if isinstance(x, datetime.datetime) else x
    # determine max date by finding maximum of date headers
    max_date = max(d for d in df.columns.values if isinstance(d, datetime.date))
    # Only keep columns with maximum date and the category names
    drop_cols = df.columns.difference([max_date,'Unnamed: 0'])
    df.drop(drop_cols, 1, inplace=True)
    # Remove NaNs, convert columns to integer (removes .0's)
    df = df.fillna(0)
    df.replace(u'\xa0',u'', regex=True, inplace=True)
    df.replace(',','', regex=True, inplace=True)
    df[max_date] = df[max_date].astype('int32')
    # Apply mapper function
    df.columns = df.columns.map(mapper)
    df = df.rename(columns = {'Unnamed: 0':'Race'})
    return df


  # Get DC excel file
  url = 'https://coronavirus.dc.gov/data'
  req = requests.get(url)
  soup = BeautifulSoup(req.text, 'html.parser')
  a = soup.find('a', string=re.compile("Download copy of DC COVID-19 data"))
#  link = "https://coronavirus.dc.gov/{}".format(a['href'])
  if a['href'][0:5] == 'https':
    link = "{}".format(a['href'])
  else:
    link = "https://coronavirus.dc.gov/{}".format(a['href'])
  display(link)
  res = requests.get(link)

  # Read, wrangle race cases tab
  df_cases = pd.read_excel(BytesIO(res.content), sheet_name="Total Cases by Race", skiprows=[0,2], engine='openpyxl')
  df_cases = wrangle(df_cases)
  display(df_cases)

  # Read, wrangle race deaths tab
  df_deaths = pd.read_excel(BytesIO(res.content), sheet_name="Lives Lost by Race", skiprows=[1], engine='openpyxl')
  df_deaths = wrangle(df_deaths)
  display(df_deaths)

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('H17',dataToWrite)

    # Write Data To Sheet
    writeTable(df_cases,'Total Cases by Race','I16',ws)
    writeTable(df_deaths,'Lives Lost by Race','K16',ws)

# DE
def runDE(ws,write):
  def find_val(span):
    val = re.findall(r'\b\d{1,3}(?:,\d{3})*(?!\d)', span.text)
    val = int(val[0].replace(',',''))
    return val
  
  def make_table(script,txt):
    pattern = re.compile(r"App.Charts.SimpleBar\(\n\s*({\"name\".*})", flags=re.MULTILINE)
    scr = re.search(pattern, script)
    dict = json.loads(scr.groups(0)[0])
    keys = ['categories', 'data']
    dict = {x:dict[x] for x in keys}
    dict['data'] = dict['data'][0][1:]
    df = pd.DataFrame(dict)
    df.columns=df.columns.str.capitalize()
    df.rename(columns={'Categories': txt},inplace=True)
    df.Data = df.Data.astype('int')
    return df
  
  confirmed = []
  probable = []
  num_found = [0,0,0,0,0]
  
  url = 'https://myhealthycommunity.dhss.delaware.gov/locations/state/testing'
  req = requests.get(url)
  soup = BeautifulSoup(req.text, 'html5lib')
  
  # Find Total Tests
  span = soup.find_all('span')
  
  for i in range(0,len(span)):
    if ("State of Delaware" in span[i]) & (num_found[4]<2):
      if num_found[4] == 0:
        total_tests = find_val(span[i+1])
      num_found[4] += 1 
  
  all_script = soup.find_all('script')
  
  for scr in all_script:
    if "total_persons_tested_by_race_ethnicity" in scr.text:
      df_tests = make_table(scr.text,'Tests')
  
  url = 'https://myhealthycommunity.dhss.delaware.gov/locations/state/cases'
  req = requests.get(url)
  soup = BeautifulSoup(req.text, 'html5lib')
  
  # Find Total Cases
  span = soup.find_all('span')
  
  for i in range(0,len(span)):
    if ("Total Positive Cases" in span[i]) & (num_found[0]==0):
      total_cases = find_val(span[i+1])
      num_found[0] += 1
    if ("Confirmed" in span[i]) & (num_found[2]<2):
      confirmed.append(find_val(span[i+1]))
      num_found[2] += 1
    if ("Probable" in span[i]) & (num_found[3]<2):
      probable.append(find_val(span[i+1]))
      num_found[3] += 1
  
  all_script = soup.find_all('script')
  
  for scr in all_script:
    if "total_cases_by_race_ethnicity" in scr.text:
      df_cases = make_table(scr.text,'Cases')
  
  url = 'https://myhealthycommunity.dhss.delaware.gov/locations/state/deaths'
  req = requests.get(url)
  soup = BeautifulSoup(req.text, 'html5lib')
  
  # Find Total Deaths
  span = soup.find_all('span')
  
  for i in range(0,len(span)):
    if ("Total Deaths" in span[i]) & (num_found[1]==0):
      total_deaths = find_val(span[i+1])
      num_found[1] += 1
    if ("Confirmed" in span[i]) & (num_found[2]<2):
      confirmed.append(find_val(span[i+1]))
      num_found[2] += 1
    if ("Probable" in span[i]) & (num_found[3]<2):
      probable.append(find_val(span[i+1]))
      num_found[3] += 1
  
  
  all_script = soup.find_all('script')
  
  for scr in all_script:
    if "total_deaths_by_race_ethnicity" in scr.text:
      df_deaths = make_table(scr.text,'Deaths')
  
  df_totals = pd.DataFrame([["Cases",total_cases, confirmed[0],probable[0]],["Deaths",total_deaths, confirmed[1],probable[1]],["Tests",total_tests,0,0]],
                          columns=['Category','Total','Confirmed','Probable'])
  display(df_totals)
  
  display(df_tests)
  display(df_cases)
  display(df_deaths)
  
  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('H14',dataToWrite)

    # Write Data To Sheet
    writeTable(df_totals,'','G15',ws)
    writeTable(df_cases,'','G19',ws)
    writeTable(df_deaths,'','G30',ws)
    writeTable(df_tests,'','G41',ws)

# FL

def runFL(ws, write):
  url = 'http://ww11.doh.state.fl.us/comm/_partners/covid19_report_archive/cases-monitoring-and-pui-information/state-report/state_reports_latest.pdf'
  url2 = 'https://services1.arcgis.com/CY1LXxl9zlJeBuRZ/arcgis/rest/services/Florida_COVID19_Cases/FeatureServer/0/query?where=COUNTY_1%3D%27State%27&outFields=C_HospYes_Res%2C+C_HospYes_NonRes&f=json'

  req = requests.get(url2)
  dict = req.json()['features'][0]['attributes']
  df_hosp_totals = pd.DataFrame({'Metric': list(dict.keys()),'Values':list(dict.values())})
  display(df_hosp_totals)

  req = requests.get(url)
  pdf = BytesIO(req.content)
  first_page = PyPDF2.PdfFileReader(pdf).getPage(0).extractText()

  totals_start = re.search(r"Total cases\n",first_page)
  totals_end = re.search(r"Cases: people with positive PCR ",first_page)

  totals_table = first_page[totals_start.start(0):totals_end.start(0)]

  StringData = StringIO(totals_table)
  df_totals = pd.read_csv(StringData, sep ="\n", header=None)
  df_totals = df_totals.drop([12,45]) #remove headers that screw up order
  df_totals = pd.DataFrame(np.reshape(df_totals.values,(25,2)), columns=['Metric','Value'])
  display(df_totals)

  third_page = PyPDF2.PdfFileReader(pdf).getPage(2).extractText()

  footnote = re.search(r"Hospitalization\n counts include",third_page)
  race_start = re.search(r"Race and ethnicity",third_page)
  race_table = third_page[race_start.start(0):footnote.start(0)]

  body_start = re.search(r"White\n",race_table)
  body_end = re.search(r"Total\n",race_table)

  body = race_table[body_start.start(0):body_end.start(0)]

  StringData = StringIO(body)
  df_race = pd.read_csv(StringData, sep ="\n", header=None)
  df_race = pd.DataFrame(np.reshape(df_race.values,(16,7)), columns=['Race and ethnicity','Cases','','Hospitalizations','','Deaths',''])
  display(df_race)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('L23',dataToWrite)

    # Write Data To Sheet
    writeTable(df_hosp_totals,'Hospitalization Totals','N23',ws)
    writeTable(df_totals,'Overall Totals','P23',ws)
    writeTable(df_race,'Demographics','R23',ws)

# GA
#from io import StringIO, BytesIO
#from datetime import date
#import pandas as pd
#import requests
#import zipfile
#import pprint

def runGA(ws, write):
  print('Run Date:', date.today())

  url = 'https://ga-covid19.ondemand.sas.com/docs/ga_covid_data.zip'
  res = requests.get(url)
  zipdata = BytesIO(res.content)
  zip = zipfile.ZipFile(zipdata, 'r')

   #Case & Death Totals
  df_GA_totals = pd.read_csv(zip.open('summary_totals.csv'))
  print('\nTable for GA Totals')
  display(df_GA_totals)

  df_GA_demo = pd.read_csv(zip.open('demographics_by_race_eth.csv'))
  print('\nTable for Copying to Spreadsheet Area of State Page')
  #display(df_GA_demo)
  df_dems = df_GA_demo[df_GA_demo['county name'] == 'Georgia']
  df_dems = df_dems.groupby(['county name','ethnicity','race'])[['cases','hospitalization','deaths']].sum().reset_index()
  df_dems = df_dems.drop(['county name'],1)
  display(df_dems)

  print('\nRace Table to Match Category Sums')
  df_hisp = df_GA_demo[df_GA_demo['ethnicity'] == "Hispanic/Latino"]
  df_hisp = df_hisp.groupby(['ethnicity'])[['cases','hospitalization','deaths']].sum()
  df_nonhisp = df_GA_demo[df_GA_demo['ethnicity'] == "Non-Hispanic/Latino"]
  df_nonhisp = df_nonhisp.groupby(['race'])[['cases','hospitalization','deaths']].sum()
  df_race = df_nonhisp.append(df_hisp)

  print('\nEthnicity Table to Match Category Sums')
  df_eth = df_GA_demo.groupby(['ethnicity'])[['cases','hospitalization','deaths']].sum()
  display(df_eth)
  df_nonhisp = df_GA_demo[df_GA_demo['ethnicity'] == "Non-Hispanic/Latino"]
  df_nonhisp = df_nonhisp.groupby(['race'])[['cases','hospitalization','deaths']].sum()

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('F20',dataToWrite)

    # Write Data To Sheet
    writeTable(df_dems,'Table for Copying to Spreadsheet Area of State Page','G18',ws)
    writeTable(df_GA_totals,'','G41',ws)

# GU
def runGU(ws,write):

  url = 'https://govguamgis.maps.arcgis.com/apps/opsdashboard/index.html#/304a0edd115b49ff9f98ad52cb66d416'
  wd = init_driver()
  wd.get(url)
  time.sleep(10)
  soup = BeautifulSoup(wd.page_source, 'html.parser')

  # Find Totals
  g_all = soup.find_all('g',{"class": "amcharts-pie-item"})

  pattern = re.compile(r'aria-label=\"(.+) \"')
  demo = []
  for i in range(len(g_all)):
    cat = re.findall(pattern,str(g_all[i]))[0].split()
    if len(cat) == 4:
      cat[0] = cat[0] + '_' + cat[1]
      cat[1:3] = cat[2:4]
    demo.append(cat)

  df_demo = pd.DataFrame(demo).loc[:,[0,2]]


  df_demo_cases = df_demo.loc[2:9,:]
  df_demo_cases.columns = ['Category','Cases']
  df_demo_cases.Cases.replace(',','', regex=True, inplace=True)
  df_demo_cases.Cases = df_demo_cases.Cases.astype('int')
  df_demo_cases.sort_values(by=['Category'],inplace=True)


  df_demo_deaths = df_demo.loc[12:19,:]
  df_demo_deaths.columns = ['Category','Deaths']
  df_demo_deaths.Deaths = df_demo_deaths.Deaths.astype('int')
  df_demo_deaths.sort_values(by=['Category'],inplace=True)

  display(df_demo_cases)
  display(df_demo_deaths)

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('H19',dataToWrite)

    # Write Data To Sheet
    writeTable(df_demo_cases,'','G21',ws)
    writeTable(df_demo_deaths,'','I21',ws)

# HI ************

def runHI(ws,write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC

  #Tableau for Race
  src="https://public.tableau.com/views/HawaiiCOVID-19-RaceChart/Overview?:embed=y&:showVizHome=no&:host_url=https%3A%2F%2Fpublic.tableau.com%2F&:embed_code_version=3&:tabs=no&:toolbar=yes&:animate_transition=yes&:display_static_image=no&:display_spinner=no&:display_overlay=yes&:display_count=yes&null&:loadOrderID=11"

  casesSrc='https://services9.arcgis.com/aKxrz4vDVjfUwBWJ/arcgis/rest/services/HIEMA_COVID_CASES_PUBLIC_LATEST_1203/FeatureServer/0/query?f=json&where=1%3D1&returnGeometry=false&spatialRel=esriSpatialRelIntersects&outFields=*&orderByFields=current_active%20desc&resultOffset=0&resultRecordCount=1&resultType=standard&cacheHint=true'
  deathSrc='https://services9.arcgis.com/aKxrz4vDVjfUwBWJ/arcgis/rest/services/MMWR_Fatality_Latest/FeatureServer/0/query?where=county%3D%27State%27&objectIds=&time=&resultType=none&outFields=*&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  hospSrc='https://services9.arcgis.com/aKxrz4vDVjfUwBWJ/arcgis/rest/services/hospital_reporting_cases/FeatureServer/0/query?where=name%3D%27State%27&returnGeometry=false&spatialRel=esriSpatialRelIntersects&outFields=*&outStatistics=%5B%7B%22statisticType%22%3A%22sum%22%2C%22onStatisticField%22%3A%22num_hospitalized_new_admit%22%2C%22outStatisticFieldName%22%3A%22value%22%7D%5D&resultType=standard&sqlFormat=none&f=pgeojson&token='


  #Summary Table xpath->Tableau Download->Crosstab Button->CSV->Table->Download
  summary_xpath='//*[@id="tabZoneId4"]/div/div/div/span[2]/div/span/span/span[3]/div[2]/div'
  tab_dnld_xpath='//*[@id="download-ToolbarButton"]'
  crosstab_xpath='//*[@id="DownloadDialog-Dialog-Body-Id"]/div/fieldset/button[3]'
  csv_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[2]/div[2]/div/label[2]'
  census_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[2]'
  dnld_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[3]/button'
  bottom_xpath='//*[@id="view3515176980683563860_249615479279001290"]/div[1]/div[2]/canvas[2]'

  #file csvs
  csv_metric = "Table.csv"

  def getCSV(metric_xpath,csv_file):
     wait = WebDriverWait(wd, 20)
     wait.until(EC.element_to_be_clickable((By.XPATH,metric_xpath))).click()
     wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ".tab-icon-download"))).click()
     print("clicked download on tableau frame")
     wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Crosstab']"))).click()
     print("chose crosstab option")
     wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "label[data-tb-test-id='crosstab-options-dialog-radio-csv-Label']"))).click()
     print("Chose CSV option")
     wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='Table']"))).click()
     print("Chose census file")
     wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Download']"))).click()
     print("Clicked dnld button")
     time.sleep(5)
     return pd.read_csv(csv_file,sep="\t",encoding="utf-16")

  #download view and convert to df
  wd=init_driver()
  wd.get(src)
  time.sleep(10)
  print(src)

  #Go Get the HI Race Table
  print("-" * 10)
  print("HI Race")
  df_race=getCSV(summary_xpath, csv_metric)
  df_race=df_race.fillna('0')
  #convert to integer
  tcols=list(df_race.columns)
  tcols[1]='Case Totals'
  tcols[5]='Death Totals'
  tcols[6]='Hospital Totals'
  df_race.columns=tcols
  #display(df_race)
  df_race['Death Totals'] = df_race['Death Totals'].replace('','4', regex=True)
  df_race['Case Totals'] = df_race['Case Totals'].replace(',','', regex=True).apply(pd.to_numeric)
  df_race['Death Totals'] = df_race['Death Totals'].replace(',','',regex=True).apply(pd.to_numeric)
  df_race['Hospital Totals'] = df_race['Hospital Totals'].replace(',','',regex=True).apply(pd.to_numeric)
  display(df_race)

  #Retrieve HI Total Cases
  req=requests.get(casesSrc)
  df_tot=pd.json_normalize(req.json()['features'])
  df_tot=df_tot.rename(columns=lambda x:re.sub('attributes.','',x))
  df_tots=df_tot.loc[:,['toDate_positiveAndPresumed']]
  df_tots['toDate_positiveAndPresumed'] = df_tots['toDate_positiveAndPresumed'].replace(',', '',regex=True).apply(pd.to_numeric)
  df_tots=df_tots.fillna('0')
  tcols=list(df_tots.columns)
  tcols[0]='Total Cases'
  df_tots.columns=tcols
  display(df_tots)

  #Retrieve HI Hospitalizations
  req=requests.get(hospSrc)
  df_hosp=pd.json_normalize(req.json()['features'])
  df_hosp=df_hosp.rename(columns=lambda x:re.sub('properties.','',x))
  df_hosps=df_hosp.loc[:,['value']]
  df_hosps=df_hosps.fillna('0')
  tcols=list(df_hosps.columns)
  tcols[0]="Hospital"
  df_hosps.columns=tcols
  df_hosps['Hospital'] = df_hosps['Hospital'].replace(',', '',regex=True).apply(pd.to_numeric)
  display(df_hosps)

  #Retrieve HI Deaths
  req=requests.get(deathSrc)
  #{"MMWRyear":2021,"MMWRweek":7,"week_ending":1613779200000,"county":"State","deaths":1,"ObjectId":420,"rolling_deaths":7.42857142857143,"previous_deaths":3,"cumulative_deaths":431,"fatality_rate":1.58607492456024}}]}
  df_death=pd.json_normalize(req.json()['features'])
  df_death=df_death.rename(columns=lambda x:re.sub('properties.','',x))
  df_deaths=df_death.loc[:,['cumulative_deaths']]
  df_deaths=df_deaths.fillna('0')
  tcols=list(df_deaths.columns)
  tcols[0]="Deaths"
  df_deaths.columns=tcols
  df_deaths['Deaths'] = df_deaths['Deaths'].replace(',', '',regex=True).apply(pd.to_numeric)
  display(df_deaths)

  wd.quit()

  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('K13',dataToWrite)

      # Write Data To Sheet
      writeTable(df_tots,'','J14',ws)
      writeTable(df_deaths,'','K14',ws)
      writeTable(df_hosps,'','L14',ws)
      writeTable(df_race,'HI Race','J17',ws)

# ID
#
def runID(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC

  def download_CSV(csv_name):
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ".tab-icon-download"))).click()
    print('clicked download')
    wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Crosstab']"))).click()
    print('clicked crosstab')
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "label[data-tb-test-id='crosstab-options-dialog-radio-csv-Label']"))).click()
    print('clicked csv')
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='" + csv_name + "']"))).click()
    print('clicked file')
    wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Download']"))).click()
    print('clicked download')
    time.sleep(4)

  def colstr2int(df,col):
    df.loc[:,col] = df.loc[:,col].replace(',','', regex=True)
    df[col] = df[col].astype('int')

  def clean_tots(df,cats,dropval):
    df.iloc[:,0] = cats
    df.columns = ['Metric','Value']
    df = df.drop(dropval)
    colstr2int(df,'Value')
    df.sort_values(by='Value',ascending=False,inplace=True)
    return df

  def clean_demo(df,cols,type):
    df = df.iloc[:, cols].copy()
    df.columns = ['Category', type]
    colstr2int(df,type)
    return df

  def get_page(tabZone):
    wait.until(EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, "iframe[title='Data Visualization']")))
    wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='" + tabZone + "']/div/div/div/div/div"))).click()
    time.sleep(2)

  url = 'https://public.tableau.com/profile/idaho.division.of.public.health#!/vizhome/DPHIdahoCOVID-19Dashboard/Home'

  # Cases Downloads
  wd = init_driver()
  wd.get(url)
  wait = WebDriverWait(wd, 60)
  print('Click on Demo Page')
  get_page('tabZoneId193')
  wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div[tb-test-id='Asymptomatic']")))

  df_cases = {}

  sheets = ['CaseRace', 'CaseEth', 'State Total Cases Display (2)']
  for sheet in sheets:
    print('Download sheet %s' % sheet)
    download_CSV(sheet)
    if sheet == 'State Total Cases Display (2)':
      df_cases[sheet] = pd.read_csv(sheet + '.csv',sep="\t", encoding="utf-16",header=None)
    else:
      df_cases[sheet] = pd.read_csv(sheet + '.csv',sep="\t", encoding="utf-16")

  wd.quit()

  # Deaths Downloads
  wd = init_driver()
  wd.get(url)
  wait = WebDriverWait(wd, 60)
  print('Click on Demo Page')
  get_page('tabZoneId191')
  wait.until(EC.presence_of_all_elements_located((By.XPATH, "//span[text()='Demographics of COVID-19 Related Deaths']")))

  df_deaths = {}

  sheets = ['Race', 'Race Pending', 'Ethnicity', 'Ethnicity Pending','Total Deaths (2)']
  for sheet in sheets:
    print('Download sheet %s' % sheet)
    download_CSV(sheet)
    if (sheet == 'Race') or (sheet == 'Ethnicity'):
      df_deaths[sheet] = pd.read_csv(sheet + '.csv',sep="\t", encoding="utf-16")
    else:
      df_deaths[sheet] = pd.read_csv(sheet + '.csv',sep="\t", encoding="utf-16",header=None)

  wd.quit()

  # Calculations and Display

  df_cases['State Total Cases Display (2)'] = clean_tots(df_cases['State Total Cases Display (2)'],['Total Cases','New','Confirmed Cases','Probable Cases'],1)
  display(df_cases['State Total Cases Display (2)'])

  df_deaths['Total Deaths (2)'] = clean_tots(df_deaths['Total Deaths (2)'],['Confirmed Deaths','Total Deaths','Proabable Deaths',''],3)
  display(df_deaths['Total Deaths (2)'])

  vals = [['Known Cases - Race', df_cases['CaseRace'].iloc[0,2]],
          ['Known Cases - Ethnicity',df_cases['CaseEth'].iloc[0,2]],
          ['Pending Deaths - Race', df_deaths['Race Pending'].iloc[0,0]],
          ['Pending Deaths - Ethnicity', df_deaths['Ethnicity Pending'].iloc[0,0]]]
  df_vals = pd.DataFrame(vals,columns=['Metric','Value'])
  colstr2int(df_vals,'Value')
  display(df_vals)

  df_cases['CaseRace'] = clean_demo(df_cases['CaseRace'],[0,3],'Cases')
  display(df_cases['CaseRace'])

  df_cases['CaseEth'] = clean_demo(df_cases['CaseEth'],[0,3],'Cases')
  display(df_cases['CaseEth'])

  df_deaths['Race'] = clean_demo(df_deaths['Race'],[0,2],'Deaths')
  display(df_deaths['Race'])

  df_deaths['Ethnicity'] = clean_demo(df_deaths['Ethnicity'],[0,2],'Deaths')
  display(df_deaths['Ethnicity'])

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    ##ws.update('L14',dataToWrite)

    # Write Data To Sheet
    writeTable(df_cases['State Total Cases Display (2)'],'','K15',ws)
    writeTable(df_deaths['Total Deaths (2)'],'','M15',ws)
    writeTable(df_vals,'','O15',ws)
    writeTable(df_cases['CaseRace'],'','K21',ws)
    writeTable(df_cases['CaseEth'],'','K31',ws)
    writeTable(df_deaths['Race'],'','M21',ws)
    writeTable(df_deaths['Ethnicity'],'','M31',ws)

#MI
def runMI(ws,write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains

  url = 'https://app.powerbigov.us/view?r=eyJrIjoiNDY0ZGVlMDItMzUzNC00ZGE5LWFjYzQtNzliOGJkZWQ4YTgzIiwidCI6ImQ1ZmI3MDg3LTM3NzctNDJhZC05NjZhLTg5MmVmNDcyMjVkMSJ9'

  def colstr2int(df,col):
    df.loc[:,col] = df.loc[:,col].replace(',','', regex=True)
    df[col] = df[col].astype('int')

  wd = init_driver()
  wd.get(url)
  wait = WebDriverWait(wd, 20)

  demo_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-group[3]/transform/div/div[2]/visual-container-modern[4]/transform/div/div[3]/div/visual-modern/div/button")))
  demo_button.click()
  print('clicked Demo button')

  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[3]/transform/div")))
  ActionChains(wd).context_click(elements[0]).perform()
  #  time.sleep(10)
  wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='Show as a table']"))).click()
  print('clicked Show Table')

  cats = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive ']")))
  for element in elements:
    cats.append(element.text)

  vals = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive tablixAlignRight ']")))
  for element in elements:
    vals.append(element.text)

  df_conf_cases = pd.DataFrame([cats[-7:],vals]).T
  df_conf_cases.columns = ['Category', 'Confirmed Cases']
  colstr2int(df_conf_cases,'Confirmed Cases')
  df_conf_cases.sort_values('Category',ascending=True,inplace=True)
  display(df_conf_cases)

  back_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[3]/transform/div/div[3]/visual-container-pop-out-bar/div/div[1]/button")))
  back_button.click()
  print('clicked Back button')

  prob_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-group/transform/div/div[2]/visual-container-modern[3]/transform/div/div[3]/div/visual-modern/div/div/div[2]/div/div[2]/div/div[1]/div/div/div[2]")))
  prob_button.click()
  print('clicked Probable case status')

  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[3]/transform/div")))
  ActionChains(wd).context_click(elements[0]).perform()
  #  time.sleep(10)
  wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='Show as a table']"))).click()
  print('clicked Show Table')

  cats = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive ']")))
  for element in elements:
    cats.append(element.text)

  vals = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive tablixAlignRight ']")))
  for element in elements:
    vals.append(element.text)

  df_prob_cases = pd.DataFrame([cats[-7:],vals]).T
  df_prob_cases.columns = ['Category', 'Probable Cases']
  colstr2int(df_prob_cases,'Probable Cases')
  df_prob_cases.sort_values('Category',ascending=True,inplace=True)
  display(df_prob_cases)

  back_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[3]/transform/div/div[3]/visual-container-pop-out-bar/div/div[1]/button")))
  back_button.click()
  print('clicked Back button')

  deaths_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@aria-label=' Bookmark Button Click to view demographic characteristics of deaths.. Click here to follow link']")))
  deaths_button.click()
  print('clicked Deaths button')
  time.sleep(5)

  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[3]/transform/div")))
  ActionChains(wd).context_click(elements[0]).perform()
  #  time.sleep(10)
  wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='Show as a table']"))).click()
  print('clicked Show Table')

  cats = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive ']")))
  for element in elements:
    cats.append(element.text)

  vals = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive tablixAlignRight ']")))
  for element in elements:
    vals.append(element.text)

  df_conf_deaths = pd.DataFrame([cats[-7:],vals]).T
  df_conf_deaths.columns = ['Category', 'Confirmed Deaths']
  colstr2int(df_conf_deaths,'Confirmed Deaths')
  df_conf_deaths.sort_values('Category',ascending=True,inplace=True)
  display(df_conf_deaths)

  back_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[19]/transform/div/div[3]/visual-container-pop-out-bar/div/div[1]/button")))
  back_button.click()
  print('clicked Back button')

  prob_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-group/transform/div/div[2]/visual-container-modern[3]/transform/div/div[3]/div/visual-modern/div/div/div[2]/div/div[2]/div/div[1]/div/div/div[2]")))
  prob_button.click()
  print('clicked Probable case status')

  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[3]/transform/div")))
  ActionChains(wd).context_click(elements[0]).perform()
  #  time.sleep(10)
  wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='Show as a table']"))).click()
  print('clicked Show Table')

  cats = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive ']")))
  for element in elements:
    cats.append(element.text)

  vals = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive tablixAlignRight ']")))
  for element in elements:
    vals.append(element.text)

  df_prob_deaths = pd.DataFrame([cats[-7:],vals]).T
  df_prob_deaths.columns = ['Category', 'Probable Deaths']
  colstr2int(df_prob_deaths,'Probable Deaths')
  df_prob_deaths.sort_values('Category',ascending=True,inplace=True)
  display(df_prob_deaths)

  df = df_conf_cases.merge(df_prob_cases,how='left',on='Category')
  df = df.merge(df_conf_deaths,how='left',on='Category')
  df = df.merge(df_prob_deaths,how='left',on='Category')
  display(df)

  wd.quit()

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('M19',dataToWrite)

    # Write Data To Sheet
    writeTable(df,'','L20',ws)

def runIL(ws,write):

  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC

  def getvals(html,title):
    cats = []
    vals = []
    for e in html:
      tspan = e.find_all('tspan')
      cats.append(tspan[0].get_text())
      vals.append(tspan[1].get_text())
    df = pd.DataFrame([cats[:-3],vals[:-3]]).T
    df.columns = ['Category',title]
    return df

  url = 'http://www.dph.illinois.gov/covid19/covid19-statistics'

  #Open Webpage
  wd = init_driver()
  wait = WebDriverWait(wd, 20)
  wd.get(url)
  time.sleep(10)

  # Cases
  soup = BeautifulSoup(wd.page_source, 'html.parser')
  div_race = soup.find('div',{"id": "pieRace"})
  cases_race = div_race.find_all_next('text',{"class": "slicetext"})

  # Deaths
  wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='liRaceChartDeaths']/a"))).click()
  time.sleep(10)
  soup = BeautifulSoup(wd.page_source, 'html.parser')
  div_race = soup.find('div',{"id": "pieRace"})
  deaths_race = div_race.find_all_next('text',{"class": "slicetext"})

  # Tests
  wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='racePagination']/li[2]/a"))).click()
  time.sleep(10)
  soup = BeautifulSoup(wd.page_source, 'html.parser')
  div_race = soup.find('div',{"id": "pieRace"})
  tests_race = div_race.find_all_next('text',{"class": "slicetext"})

  df_cases = getvals(cases_race,'Cases')
  df_deaths = getvals(deaths_race,'Deaths')
  df_tests = getvals(tests_race,'Tests')

  df = df_cases.merge(df_deaths,how='left',on='Category')
  df = df.merge(df_tests,how='left',on='Category')
  cols = df.columns[1:4]
  df.loc[:,cols] = df.loc[:,cols].replace(',','', regex=True)
  df[cols] = df[cols].astype('int')
  df['Category'] = df['Category'].replace('\*','', regex=True)
  df = df.sort_values(by='Category')
  display(df)

  wd.quit()

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('G33',dataToWrite)

    # Write Data To Sheet
    writeTable(df,'','F34',ws)

# IN
#import pandas as pd
#import requests

def runIN(ws, write):

  url = 'https://hub.mph.in.gov/dataset/62ddcb15-bbe8-477b-bb2e-175ee5af8629/resource/2538d7f1-391b-4733-90b3-9e95cd5f3ea6/download/covid_report_demographics.xlsx'
  df_IN_casesRace = pd.read_excel(url, sheet_name='Race', skiprows=0, engine='openpyxl')
  print("Cases by Race")
  display(df_IN_casesRace)
  df_IN_casesEthnicity = pd.read_excel(url, sheet_name='Ethnicity', skiprows=0, engine='openpyxl')
  print("\nCases by Ethnicity")
  display(df_IN_casesEthnicity)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('G21',dataToWrite)

    # Write Data To Sheet
    writeTable(df_IN_casesRace,'Cases by Race','H22',ws)
    writeTable(df_IN_casesEthnicity,'Cases by Ethnicity','H29',ws)

# KY
#from io import StringIO, BytesIO
#from bs4 import BeautifulSoup
#import pandas as pd
#import tabula
#import PyPDF2
#import re
#import requests

import ssl
ssl._create_default_https_context = ssl._create_unverified_context # To fix bad certificate issue

def runKY(ws, write):
  url = 'https://chfs.ky.gov/agencies/dph/covid19/COVID19DailyReport.pdf'

  req = requests.get(url, verify=False)
  pdf = BytesIO(req.content)
  first_page = PyPDF2.PdfFileReader(pdf).getPage(0).extractText()

  # Known race & ethnicity for cases
  match = re.search(r'Report(\d+\w+\d+)Daily',first_page)
  print('\nDate of KY Report:',match.group(1))
  match = re.search(r'Race known for[ ]*(\d+.\d)%[ ]*of cases and[ ]*(\d+.\d)%[ ]*of deaths',first_page)
  knownRaceCases = match.group(1)
  knownRaceDeaths = match.group(2)
  print('\n% known race for cases = ' + knownRaceCases + '%','\n% known race for deaths = ' + knownRaceDeaths + '%')
  match = re.search(r'Ethnicity known for[ ]*(\d+.\d)%[ ]*of cases and[ ]*(\d+.\d)%[ ]*of deaths',first_page)
  knownEthCases = match.group(1)
  knownEthDeaths = match.group(2)
  print('% known ethnicity for cases = ' + knownEthCases + '%','\n% known ethnicity for deaths = ' + knownEthDeaths + '%\n')

  # Totals
  tables = tabula.read_pdf(url,pages=1,multiple_tables = True)
  totals = tables[0].drop(['Measure'],axis=1)
  totals = totals.iloc[0:2]

  #Remove "," from Case numbers
  totals['Total']=totals['Total'].str.replace(r",","")
  totals['Confirmed']=totals['Confirmed'].str.replace(r",","")
  totals['Probable']=totals['Probable'].str.replace(r",","")
  #Convert Str to Numeric
  totals['Total']=pd.to_numeric(totals['Total'])
  totals['Confirmed']=pd.to_numeric(totals['Confirmed'])
  totals['Probable']=pd.to_numeric(totals['Probable'])
  totals=totals.fillna('')
  print("\nCase and Death Totals")
  display(totals)

  #Race Tables
  race = tables[2]
  race = race.loc[(race.index%2==1) & (race.index<13),['Cases','Race','Deaths']]
  race = race.reset_index(drop=True)
  #Remove "," from Case Numbers
  race['Cases']=race['Cases'].str.replace(r",","")
  race['Deaths']=race['Deaths'].str.replace(r",","")
  #Convert Str to Numeric
  race['Cases']=pd.to_numeric(race['Cases'])
  race['Deaths']=pd.to_numeric(race['Deaths'])
  print('\nRace Table')
  display(race)

  #Ethnicity Tables
  eth = tables[2]
  eth = eth.loc[(eth.index%2==1) & (eth.index>13),['Cases','Race','Deaths']]
  eth.rename(columns={'Race':'Ethnicity'}, inplace=True)
  eth = eth.reset_index(drop=True)
  #Remove "," from Case Numbers
  eth['Cases']=eth['Cases'].str.replace(r",","")
  eth['Deaths']=eth['Deaths'].str.replace(r",","")
  #Convert Str to Numeric
  eth['Cases']=pd.to_numeric(eth['Cases'])
  eth['Deaths']=pd.to_numeric(eth['Deaths'])
  print('\nEthnicity Table')
  display(eth)


  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('L37',dataToWrite)

    dataToWrite = [[knownRaceCases, 'Known %', knownRaceDeaths]]
    #ws.update('J26',dataToWrite)

    dataToWrite = [[knownEthCases, 'Known %', knownEthDeaths]]
    #ws.update('J31',dataToWrite)


    # Write Data To Sheet
    writeTable(race,'Race Table','J18',ws)
    writeTable(eth,'Ethnicity Table','J27',ws)
    writeTable(totals,'Totals Table','J32',ws)

# LA
def runLA(ws,write):

  url = 'https://services5.arcgis.com/O5K6bb5dZVZcTo5M/ArcGIS/rest/services/Case_Deaths_Race_Region_new/FeatureServer/0/query?where=1%3D1&outFields=LDH_region%2C+Race%2C+Deaths%2C+Cases&returnGeometry=false&f=json'

  req = requests.get(url)
  dict = req.json()['features']

  for i in range(0,len(dict)):
    cols = list(dict[i]['attributes'].keys())
    vals = list(dict[i]['attributes'].values())
    df_tmp = pd.DataFrame([vals],columns=cols)
    if i == 0:
      df_demo = df_tmp
    else:
      df_demo = df_demo.append(df_tmp)


  df_demo = df_demo.sort_values(by=['LDH_Region','Race'])

  display(df_demo)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('H26',dataToWrite)

    # Write Data To Sheet
    writeTable(df_demo,'','J27',ws)

# MA

#from io import StringIO, BytesIO
#from bs4 import BeautifulSoup
#import pandas as pd
#import re
#import requests
#import zipfile
def runMA(ws, write):
  url = 'https://www.mass.gov/info-details/covid-19-response-reporting#covid-19-interactive-data-dashboard-'
  req = requests.get(url)
  soup = BeautifulSoup(req.text, 'html.parser')
  a = soup.find('a', string=re.compile("COVID-19 Raw Data"))
  link = "https://www.mass.gov{}".format(a['href'])
  res = requests.get(link)

  #Cases -
  df_cases = pd.read_excel(BytesIO(res.content), sheet_name="Cases (Report Date)", engine='openpyxl')
  maxdate = df_cases['Date'].max()
  print("Case Totals")
  print(maxdate,'\n')
  df_cases = df_cases[df_cases['Date'] == maxdate]
  display(df_cases)

  #Deaths -
  df_deaths = pd.read_excel(BytesIO(res.content), sheet_name="DeathsReported (Report Date)", engine='openpyxl')
  maxdate = df_deaths['Date'].max()
  print("\nDeath Totals")
  print(maxdate,'\n')
  df_deaths = df_deaths[df_deaths['Date'] == maxdate]
  display(df_deaths)

  #Demographics
  df_dems = pd.read_excel(BytesIO(res.content), sheet_name='RaceEthnicityLast2Weeks', engine='openpyxl')
  maxdate = df_dems['Date'].max()
  print("\nMA Demographics")
  print(maxdate,'\n')
  df_dems = df_dems[df_dems['Date'] == maxdate]
  display(df_dems)

  #Demographic Totals for DC
  print ('\nMA Race and Ethnicity Totals')
  print(maxdate,'\n')
  df_dem_tot = df_dems.groupby(['Date'])[['All Cases','Ever Hospitaltized','Deaths']].sum()
  display(df_dem_tot)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('F26',dataToWrite)

    # Write Data To Sheet
    writeTable(df_cases,'Case Totals','G23',ws)
    writeTable(df_deaths,'Death Totals','G27',ws)
    writeTable(df_dems,'MA Demographics','G31',ws)
    writeTable(df_dem_tot,'MA Race and Ethnicity Totals','G42',ws)

# MD
def runMD(ws,write):
    
  url = 'https://coronavirus.maryland.gov/'
  req = requests.get(url)
  soup = BeautifulSoup(req.text, 'html5lib')
  # Find large text block which contains tables
  script = soup.find('script',attrs={'id':'site-injection'})
  win_start = script.text.find('window.__SITE=')
  # Convert URL codes to characters
  win_url = unquote(script.text[win_start:-5])
  race_string = win_url[re.search("By Race and Ethnicity",win_url).start():]
  race_table = race_string[race_string.find('<table'):race_string.find('</table>"')+8]
  # Clean data, move first row to column names
  df = pd.read_html(race_table)[0]
  df = df.fillna('')
  df.columns = df.iloc[0]
  df_demo = df.drop([0])
  if len(df_demo['Cases']) == 7:
      df_demo = df_demo.drop([7])
  # Convert 1st column (cases) to int (other columns fine as strings)
  df_demo['Cases']=df_demo['Cases'].astype('int')
  display(df_demo)

  totals_string = win_url[re.search("COVID-19 Statistics in Maryland",win_url).start():]
  totals_table = totals_string[totals_string.find('<p'):]
  totals_table = totals_table[:totals_table.find('</p>')+4]
  soup = BeautifulSoup(totals_table,'html5lib')
  p_text = soup.find('p').text
  df_totals = pd.read_csv(StringIO(re.sub(r'\\n', '\\n', p_text)), sep=':', header=None)
  df_totals.columns = ["Metric","Value"]
  display(df_totals)


  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('F33',dataToWrite)

    # Write Data To Sheet
    writeTable(df_demo,'','A33',ws)
    writeTable(df_totals,'','G29',ws)

# ME #Race only
def runME(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains

  #URLs
  raceethsrc='https://analytics.maine.gov/t/CDCExternal/views/covid-19-maine-cdc-dashboard/6_COVID-19databyrace?%3Aembed=y&amp%3B%3AshowVizHome=no&amp%3B%3Ahost_url=https%3A%2F%2Fanalytics.maine.gov%2F&amp%3B%3Aembed_code_version=3&amp%3B%3Atabs=no&amp%3B%3Atoolbar=yes&amp%3B%3AshowAppBanner=false&amp%3B%3Adisplay_spinner=no&amp%3B%3AloadOrderID=0'
  #xpaths
  #Race & Ethnicity Toggle
  race_toggle_xpath='//*[@id="tableau_base_widget_ParameterControl_0"]/div/div[2]/span/div[2]/span/svg'

  #Select hospitalizations
  #race_hosp_xpath='//*[@id="tabZoneId8"]/div/div/div/div[1]/div[5]/div[1]/canvas'
  race_hosp_xpath='//*[@class="tabCanvas tab-widget"]'
  #Download Toolbar Button
  tab_btn_xpath = '//*[@id="download-ToolbarButton"]/span[1]'
  #download form options
  data_btn_xpath='//*[@data-tb-test-id="DownloadData-Button"]'
  #data_btn_xpath='//*[@id="DownloadDialog-Dialog-Body-Id"]/div/fieldset/button[2]'
  #downloaded csv paths
  csv_Race = "case-by-race-bars_data.csv"
  csv_Eth = "case-by-race-bars_data.csv"

  def getCSV(metric_csv):

    wait.until(EC.element_to_be_clickable((By.XPATH,tab_btn_xpath))).click()
    time.sleep(10)
    print("clicked download button")
    print('record the windows that are open')
    window_before = wd.window_handles[0]
    print(window_before)
    #switch to the new window that opens up
    wait.until(EC.element_to_be_clickable((By.XPATH,data_btn_xpath))).click()
    print("clicked data button")
    window_after = wd.window_handles[1]
    wd.switch_to_window(window_after)
    print("window after")
    wait.until(EC.element_to_be_clickable((By.XPATH,"//*[@class='csvLink_summary']"))).click()
    print("clicked text file option")
    time.sleep(5)
    #make df
    df = pd.read_csv(metric_csv, sep="\t", encoding="utf-16") #dumb tableau encoding
    print("-" *10)
    return df

 #Race
  wd=init_driver()
  wd.get(raceethsrc)
  wait = WebDriverWait(wd, 160)
  print(raceethsrc)
  #Demographics by Race
  print("\nDemographics by Race")
  wait.until(EC.element_to_be_clickable((By.XPATH,race_hosp_xpath))).click()
  print("selected Hospitalizations")
  df_casesRace = getCSV(csv_Race)
  df_casesRace.fillna('0')
  #Get rid of extra rows
  df_casesRace=df_casesRace.drop(['Population'],1).fillna('0')
  display(df_casesRace)
  wd.quit()

  #Ethnicity
  wd=init_driver()
  wd.get(raceethsrc)
  wait = WebDriverWait(wd, 120)
  print(raceethsrc)

  #Demographics by Ethnicity
  print("\nDemographics by Ethnicity")
  wd.save_screenshot("eth_Begin.png");

 
  toggle_zero='//*[@id="tableau_base_widget_ParameterControl_0"]/div/div[2]/span/div[2]/span' # Widget
 #Ethnicity
  toggle_one='//*[@id="tab-menuItem2"]/div/span'
  toggle_xpath_eth='//*[@id="tab-menuItem2"]'
  toggle_last='//*[@id="tab-menuItem2"]/div/span'

  toggle_eth_xpath='//*[@id="tab-menuItem2"]/div/span'
  #toggle_eth_xpath='//*[@aria-activedescendant="tab-menuItem2"]'

  toggle_xpath='//*[@id="tab-ui-id-1616981554102"]'
  choose_eth='//*[@id="tab-menuItem2"]/div/span'

  little_widget='//*[@aria-label="Toggle race / ethnicity Ethnicity"]'
  raceeth_xpath='//*[@aria-label="Toggle race / ethnicity"]'
  eth_hosp_xpath='//*[@class="tab-tooltip tab-widget tab-tooltipBR"]'

  #print("Toggle 0")
  #time.sleep(10)
  #wd.find_element_by_xpath(toggle_zero).click()
  #wd.save_screenshot("ethToggleZero.png");

  #print("Toggle One")
  #time.sleep(20)
  #toggle_btn=wd.find_element_by_xpath(toggle_one).click()
  #wd.save_screenshot("ethToggleOne.png");
  #print("Toggle one done, now choosing Toggle two")
  #wd.save_screenshot("ethToggleOne.png");

  #print("Toggle Xpath Eth")
  #toggle_btn=wd.find_element_by_xpath(toggle_xpath_eth).click()
  #wd.save_screenshot("ethToggleXpath.png");
  #print("Toggle xpath eth done, now choosing xpath last")
  #wd.save_screenshot("ethToggleXpath.png");  

  #wait.until(EC.element_to_be_clickable((By.XPATH, eth_hosp_xpath))).click()
  #print("clicked Hospitalization")

  #df_casesEth = getCSV(csv_Eth)
  #df_casesEth.fillna('0')
  #Get rid of extra rows
  #df_casesEth=df_casesEth.drop(['Population'],1).fillna('0')
  #display(df_casesEth)
  #wd.quit()
  
  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('G26',dataToWrite)
      #Write Demographic Data
      #writeTable(df_totals,'Confirmed Case & Death Totals','L15')
      writeTable(df_casesRace,'','H27',ws)
      #writeTable(df_casesEth,'Demographics by Ethnicity','L23')

#MN
#ARCGIS
def runMN(ws, write):
  url='https://services2.arcgis.com/V12PKGiMAH7dktkU/ArcGIS/rest/services/COVIDSUMMARY032920/FeatureServer/0'
  urlRaceCase='https://services2.arcgis.com/V12PKGiMAH7dktkU/arcgis/rest/services/MyMapService/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=EvrHospYes%2C+TotalCases%2C+RaceWht%2C+RaceBlk%2C+RaceAsian%2C+RaceAmerIn%2C+RacePacifi%2C+RaceMultip%2C+RaceOther%2C+RaceHisp%2C++RaceUnk&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urlRaceCase)
  df_case=pd.json_normalize(req.json()['features'])
  df_case=df_case.rename(columns=lambda x: re.sub('properties.','',x))
  df_caseR=df_case.transpose()
  df_caseR=df_caseR.reset_index()
  df_caseR=df_caseR[2:]
  display(df_caseR)

  urlRaceDeath='https://services2.arcgis.com/V12PKGiMAH7dktkU/arcgis/rest/services/MyMapService/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=EvrHospYes%2C++OutcmDied%2C+DeathWht%2C+DeathBlk%2C+DeathAsian%2C+DeathNativ%2C+DeathPacif%2C++DeathRaceM%2C+DeathOther%2C+DeathHisp%2C++DeadUnknow&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urlRaceDeath)
  df_death=pd.json_normalize(req.json()['features'])
  df_death=df_death.rename(columns=lambda x: re.sub('properties.','',x))
  df_deathR=df_death.transpose()
  df_deathR=df_deathR[2:]
  display(df_deathR)

  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('C48',dataToWrite)

      # Write Data To Sheet
      writeTable(df_caseR,'','B51',ws)
      writeTable(df_deathR,'','D51',ws)
     # writeTable(df_tot,'','I19')

#MO
def runMO(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains

  #Sumary Dashboard
  url="https://showmestrong.mo.gov/data-download/"

  #iframe src
  src="https://results.mo.gov/t/COVID19/views/COVID-19DataforDownload/MetricsbyCounty?:embed=y&amp;:showVizHome=no&amp;:host_url=https%3A%2F%2Fresults.mo.gov%2F&amp;:embed_code_version=3&amp;:tabs=yes&amp;:toolbar=yes&amp;:showAppBanner=false&amp;:display_spinner=no&amp;:loadOrderID=0"

  #xpaths for race/ethnicity/totals
  race_xpath="(//span[contains(@id, 'tableauTabbedNavigation_tab_3')])"
  ethnicity_xpath="(//span[contains(@id, 'tableauTabbedNavigation_tab_4')])"

  #xpaths for all of the buttons
  demos_dnld_xpath= '//*[@id="download-ToolbarButton"]'
  crosstab_xpath='//*[@id="DownloadDialog-Dialog-Body-Id"]/div/button[3]'
  csv_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[2]/div[2]/div/label[2]'
  dnld_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[3]/button'

  #csv names
  race_csv='Metrics by Race.csv'
  ethnicity_csv='Metrics by Ethnicity.csv'

  #Order for downloads:
  #Go to url
  #Go to race or ethnicity or totals tab
  #Click the demos_dnld_xpath
  #Click crosstab
  #Click csv
  #Click metric file - NOT NEEDED
  #Click dnld button

  #define getCSV MO style
  def getCSV(metric_xpath,metric_csv):
      wait.until(EC.element_to_be_clickable((By.XPATH,metric_xpath))).click()
      #print("clicked metric on tableau frame")
      time.sleep(10)
      wait.until(EC.element_to_be_clickable((By.XPATH,demos_dnld_xpath))).click()
      #print("clicked download on tableau frame")
      wait.until(EC.element_to_be_clickable((By.XPATH,crosstab_xpath))).click()
      #print("clicked crosstab")
      wait.until(EC.element_to_be_clickable((By.XPATH,csv_xpath))).click()
      #print("clicked csv option")
      wait.until(EC.element_to_be_clickable((By.XPATH,dnld_xpath))).click()
      #print('clicked download button')
      time.sleep(10)
      #make df
      df=pd.read_csv(metric_csv,sep="\t", encoding="utf-16")

      df=df.fillna('0')
      return df

  #initialize the driver, get the url and take a nap
  wd=init_driver()
  wd.get(src)
  wait = WebDriverWait(wd, 240)
  print("Demographics Tableau")
  print(src)
  time.sleep(15)

  #get demographic counts
  race = getCSV(race_xpath, race_csv)
  time.sleep(5)
  ethnicity = getCSV(ethnicity_xpath,ethnicity_csv)
  wd.quit()

  #Fix column names
  tcols=list(race.columns)
  tcols[0]='Race'
  tcols[1]='Confirmed Cases'
  tcols[2]='Probable Cases'
  tcols[3]='Confirmed Deaths'
  tcols[4]='PCR Tests'
  tcols[5]='Antigen Tests'
  tcols[6]='Serology Tests'
  race.columns=tcols
  tcols=list(ethnicity.columns)
  tcols[0]='Ethnicity'
  tcols[1]='Confirmed Cases'
  tcols[2]='Probable Cases'
  tcols[3]='Confirmed Deaths'
  tcols[4]='PCR Tests'
  tcols[5]='Antigen Tests'
  tcols[6]='Serology Tests'
  ethnicity.columns=tcols
  
  #Remove ',' and convert to int for Race
  race['Confirmed Cases']=race['Confirmed Cases'].str.replace(r",","").astype(int)
  race['Probable Cases']=race['Probable Cases'].str.replace(r",","").astype(int)
  race['Confirmed Deaths']=race['Confirmed Deaths'].str.replace(r",","").astype(int)
  race['PCR Tests']=race['PCR Tests'].str.replace(r",","").astype(int)
  race['Antigen Tests']=race['Antigen Tests'].str.replace(r",","").astype(int)
  race['Serology Tests']=race['Serology Tests'].str.replace(r",","").astype(int)
  #For Ethnicity
  ethnicity['Confirmed Cases']=ethnicity['Confirmed Cases'].str.replace(r",","").astype(int)
  ethnicity['Probable Cases']=ethnicity['Probable Cases'].str.replace(r",","").astype(int)
  ethnicity['Confirmed Deaths']=ethnicity['Confirmed Deaths'].str.replace(r",","").astype(int)
  ethnicity['PCR Tests']=ethnicity['PCR Tests'].str.replace(r",","").astype(int)
  ethnicity['Antigen Tests']=ethnicity['Antigen Tests'].str.replace(r",","").astype(int)
  ethnicity['Serology Tests']=ethnicity['Serology Tests'].str.replace(r",","").astype(int)
 
  display(race)
  print("\n")
  display(ethnicity)


  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('J16',dataToWrite)

      # Write Data To Sheet
      writeTable(race,'Race Totals','J17',ws)
      writeTable(ethnicity,'Ethnicity Totals','J28',ws)

#MS
def runMS(ws, write):

  #Extract the latest file from the URL by finding the biggest number
  big=0
  html_page = urllib.request.urlopen("https://msdh.ms.gov/msdhsite/_static/14,0,420,884.html")
  soup = BeautifulSoup(html_page, "html.parser")
  for link in soup.findAll('a', attrs={'href': re.compile(".pdf")}):
    filename = link.get('href')
    segments = filename.rpartition('/')
    index = segments[2].split(".")
    index = int(index[0])
    #print("\nindex")
    #print(index)
    if (index > big) and ('Cases and Deaths' in link['title']):
        big = index
  #print(big)

  #Put the filename back together
  url=('https://msdh.ms.gov/msdhsite/_static/resources/{}.pdf'.format(big))
  print(url)

  print("\nMS Cases by Race & Ethnicity")
  #Grab first half & second havles of tables - strip off first two rows and then rename columns
  casesTable1 = tabula.read_pdf(url,pages=1 ,multiple_tables=True, lattice=True)
  print("table2")
  casesTable2 = tabula.read_pdf(url,pages=2 ,multiple_tables=True, lattice=True)
  deathsTable1 = tabula.read_pdf(url,pages=3 ,multiple_tables=True, lattice=True)
  deathsTable2 = tabula.read_pdf(url,pages=4 ,multiple_tables=True, lattice=True)
  #Read in the url and on Page 4, find the Race & Ethnicity at the bottom of the page
  case1=casesTable1[0]
  case2=casesTable2[0]
  death1=deathsTable1[0]
  death2=deathsTable2[0]
  #Slice off header rows for CASES & DEATHS
  case1=case1[2:]
  case2=case2[2:]
  death1=death1[2:]
  death2=death2[2:]
  #Concatenate Death tables
  death=pd.concat([death1,death2], axis=0, ignore_index=True)

  def column_namer(in_df,out_df):
     #Rename Case Columns - done because of funky behavoir when slicing off header
     cols = list(in_df.columns)
     cols[0] = 'County'
     cols[1] = 'Tot Cases'
     cols[2] = 'Blk NH'
     cols[3] = 'White NH'
     cols[4] = 'AIAN NH'
     cols[5] = 'Asian NH'
     cols[6] = 'Other NH'
     cols[7] = 'Unk NH'
     cols[8] = 'Junk'
     cols[9] = 'Blk Hisp'
     cols[10] = 'White Hisp'
     cols[11] = 'AIAN Hisp'
     cols[12] = 'Asian Hisp'
     cols[13] = 'Other Hisp'
     cols[14] = 'Unk Hisp'
     cols[15] = 'Blk Unk'
     cols[16] = 'White Unk'
     cols[17] = 'AIAN Unk'
     cols[18] = 'Asian Unk'
     cols[19] = 'Other Unk'
     cols[20] = 'Unk Unk'
     out_df.columns = cols
     return out_df

  #Concatenate the dataframes
  column_namer(case1, case1)
  column_namer(case2, case2)
  case=pd.concat([case1,case2], axis=0,ignore_index=True)  #ignore index fixes index wonkiness when concatenating
  #create the checker dataframe (casetotCheck)
  casetotCheck=case.drop('Junk',1).fillna('0')
  casetot=casetotCheck.drop('County',1).astype(int)
  #display(casetotCheck.iloc[11])

  #display(casetot)
  case_tot=casetot.tail(1)  #Select just the last row as the totals to write to google sheet
  case_county=casetot.drop(case.tail(1).index) #Select every row to compare totals
  case_bycounty=case_county.sum(axis=0)

  #display(case_bycounty)
  display(case_tot)

  #Rename Death Columns
  cols = list(death.columns)
  cols[0] = 'County'
  cols[1] = 'Tot Deaths'
  cols[2] = 'Blk NH'
  cols[3] = 'White NH'
  cols[4] = 'AIAN NH'
  cols[5] = 'Asian NH'
  cols[6] = 'Other NH'
  cols[7] = 'Unk NH'
  cols[8] = 'Blk Hisp'
  cols[9] = 'White Hisp'
  cols[10] = 'AIAN Hisp'
  cols[11] = 'Asian Hisp'
  cols[12] = 'Other Hisp'
  cols[13] = 'Unk Hisp'
  cols[14] = 'Blk Unk'
  cols[15] = 'White Unk'
  cols[16] = 'AIAN Unk'
  cols[17] = 'Asian Unk'
  cols[18] = 'Other Unk'
  cols[19] = 'Unk Unk'
  death.columns = cols

  deathtot=death.drop('County',1).fillna('0').astype(int)
  death_tot=deathtot.tail(1) #Last row has the totals
  death_county=deathtot.drop(deathtot.tail(1).index) #All rows BUT the last
  death_county=death_county.sum(axis=0)
  #display(death_county)
  display(death_tot)

  #Do counties match tot?
  print("\nTotal vs. Counties ok if True")
  display(case_bycounty.eq(case_tot))
  display(death_county.eq(death_tot))

  #Totals in progress

  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('B36',dataToWrite)

      # Write Data To Sheet
      writeTable(case_tot,'','A37',ws)
      writeTable(death_tot,'','A40',ws)

#MT
def runMT(ws, write):
 
  #Setup Dates
  todaysDate=date.today()
  latest= todaysDate
  latest = latest.strftime('%Y%m%d')

  #From the link - find the link with DemographicsTable in the filename
  link = 'https://dphhs.mt.gov/publichealth/cdepi/diseases/coronavirusmt/demographics'
  req = requests.get(link)
  soup = BeautifulSoup(req.text, 'html.parser')
  for link in soup.findAll('a', attrs={'href': re.compile(".pdf")}):
    filename = link.get('href')
    segments = filename.rpartition('/')
    namedate = segments[2].split('.')
    name=namedate[0]
    #print(name)
    if 'DemographicTables' in name:
      #print('found')
      url = "https://dphhs.mt.gov{}".format(filename)
      print(url)

 
  #Read in the url, page 1 and select the first table & display the values for the Totals   
  tables = tabula.read_pdf(url,pages=1,multiple_tables=True)
  print("read tables")
  totals = tables[0]
  tcols = list(totals.columns)
  tcols[0] = 'MT'
  tcols[1] = 'Totals'
  totals.columns = tcols
  print("setup totals")

  hosp = tables[2]
  tcols = list(hosp.columns)
#  tcols[0] = 'Hospitalization Status'
#  tcols[1] = 'Number of Cases'
  tcols[0] = 'Status'
  tcols[1] = 'Cases'
  hosp.columns = tcols
  print("setup hosp columns")

  #Replace NaN with ''
  totals=totals.fillna('0').drop(totals.index[0])
  #totals=totals.fillna('0')
  #print("did fillna on totals and lopped the top row")
  #display(totals)

  #hosp=hosp.fillna('0').drop(hosp.index[0])
  hosp=hosp.fillna('0')
  #print("did fillna on hosp and lopped the top row")
  #display(hosp)

  #Remove parens from numbers
  display(totals['Totals'].dtypes)
  if (totals['Totals'].dtypes == 'str'):
    totals['Totals']=totals['Totals'].str.replace(r"1E5","100000",regex=True)
    totals['Totals']=totals['Totals'].str.replace(r"\(.*\)","",regex=True).astype(int)
  #print("totals regex")
  #hosp['Number of Cases']=hosp['Number of Cases'].replace("\(.*\)","",regex=True).astype(int)
  hosp['Cases']=hosp['Cases'].str.replace(r"\(.*\)","",regex=True).astype(int)
  print('MT Totals Table\n')
  display(totals)
  print('MT Hospitalized\n')
  display(hosp)

  #Read in the url and on Page 4, find the Race & Ethnicity at the bottom of the page
  raceeth = tabula.read_pdf(url,pages=5,multiple_tables=True, lattice=True)
  race=raceeth[0]
  #print("\n Race from page 5")
  #print(type(race))
  #print(race.columns)

  #Rename columns
  cols = list(race.columns)
  cols[0] = 'Race'
  cols[1] = 'Cases'
  cols[2] = 'Deaths'
  race.columns = cols
  #Remove "(contents)" from Case numbers
  race['Cases']=race['Cases'].replace("\(.*\)","",regex=True)
  race['Deaths']=race['Deaths'].replace("\(.*\)","",regex=True)
  #Replace < 5 with 1
  race['Deaths']=race['Deaths'].replace("< 5","1",regex=True)
  raceeth = race.drop(race.index[0])
  raceeth=raceeth.fillna('0')

  #Convert Str to Numeric
  raceeth = raceeth.astype({"Cases": int, "Deaths": int})
  print("\nRace and Ethnicity Tables")
  display(raceeth)

  if write == True:
    # Write Paste Date To Sheet
     dataToWrite = [[date.today().strftime('%m/%d')]]
     #ws.update('J18',dataToWrite)

     # Write Data To Sheet
     writeTable(raceeth,'Race Table','H19',ws)
     writeTable(totals,'Totals  Table','H33',ws)
     writeTable(hosp,'Ever Hospitalized','H39',ws) 


#NC
def runNC(ws, write):

  #totals ARGCIS
  url='https://services.arcgis.com/iFBq2AW9XO0jYYF7/ArcGIS/rest/services/NCCovid19/FeatureServer/0/query?outStatistics=%5B%7B%27onStatisticField%27%3A+%27Hosp%27%2C+%27statisticType%27%3A+%27max%27%7D%2C+%7B%27onStatisticField%27%3A+%27Total%27%2C+%27statisticType%27%3A+%27sum%27%7D%2C+%7B%27onStatisticField%27%3A+%27Deaths%27%2C+%27statisticType%27%3A+%27sum%27%7D%5D&where=1%3D1'
  #iframe src
  src ='https://public.tableau.com/views/NCDHHS_COVID-19_DataDownload/Demographics'

  #for Totals
  urltot='https://services.arcgis.com/iFBq2AW9XO0jYYF7/ArcGIS/rest/services/NCCovid19/FeatureServer/0/query?where=1%3D1&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&returnGeodetic=false&outFields=&returnGeometry=true&returnCentroid=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=%5B%7B%27onStatisticField%27%3A+%27Hosp%27%2C+%27statisticType%27%3A+%27max%27%7D%2C+%7B%27onStatisticField%27%3A+%27Total%27%2C+%27statisticType%27%3A+%27sum%27%7D%2C+%7B%27onStatisticField%27%3A+%27Deaths%27%2C+%27statisticType%27%3A+%27sum%27%7D%5D&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urltot)
  df_tot=pd.json_normalize(req.json()['features'])
  df_tot=df_tot.rename(columns=lambda x: re.sub('properties.','',x))
  display(df_tot)

  #xpaths for all of the buttons
  demos_dnld_xpath= '//*[@id="download-ToolbarButton"]'
  crosstab_xpath='//*[@id="DownloadDialog-Dialog-Body-Id"]/div/fieldset/button[3]'
  csv_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[2]/div[2]/div/label[2]'
  race_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[4]/div/div/div/div'
  eth_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[2]/div/div/div/div'
  dnld_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[3]/button'

  #csv names
  race_csv='TABLE_RACE.csv'
  ethnicity_csv='TABLE_ETHNICITY.csv'

  #Order for downloads
  #Go to url->demos_dnld_xpath->crosstab_xpath->csv_xpath->metric_xpath->dnld_xpath

  #define getCSV NC style
  def getCSV(metric_xpath,metric_csv):
      demos_dnld_btn=wd.find_element_by_xpath(demos_dnld_xpath)
      demos_dnld_btn.click()
      #print("clicked download on tableau frame")
      time.sleep(5)
      crosstab_btn=wd.find_element_by_xpath(crosstab_xpath)
      crosstab_btn.click()
      #print("clicked crosstab")
      time.sleep(15)
      ##select csv option
      csv_btn=wd.find_element_by_xpath(csv_xpath)
      csv_btn.click()
      print("clicked csv option")
      time.sleep(5)
      wd.find_element_by_xpath(metric_xpath).click()
      #print("clicked metric on tableau frame")
      time.sleep(5)
      dnld_btn=wd.find_element_by_xpath(dnld_xpath)
      dnld_btn.click()
      print('clicked download button')
      time.sleep(10)

      df=pd.read_csv(metric_csv,sep="\t", encoding="utf-16")
      df=df.fillna('0')
      return df

  #inititalize the driver, get the url and take a nap
  wd=init_driver()
  wd.get(src)
  time.sleep(5)
  #print("Demographics Tableau")
  print(src)
  time.sleep(5)

  #get total counts
  race = getCSV(race_xpath, race_csv)
  time.sleep(5)
  ethnicity = getCSV(eth_xpath,ethnicity_csv)
  time.sleep(5)
  wd.quit()

  #Remove , and convert to int for Race
  race['Cases']=race['Cases'].str.replace(r",","").astype(int)
  race['Deaths']=race['Deaths'].str.replace(r",","").astype(int)
  #For Ethnicity
  ethnicity['Cases']=ethnicity['Cases'].str.replace(r",","").astype(int)
  ethnicity['Deaths']=ethnicity['Deaths'].str.replace(r",","").astype(int)

  display(race)
  display(ethnicity)

  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('H19',dataToWrite)

      # Write Data To Sheet
      writeTable(race,'Race Totals','G20',ws)
      writeTable(ethnicity,'Ethnicity Totals','G28',ws)

#ND
def runND(ws,write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains

  url = 'https://app.powerbigov.us/view?r=eyJrIjoiYTExNDg5YjctOTQxZi00YmU2LWI5Y2YtNWVkMDFmNDUyN2FlIiwidCI6IjJkZWEwNDY0LWRhNTEtNGE4OC1iYWUyLWIzZGI5NGJjMGM1NCJ9'

  def colstr2int(df,col):
    df.loc[:,col] = df.loc[:,col].replace(',','', regex=True)
    df[col] = df[col].astype('int')

  wd = init_driver()
  wd.get(url)
  wait = WebDriverWait(wd, 20)
  wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "i[title='Next Page']"))).click()
  print('clicked Next Page')
  time.sleep(1)
  wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "i[title='Next Page']"))).click()
  print('clicked Next Page')

  cum_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[12]/transform/div/div[3]/div/visual-modern/div/button")))
  cum_button.click()
  print('clicked Cumulative')

  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-modern[13]/transform/div/div[3]/div/visual-modern")))
  ActionChains(wd).context_click(elements[0]).perform()
  #  time.sleep(10)
  wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='Show as a table']"))).click()
  print('clicked Show Table')

  cats = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive ']")))
  for element in elements:
    cats.append(element.text)

  vals = []
  elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive tablixAlignRight ']")))
  for element in elements:
    vals.append(element.text)

  df = pd.DataFrame([cats[-7:],vals]).T
  df.columns = ['Category', 'Cases']
  colstr2int(df,'Cases')
  df.sort_values('Category',ascending=True,inplace=True)
  display(df)
  wd.quit()

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('D22',dataToWrite)

    # Write Data To Sheet
    writeTable(df,'','C23',ws)

#NE
def runNE(ws, write):
    #totals, case dems, death dems
  print("NE Dems")
  #NE Deaths  FIXED
  urldeath='https://gis.ne.gov/Enterprise/rest/services/Covid19MapV6/MapServer/4/query?f=json&where=1%3D1&returnGeometry=false&spatialRel=esriSpatialRelIntersects&outFields=*&outSR=102100&resultOffset=0&resultRecordCount=50'
  #headers = {"User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:79.0) Gecko/20100101 Firefox/79.0"}
  req = requests.get(urldeath)
  #Read in from ArcGIS->replace NaN
  df_death=pd.json_normalize(req.json()['features'])
  df_death=df_death.fillna('0').astype('int64')
  df_death= df_death.rename(columns={'attributes.DEATH_TOTAL':'Deaths'})
  df_death=df_death['Deaths'].astype('int64')
  #print("Deaths")
  #display(df_death)

  #NE Hospitalizations FIXED
  urlhosp='https://gis.ne.gov/Enterprise/rest/services/Covid19MapV6/MapServer/5/query?f=json&where=1%3D1&returnGeometry=false&spatialRel=esriSpatialRelIntersects&outFields=*&outStatistics=%5B%7B%22statisticType%22%3A%22avg%22%2C%22onStatisticField%22%3A%22HOSP_0_19%22%2C%22outStatisticFieldName%22%3A%22HOSP_0_19%22%7D%2C%7B%22statisticType%22%3A%22avg%22%2C%22onStatisticField%22%3A%22HOSP_20_34%22%2C%22outStatisticFieldName%22%3A%22HOSP_20_34%22%7D%2C%7B%22statisticType%22%3A%22avg%22%2C%22onStatisticField%22%3A%22HOSP_35_44%22%2C%22outStatisticFieldName%22%3A%22HOSP_35_44%22%7D%2C%7B%22statisticType%22%3A%22avg%22%2C%22onStatisticField%22%3A%22HOSP_45_54%22%2C%22outStatisticFieldName%22%3A%22HOSP_45_54%22%7D%2C%7B%22statisticType%22%3A%22avg%22%2C%22onStatisticField%22%3A%22HOSP_55_64%22%2C%22outStatisticFieldName%22%3A%22HOSP_55_64%22%7D%2C%7B%22statisticType%22%3A%22avg%22%2C%22onStatisticField%22%3A%22HOSP_65_74%22%2C%22outStatisticFieldName%22%3A%22HOSP_65_74%22%7D%2C%7B%22statisticType%22%3A%22avg%22%2C%22onStatisticField%22%3A%22HOSP_75_84%22%2C%22outStatisticFieldName%22%3A%22HOSP_75_84%22%7D%2C%7B%22statisticType%22%3A%22avg%22%2C%22onStatisticField%22%3A%22HOSP_85UP%22%2C%22outStatisticFieldName%22%3A%22HOSP_85UP%22%7D%5D&outSR=102100'
  req = requests.get(urlhosp)
  #Read in from ArcGIS->replace NaN
  df_hosp=pd.json_normalize(req.json()['features'])
  df_hosp=df_hosp.fillna('0')
  df_hosp['Hospitalized']=df_hosp.sum(axis=1)
  df_hosp=df_hosp['Hospitalized'].astype('int64')
  #display(df_hosp)

  #NE Case Totals FIXED
  urlpos='https://gis.ne.gov/Enterprise/rest/services/Covid19MapV6/MapServer/1/query?f=json&where=1%3D1&returnGeometry=false&spatialRel=esriSpatialRelIntersects&outFields=*&orderByFields=AllTestsAsOfThisDate%20desc&outSR=102100&resultOffset=0&resultRecordCount=1'
  req = requests.get(urlpos)
  #Read in from ArcGIS & replace NaN
  df_pos=pd.json_normalize(req.json()['features'])
  df_pos=df_pos.fillna('0')
  df_pos= df_pos.rename(columns={'attributes.TotalPositiveAsOfThisDate':'Cases'})
  df_pos=df_pos['Cases'].astype('int64')
  #print("Positive Cases")
  #display(df_pos)

  #Combine 3 dataframes to get a total dataframe
  df_tot=pd.concat([df_pos,df_hosp,df_death],axis=1)
  display(df_tot)

  #NE Demographics
  urldems='https://gis.ne.gov/enterprise/rest/services/Covid19MapV6/MapServer/6/query?f=json&where=1%3D1&returnGeometry=false&spatialRel=esriSpatialRelIntersects&outFields=*&orderByFields=Category%20desc&resultOffset=0&resultRecordCount=500'
  req = requests.get(urldems)
  #Read in from ArcGIS->replace NaN->slice unneeded columns->rotate->slice number indices->
  #remove unneeded new columns-> remove attributes. from race & ethnicity categories
  df_cases=pd.json_normalize(req.json()['features'])
  df_cases=df_cases.fillna('0')
  df_casesF=df_cases.transpose().reset_index()
  df_casesF=df_casesF[1:12]
  df_casesF=df_casesF.reset_index()
  df_casesF=df_casesF.drop(['level_0',0,4],1)
  df_casesF['index']=df_casesF['index'].str.replace(r"attributes.","")
  display(df_casesF)

  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('B38',dataToWrite)

      # Write Data To Sheet
      writeTable(df_tot,'','C39',ws)
      writeTable(df_casesF,'Case Totals','C41',ws)

# NH
def runNH(ws, write):

  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains

  #interactive equity dash
  #iframe
  url="https://www.nh.gov/t/DHHS/views/COVID19InteractiveEquityDashboard/COVID19InteractiveEquityDashboard?:isGuestRedirectFromVizportal=y&amp;:embed=y"
  #Cases Button xpaths
  cases_xpath='//*[@id="[Parameters].[Parameter 3]_0"]'
  deaths_xpath='//*[@id="[Parameters].[Parameter 3]_1"]'

  #compare rates by
  raceeth_xpath = "//*[@id='[Parameters].[Parameter 1]_0']"

  #compare numbers by sex "No"
  sex_xpath = "//*[@id='[Parameters].[Parameter 5]_0']"

  #download form options
  data_btn_xpath = "//*[@id='view1917354692013186876_13459133749204884095']/div[1]/div[2]/canvas[2]"

  #downloaded csv paths
  csv_Cases = "CrudeCount-Cases.csv"
  csv_Deaths = "CrudeCount-Deaths.csv"

  def getCSV(demo_xpath, demo_csv):
        #Demographics for Cases
        print("Demographics for Cases/Deaths")
        wait.until(EC.element_to_be_clickable((By.XPATH, demo_xpath))).click()
        print("clicked Cases/Deaths")

        #choose race or ethnicity vs. Age
        wait.until(EC.element_to_be_clickable((By.XPATH, raceeth_xpath))).click()
        print("clicked race or ethnicity vs age")

        #choose sex yes or no view
        wait.until(EC.element_to_be_clickable((By.XPATH, sex_xpath))).click()
        print("chose sex=no")

        #click data option
        wait.until(EC.element_to_be_clickable((By.XPATH, data_btn_xpath))).click()
        print("clicked download button")   

        #download view and convert to df
        df = pd.read_csv(demo_csv, sep=",", encoding="utf-8", na_values=['']) #dumb tableau encoding
        df = df.fillna('0')

        return df

  #cases
  wd=init_driver()
  wd.get(url)
  wait = WebDriverWait(wd, 20)
  time.sleep(5)
  print("\nTableau URL")
  print(url)

  #make df_casesRace
  df_casesRace = getCSV(cases_xpath, csv_Cases)
  tcols = list(df_casesRace.columns)
  tcols[0] = 'Demographic'
  tcols[1] = 'Sex'
  tcols[2] = 'Cases'
  tcols[3] = 'Suppression'
  tcols[4] = 'Date'
  df_casesRace.columns = tcols
  df_casesRace['Cases'] = df_casesRace['Cases'].astype('int')
  print("-" *10)
  display(df_casesRace)
  wd.quit()


  #deaths
  wd=init_driver()
  wd.get(url)
  wait = WebDriverWait(wd, 20)
  time.sleep(5)
  print("\nTableau URL")
  print(url)

  #make df_deathsRace
  df_deathsRace = getCSV(deaths_xpath, csv_Deaths)
  tcols = list(df_deathsRace.columns)
  tcols[0] = 'Demographic'
  tcols[1] = 'Sex'
  tcols[2] = 'Suppression'
  tcols[3] = 'Deaths'
  tcols[4] = 'Date'
  df_deathsRace.columns = tcols
  df_deathsRace['Deaths'] = df_deathsRace['Deaths'].astype(int)
  print("-" *10)
  print("\nDeaths")
  display(df_deathsRace)

  #Summary Dash for Totals
  #urlSum = "https://www.nh.gov/covid19/dashboard/case-summary.htm"
  urlSumT="https://www.nh.gov/t/DHHS/views/COVID-19CaseDashboard/Summary?:iid=1&amp;:isGuestRedirectFromVizportal=y&amp;:embed=y"
  wd.quit()
  wd=init_driver()
  wd.get(urlSumT)
  wait = WebDriverWait(wd, 20)
  time.sleep(5)

  #xpaths
  summary_xpath ='//*[@id="tableauTabbedNavigation_tab_0"]'
  race_xpath='//*[@id="tabZoneId125"]/div/div/div/div[1]/div[5]/div[1]/canvas'
  metrics_xpath='//*[@id="view2676828700685879255_11683864725981880803"]/div[1]/div[2]/canvas[2]'
  totals_xpath="//*[@id='view2676828700685879255_12402419230462992794']/div[1]/div[2]/canvas[2]"
  casepercent_xpath="//*[@id='title2676828700685879255_18320527963907039571']/div[1]/div/span/div/span[2]"
  hosppercent_xpath="//*[@id='title2676828700685879255_18320527963907039571']/div[1]/div/span/div/span[4]"
  deathpercent_xpath="//*[@id='title2676828700685879255_18320527963907039571']/div[1]/div/span/div/span[6]"
  
  #press summary tab
  print("\nSummary Tab")
  wait.until(EC.element_to_be_clickable((By.XPATH, summary_xpath))).click()

  print("\nPercents of Known Demographics")
  casepercent=wd.find_element_by_xpath(casepercent_xpath)

  #print("\nCase Percent")
  casepercent=casepercent.text
  casepercent=casepercent.replace('%','', 1)
  casepercent=float(casepercent)
  #print(casepercent)

  #print("\nHospPercent")
  hosppercent=wd.find_element_by_xpath(hosppercent_xpath)
  hosppercent=hosppercent.text
  hosppercent=float(hosppercent.replace('%','', 1))
  #print(hosppercent)

  #print("\nDeathPercent")
  deathpercent=wd.find_element_by_xpath(deathpercent_xpath)
  deathpercent=deathpercent.text
  deathpercent=float(deathpercent.replace('%','', 1))
  #print(deathpercent)

  # initialize list of lists
  dataPercent = {'Percent Known':['Case %','Hosp %','Deaths %'],'Value':[casepercent, hosppercent, deathpercent]}
  ##Put the reads into one row of a matrix, then convert to dataframe
  knownpercent = pd.DataFrame(dataPercent)
  display(knownpercent)

  wd.quit()

  #Hospitalizations & Totals
  wd=init_driver()

  #Sumary Dashboard
  urlSum="https://www.nh.gov/t/DHHS/views/COVID-19CaseDashboard/Summary?:iid=1&amp;:isGuestRedirectFromVizportal=y&amp;:embed=y"
  wd.get(urlSum)
  wait = WebDriverWait(wd, 20)
  time.sleep(5)
  print("\nTableau URL")
  print(urlSum)

  #xpaths
  summary_xpath ='//*[@id="tableauTabbedNavigation_tab_0"]'
  dnld_xpath = '//*[@id="download-ToolbarButton"]/span[1]'
  sumpdf_xpath = '//*[@id="DownloadDialog-Dialog-Body-Id"]/div/button[4]'
  view_xpath='//*[@id="tableau-ui-1612745976402"]/span'
  dnldpdf_xpath='//*[@id="PdfDialog-Dialog-Body-Id"]/div/div[2]/div[4]/button'

  #press summary tab
  print("Choose Summary Tab")
  wait.until(EC.element_to_be_clickable((By.XPATH, summary_xpath))).click()

  #press download button
  print("Choose Download Tab")
  wait.until(EC.element_to_be_clickable((By.XPATH, dnld_xpath))).click()

  #Choose pdf
  print("Choose PDF")
  wait.until(EC.element_to_be_clickable((By.XPATH, sumpdf_xpath))).click()

  #Choose Download
  print("Download PDF")
  wait.until(EC.element_to_be_clickable((By.XPATH, dnldpdf_xpath))).click()

  summaryTable = tabula.read_pdf('Summary.pdf',lattice=True, multiple_tables=True, pages=1)
  #display(summaryTable)

  #Totals Table
  totals = summaryTable[0].iloc[3:]
  tcols=list(totals.columns)
  tcols[2] = 'Infections'
  tcols[4] = 'Ever Hospitalized'
  tcols[8] = 'Deaths'
  totals.columns = tcols
  cols=['Infections','Ever Hospitalized','Deaths']
  totals=totals[cols]
  totals['Infections']=totals['Infections'].str.replace(r",",'',1).astype(int)
  totals['Ever Hospitalized']=totals['Ever Hospitalized'].str.replace(r",",'',1).astype(int)
  totals['Deaths']=totals['Deaths'].str.replace(r",",'',1).astype(int)

  totals=totals.fillna('0')
  display(totals)

  #Hospitalizations
  hosp=summaryTable[8].iloc[1:]
  tcols=list(hosp.columns)
  tcols[0] = 'Race/Ethnicity'
  tcols[3] = 'Hosp'
  hosp.columns = tcols
  cols=['Race/Ethnicity','Hosp']
  hosp.index =['White','Hispanic or Latino','Black or African American','Other','Asian','Total']
  hosp=hosp[cols]
  hosp['Race/Ethnicity']=['White','Hispanic or Latino','Black or African American','Other','Asian','Total']
  hosp['Hosp']=hosp['Hosp'].str.replace(r",",'',1).astype(int)
  display(hosp)

  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('F31',dataToWrite)

      # Write Data To Sheet
      #writeTable(df_totals,'Race & Ethnicity Totals','G32',ws)
      writeTable(df_casesRace,'Confirmed Cases by Race & Ethnicity','G31',ws)
      writeTable(df_deathsRace,'Confirmed Deaths by Race & Ethnicity','S31',ws)
      writeTable(knownpercent,'Percent of Known Demographics','G43',ws)
      writeTable(totals,'Race & Ethnicity Totals','G48',ws)
      writeTable(hosp,'Hosp by Race & Ethnicity','M31',ws)

# NM
def runNM(ws, write):

  #totals, case dems, death dems
  print("NM Out")
  urlAll='https://e7p503ngy5.execute-api.us-west-2.amazonaws.com/prod/GetPublicStatewideData'
  req=requests.get(urlAll)
  display(req)
  df_cases=pd.json_normalize(req.json()['data'])
  df_cases=df_cases.drop(['created','cvDataId','updated','archived','currentHospitalizations','recovered','male','female','genderNR','0-9','10-19','20-29','30-39','40-49','50-59','60-69','70-79','80-89','90+','ageNR'],1)
  df_casesR=df_cases.transpose().reset_index()

  display(df_casesR)

  #Death Demographics
  url = 'https://cv.nmhealth.org/epidemiology-reports/'
  req = requests.get(url)
  soup = BeautifulSoup(req.text, 'html.parser')
  a = soup.find('a', string=re.compile("Download The Latest COVID-19 Mortality Report"))
  url_mort = a['href']
  tables = tabula.read_pdf(url_mort,pages=6,multiple_tables=False,stream=True,pandas_options={'header': 1})
  display(tables)
  death_table=tables[0].iloc[:,[0,-1]]
  display(death_table)
  death_table['Total']=death_table['Total'].fillna('0')
  #slice the split header rows
  #death_table2=death_table.drop([1,3,5]).reset_index()
  #death_table2=death_table2.drop(['index'],1)
  #display(death_table2)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('B36',dataToWrite)

    # Write Data To Sheet
    writeTable(df_casesR,'','B37',ws)
    writeTable(death_table,'','E36',ws)

# NV
def runNV(ws,write):

  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC

  def get_df(col2):
    cats = []
    elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='expandableContent pivotTableCellWrap ']")))
    for element in elements:
      if not element.text.isupper():
        cats.append(element.text)
    vals = []
    elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellNoWrap tablixAlignLeft ']")))
    for element in elements:
        vals.append(element.text)
    df = pd.DataFrame([cats, vals]).T
    df.columns=['Category',col2]
    return df

  url = 'https://app.powerbigov.us/view?r=eyJrIjoiMjA2ZThiOWUtM2FlNS00MGY5LWFmYjUtNmQwNTQ3Nzg5N2I2IiwidCI6ImU0YTM0MGU2LWI4OWUtNGU2OC04ZWFhLTE1NDRkMjcwMzk4MCJ9'

  wd = init_driver()
  wd.get(url)
  wait = WebDriverWait(wd, 20)

  wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[aria-label=' Page navigation Button. Demographics']"))).click()
  df_cases = get_df('Cases')

  wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-group[3]/transform/div/div[2]/visual-container-modern[3]/transform/div/div[3]/div/visual-modern/div/button"))).click()
  df_deaths = get_df('Deaths')

  wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@id='pvExplorationHost']/div/div/exploration/div/explore-canvas-modern/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container-group[3]/transform/div/div[2]/visual-container-modern[2]/transform/div/div[3]/div/visual-modern/div/button"))).click()
  df_tests = get_df('Tests')

  df = df_cases.merge(df_deaths,how='left',on='Category')
  df = df.merge(df_tests,how='left',on='Category')
  df.drop(df.index[0:10],inplace=True)
  display(df)

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('B37',dataToWrite)

    # Write Data To Sheet
    writeTable(df,'','A38',ws)

# NY
#import pandas as pd
#import requests

def runNY(ws, write):
  url = 'https://raw.githubusercontent.com/nychealth/coronavirus-data/master/totals/probable-confirmed-by-race.csv'
  df_nyc_deaths = pd.read_csv(url)
  print('NYC Deaths by Race')
  display(df_nyc_deaths)


  url = 'https://raw.githubusercontent.com/nychealth/coronavirus-data/master/totals/by-race.csv'
  df_nyc_cases_hosp = pd.read_csv(url)
  print('\nNYC Cases and Hospitalizations by Race\n')
  display(df_nyc_cases_hosp)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('J25',dataToWrite)

    # Write Data To Sheet
    writeTable(df_nyc_deaths,'','J17',ws)
    writeTable(df_nyc_cases_hosp,'','O17',ws)

# OR
def runOR(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC

  def getCSV_OR(csv_file,first=1,contains='',header='infer'):
    wait = WebDriverWait(wd, 20)
    if first == 1:
      print('Waiting on frame...')
      wait.until(EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, "iframe[title='Data Visualization']")))
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ".tab-icon-download"))).click()
    print('Clicked download')
    wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Crosstab']"))).click()
    print('Clicked crosstab')
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "label[data-tb-test-id='crosstab-options-dialog-radio-csv-Label']"))).click()
    print('Clicked csv button')
    if contains == '':
      wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='" + csv_file + "']"))).click()
    else:
      wait.until(EC.element_to_be_clickable((By.XPATH, "//div[contains(@title," + contains + ")]")))
      #wait.until(EC.element_to_be_clickable((By.XPATH, "//div[contains(@title," + contains + ")]"))).click()
    print('Clicked sheet')
    wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Download']"))).click()
    print('Clicked download')
    time.sleep(5)
    return pd.read_csv(csv_file + '.csv',skiprows=1,sep="\t", encoding="utf-16",header=header)

  url = 'https://public.tableau.com/profile/oregon.health.authority.covid.19#!/vizhome/OregonCOVID-19CaseDemographicsandDiseaseSeverityStatewide-SummaryTable/DemographicDataSummaryTable'
  url_hosp_total = 'https://public.tableau.com/profile/oregon.health.authority.covid.19#!/vizhome/OregonHealthAuthorityCOVID-19SummaryTable_15889676399110/OregonsEpiCurveSummaryTable'

  deaths_csv = 'Demographic Data - Death Status'
  hosp_csv = 'Demographic Data - Hospitalizaton Status'
  epi_csv = "Oregon's Epi Curve Summary Table"

  wd = init_driver()
  wd.get(url)

  df = getCSV_OR(deaths_csv)
  df_cases_deaths = df[~df['Demographic'].str.contains('Group')]
  df_cases_deaths = df_cases_deaths.fillna(0)

  cols = df_cases_deaths.columns[2:5]
  df_cases_deaths.loc[:,cols] = df_cases_deaths.loc[:,cols].replace(',','', regex=True)
  df_cases_deaths[cols] = df_cases_deaths[cols].astype('int')

  df_cases_deaths.loc[24,cols] = df_cases_deaths[cols][7:9].sum()
  df_cases_deaths.drop(23,inplace=True)
  df_cases_deaths = df_cases_deaths.drop(df_cases_deaths.columns[[2]],axis=1)
  df_cases_deaths.rename(columns={"Total": "Cases"},inplace=True)
  display(df_cases_deaths)

  df = getCSV_OR(hosp_csv,0)

  df_hosp = df[~df['Demographic'].str.contains('Group')]
  df_hosp = df_hosp.drop(df_hosp.columns[[3,4]],axis=1)
  df_hosp.loc[:,'Hospitalized'] = df_hosp.loc[:,'Hospitalized'].replace(',','', regex=True)
  df_hosp['Hospitalized'] = df_hosp['Hospitalized'].astype('int')
  display(df_hosp)

  wd.quit()

  wd=init_driver()
  wd.get(url_hosp_total)
  df = getCSV_OR(epi_csv,1,"'Epi Curve Summary Table'",None)
  hosp_total = df.iloc[0,3]
  hosp_total = int(hosp_total.replace(',',''))
  print('hosp_total = ',hosp_total)

  wd.quit()

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('G21',dataToWrite)

    # Write Paste Date To Sheet
    dataToWrite = [[hosp_total]]
    #ws.update('J48',dataToWrite)

    # Write Data To Sheet
    writeTable(df_cases_deaths,'','H21',ws)
    writeTable(df_hosp,'','H36',ws)

# PA
def runPA(ws, write):

  #Totals
  urlTot='https://services1.arcgis.com/Nifc7wlHaBPig3Q3/arcgis/rest/services/COVID_PA_Counties/FeatureServer/0//query?where=1%3D1&objectIds=68&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&returnGeodetic=false&outFields=County%2CCases%2C+Confirmed%2C+Probable%2C+Deaths&returnGeometry=true&returnCentroid=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urlTot)
  df_tot=pd.json_normalize(req.json()['features'])
  df_tot=df_tot.rename(columns=lambda x:re.sub('properties.','',x))
  df_tots=df_tot.loc[:,['Cases','Confirmed','Probable','Deaths']]
  display(df_tots)

  #PA Cases
  print("\nPA Cases")
  urlRaceCases='https://services1.arcgis.com/Nifc7wlHaBPig3Q3/ArcGIS/rest/services/racedata/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=Positive_Cases%2C+Race&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urlRaceCases)
  df_Rcases=pd.json_normalize(req.json()['features'])
  df_Rcases=df_Rcases.rename(columns=lambda x: re.sub('properties.','',x))
  df_casesR=df_Rcases.loc[:,['Race','Positive_Cases']]
  display(df_casesR)

  urlEthCases='https://services1.arcgis.com/Nifc7wlHaBPig3Q3/ArcGIS/rest/services/ethniccases/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=Ethnicity%2C+Cases&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urlEthCases)
  df_Ecases=pd.json_normalize(req.json()['features'])
  df_Ecases=df_Ecases.rename(columns=lambda x: re.sub('properties.','',x))
  df_casesE=df_Ecases.loc[:,['Ethnicity','Cases']]
  print("\nBy Ethnicity")
  display(df_casesE)

  #PA Deaths
  print("PA Deaths")
  urlEthDeath='https://services1.arcgis.com/Nifc7wlHaBPig3Q3/ArcGIS/rest/services/Death_Ethnicity/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=Ethnicity+%2CF__of_Deaths&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  urlRaceDeath='https://services1.arcgis.com/Nifc7wlHaBPig3Q3/ArcGIS/rest/services/Death_Race/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=Race%2C+Deaths&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pgeojson&token='
  req=requests.get(urlRaceDeath)
  df_Rdeath=pd.json_normalize(req.json()['features'])
  df_Rdeath=df_Rdeath.rename(columns=lambda x: re.sub('properties.','',x))
  df_deathR=df_Rdeath.loc[:,['Race','Deaths']]
  print("\nby Race")
  display(df_deathR)

  req=requests.get(urlEthDeath)
  df_Edeath=pd.json_normalize(req.json()['features'])
  df_Edeath=df_Edeath.rename(columns=lambda x: re.sub('properties.','',x))
  df_deathE=df_Edeath.loc[:,['Ethnicity','F__of_Deaths']]

  print("\nby Ethnicity")
  display(df_deathE)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('H16',dataToWrite)

    # Write Data To Sheet
    writeTable(df_casesR, 'Cases by Race', 'H25',ws)
    writeTable(df_casesE,'Cases by Ethnicity','H34',ws)
    writeTable(df_deathR,'Deaths by Race','K25',ws)
    writeTable(df_deathE,'Deaths by Ethnicity','K34',ws)
    writeTable(df_tots,'Totals', 'H17',ws)

# RI

def runRI(ws,write):

#  key_ri = '1c2QrNMz8pIbYEKzMJL7Uh2dtThOJa2j1sSMwiDo5Gz4'
#  try:
#    wb_ri = gc.open_by_key(key_ri)
#  except NameError:
#    auth.authenticate_user()
#    gc = gspread.oauth()
#    gc = gspread.authorize(GoogleCredentials.get_application_default())
#    wb_ri = gc.open_by_key(key_ri)

  # Get Totals from Summary tab, discard unused rows
  summary_url = 'https://docs.google.com/spreadsheets/d/1c2QrNMz8pIbYEKzMJL7Uh2dtThOJa2j1sSMwiDo5Gz4/gviz/tq?tqx=out:csv&sheet=Summary'
  df_totals = pd.read_csv(summary_url)
  df_totals = df_totals.iloc[[9,12,14,24],:]
  display(df_totals)

  # Get race info from Demographics tab, wrangle data
  demo_url = 'https://docs.google.com/spreadsheets/d/1c2QrNMz8pIbYEKzMJL7Uh2dtThOJa2j1sSMwiDo5Gz4/gviz/tq?tqx=out:csv&sheet=Demographics'
  df_demo= pd.read_csv(demo_url,header=None)
  df_demo.drop(df_demo.columns[[2, 4, 6, 8,9,10,11,12,13,14]], axis = 1, inplace = True)
  header = df_demo.T[0].str.split(' N=', expand=True)
  df_demo = header.T.append(df_demo.iloc[1:,:])
  df_demo.reset_index(drop=True,inplace=True)
  df_demo.iloc[0:2,0]=['','Totals']
  df_demo.columns = df_demo.iloc[0,:].replace({'\n': ''}, regex=True)
  totals = df_demo.iloc[[1]]
  df_demo = totals.append(df_demo.iloc[16:27,:])
  df_demo = df_demo.replace({'<5': '1'}, regex=True)
  df_demo = df_demo.fillna('')

  cols = list(df_demo.columns)
  cols = [s for s in cols if s != '']
  df_demo[cols] = df_demo[cols].replace(',','',regex=True)
  df_demo[cols] = df_demo[cols].apply(pd.to_numeric,errors='coerce')
  df_demo = df_demo.fillna('0')
  df_demo[cols] = df_demo[cols].astype('int')
  cols = list(df_demo.columns)
  cols[0] = 'Category'
  df_demo.columns = cols

  # Add excess to Unknowns
  unknown_row = df_demo.index[df_demo['Category'] == 'Unknown or pending further information']
  tot_row = df_demo.index[df_demo['Category'] == 'Totals']
  tot = df_demo['Deaths'][tot_row[0]]
  subtot = df_demo['Deaths'][(tot_row[0]+1):(len(df_demo['Deaths'])-1)].sum()
  df_demo.loc[unknown_row[0],'Deaths'] = tot - subtot
  df_demo = df_demo.iloc[:,0:5]
  display(df_demo)

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('R32',dataToWrite)

    # Write Data To Sheet
    writeTable(df_totals,'Summary','R19',ws)
    writeTable(df_demo,'Demographics','T19',ws)

# SD
def runSD(ws,write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from selenium.webdriver import ActionChains

  url = 'https://app.powerbigov.us/view?r=eyJrIjoiZGJjZWYwZmEtMWVjMy00OTUwLThkMzgtZDhkNzAwOWQ3YzNlIiwidCI6IjcwYWY1NDdjLTY5YWItNDE2ZC1iNGE2LTU0M2I1Y2U1MmI5OSJ9'
#  url = 'https://app.powerbigov.us/view?r=eyJrIjoiZDUwODIyNGEtODdkZC00MmI4LWFmOTctZWJjOWRkYmIzNzhhIiwidCI6IjcwYWY1NDdjLTY5YWItNDE2ZC1iNGE2LTU0M2I1Y2U1MmI5OSJ9'

  def colstr2int(df,col):
    df.loc[:,col] = df.loc[:,col].replace(',','', regex=True)
    df[col] = df[col].astype('int')

  def getdf_SD(category):
    wd = init_driver()
    wd.get(url)
    wait = WebDriverWait(wd, 60)
    print('Got Website')

    time.sleep(6)
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "i[title='Next Page']"))).click()
    print('Clicked Next Page')
    time.sleep(4)
    if category != 'Cases':
      wait.until(EC.element_to_be_clickable((By.XPATH, "//*[@aria-label='" + category + "']/div"))).click()
      print('Clicked metric')
    else:
      time.sleep(1)
  #  time.sleep(10)
    elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "g[class='axisGraphicsContext columnChart']")))
    ActionChains(wd).context_click(elements[0]).perform()
  #  time.sleep(10)
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div[title='Show as a table']"))).click()
    print('Show table')

    cats = []
    elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive ']")))
    for element in elements:
      cats.append(element.text)

    vals = []
    elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//*[@class='pivotTableCellWrap cell-interactive tablixAlignRight ']")))
    for element in elements:
      vals.append(element.text)

    df = pd.DataFrame([cats[-7:],vals[-7:]]).T
    df.columns = ['Category', category]
    colstr2int(df,category)

    wd.quit()

    return df

  df_cases = getdf_SD('Cases')
  df_deaths = getdf_SD('Deceased Among Cases')
  df_hosp = getdf_SD('Ever Hospitalized')

  df = df_cases.merge(df_deaths,how='left',on='Category')
  df = df.merge(df_hosp,how='left',on='Category')
  df.sort_values(by=['Category'], ascending=True, inplace=True)
  display(df)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    ##ws.update('J19',dataToWrite)

    # Write Data To Sheet
    writeTable(df,'','I20',ws)

# TN
def runTN(ws, write):
  url = 'https://apps.health.tn.gov/AEM_embed/TDH-2019-Novel-Coronavirus-Epi-and-Surveillance.pdf#toolbar=0'
  req = requests.get(url)
  pdf = BytesIO(req.content)
  first_page = PyPDF2.PdfFileReader(pdf)

  tables = tabula.read_pdf(url,pages=1)

  df = pd.DataFrame(tables[0].iloc[18:29,0:4])
  temp = df.iloc[:,0].str.split(r'^(.*) (\d+,*\d+)$', expand = True)
  temp.columns = ['','Race','Cases','']
  temp = temp.drop(columns='')
  temp2 = df.iloc[:,3].str.split(r'(^\d*,*\d+)', expand = True)
  temp2.columns = ['','Deaths','']
  temp2 = temp2.drop(columns='')
  temp2.iloc[-3:,0] = df.iloc[-3:,2]
  df = temp.join(temp2)
  df = df.replace([None],'---')
  df.iloc[7,0] = 'Ethnicity'
  df.replace(',','', regex=True, inplace=True)
  df.iloc[np.r_[0:7, 8:11],[1,2]] = df.iloc[np.r_[0:7, 8:11],[1,2]].astype('int')
  display(df)


  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('K20',dataToWrite)

    # Write Data To Sheet
    writeTable(df,'','J21',ws)

# TX
#import pandas as pd
#runTX(ws)
def runTX(ws,write):

  #url = 'https://dshs.texas.gov/coronavirus/TexasCOVID19Demographics.xlsx.asp'
  url = 'https://dshs.texas.gov/coronavirus/TexasCOVID19Demographics.xlsx'

  df_cases = pd.read_excel(url, sheet_name='Cases by RaceEthnicity', skiprows=0, engine='openpyxl')
  print("Cases by Race")
  display(df_cases)

  df_deaths = pd.read_excel(url, sheet_name='Fatalities by Race-Ethnicity', skiprows=0, engine='openpyxl')
  print("\nDeaths by Race")
  display(df_deaths)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('F31',dataToWrite)

    # Write Data To Sheet
    writeTable(df_cases,'Cases by Race','H20',ws)
    writeTable(df_deaths,'Deaths by Race','H29',ws)

# UT
def runUT(ws, write):
  url = 'https://coronavirus-dashboard.utah.gov/demographics.html'
  wd = init_driver()
  wd.get(url)
  #time.sleep(20)
  soup = BeautifulSoup(wd.page_source, 'html.parser')

  # Find Table
  t_1 = soup.find_all('table',{"id": "DataTables_Table_1"})
  t_2 = soup.find_all('table',{"id": "DataTables_Table_2"})

  table_body = t_1[0].find('tbody')
  rows = table_body.find_all('tr')
  data = []
  col_names = ['Category','','Cases','','','Hospitalizations','','Deaths','','']
  for row in rows:
      cols = row.find_all('td')
      cols = [ele.text.strip() for ele in cols]
      data.append([ele for ele in cols if ele]) # Get rid of empty values
  df = pd.DataFrame(data, columns=col_names)
  df.drop(df[''], axis = 1, inplace = True)
  df = df.astype({'Cases': 'int64', 'Hospitalizations': 'int64', 'Deaths': 'int64'})

  table_body = t_2[0].find('tbody')
  rows = table_body.find_all('tr')
  data = []
  col_names = ['Category','','Persons Tested','','','','','']
  for row in rows:
      cols = row.find_all('td')
      cols = [ele.text.strip() for ele in cols]
      data.append([ele for ele in cols if ele]) # Get rid of empty values

  df_tests = pd.DataFrame(data, columns=col_names)
  df_tests.drop(df_tests[''], axis = 1, inplace = True)
  df_tests.iloc[:,1] = df_tests.iloc[:,1].astype('int')

  display(df)
  display(df_tests)

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('G34',dataToWrite)

    # Write Data To Sheet
    writeTable(df,'','H17',ws)
    writeTable(df_tests,'','H34',ws)

# VA
#import pandas as pd
# import requests

def runVA(ws, write):
  # VA Totals
  url = 'https://data.virginia.gov/api/views/bre9-aqqr/rows.csv?accessType=DOWNLOAD'
  df_totals = pd.read_csv(url,parse_dates=['Report Date'])
  maxdateTot = df_totals ['Report Date'].max()
  print('\nVA Case, Death and Hospitalizations Totals')
  print(maxdateTot,'\n')
  #df_cases = df_totals.groupby(['Report Date'])[['totalcountconfirmed','totalcountdeaths']].sum()
  df_tots = df_totals[df_totals['Report Date'] == maxdateTot]
  df_tots = df_tots.groupby(['Report Date'])[['Total Cases','Hospitalizations','Deaths']].sum()
  df_tots = df_tots.replace(',','', regex=True).apply(pd.to_numeric)
  display(df_tots)


  # VA Case, Deaths, Hosptializations Confirmed & Probables"
  url = 'https://data.virginia.gov/api/views/uqs3-x7zh/rows.csv?accessType=DOWNLOAD'
  df_totals = pd.read_csv(url,parse_dates=['Report Date'])
  maxdateTot = df_totals ['Report Date'].max()
  print('\nVA Case, Death and Hospitalizations Totals')
  print(maxdateTot,'\n')
  df_confProb = df_totals[df_totals['Report Date'] == maxdateTot]
  df_confProb = df_confProb.fillna('0')
  df_confProb['Number of Cases'] = df_confProb['Number of Cases'].replace(',','', regex=True).apply(pd.to_numeric)
  df_confProb['Number of Hospitalizations'] = df_confProb['Number of Hospitalizations'].replace(',','', regex=True).apply(pd.to_numeric)
  df_confProb['Number of Deaths'] = df_confProb['Number of Deaths'].replace(',','', regex=True).apply(pd.to_numeric)

  display(df_confProb)

  # VA Race & Ethnicity
  url='https://data.virginia.gov/api/views/9sba-m86n/rows.csv?accessType=DOWNLOAD'
  df_raceeth = pd.read_csv(url,parse_dates=['Report Date'])

  print ('\nVA Race and Ethnicity Totals')
  maxdateRace = df_raceeth ['Report Date'].max()
  print(maxdateRace,'\n')
  df_dem = df_raceeth[df_raceeth['Report Date'] == maxdateRace]
  df_dem = df_dem.groupby('Race and Ethnicity').sum()
  df_dem = df_dem.fillna('0')
  df_dem['Race/Ethnicity']=['Asian or Pacific Islander', 'Black', 'Latino', 'Native American',
       'Not Reported', 'Other Race', 'Two or more races', 'White']
  df_dem['Number of Cases'] = df_dem['Number of Cases'].replace(',','', regex=True).apply(pd.to_numeric)
  df_dem['Number of Hospitalizations'] = df_dem['Number of Hospitalizations'].replace(',','', regex=True).apply(pd.to_numeric)
  df_dem['Number of Deaths'] = df_dem['Number of Deaths'].replace(',','', regex=True).apply(pd.to_numeric)

  display(df_dem)

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('B33',dataToWrite)
    #ws.update('J14',dataToWrite)

    # Write Data To Sheet
    writeTable(df_confProb,'','K15',ws)
    writeTable(df_tots,'','K19',ws)
    writeTable(df_dem,'','C33',ws)

# VT
def runVT(ws, write):
  # Internet Explorer version of the dashboard
  url = 'https://vcgi.maps.arcgis.com/apps/opsdashboard/index.html#/25e8ef0453e347a8b216ddf98a25a040'
  wd = init_driver()
  wd.get(url)
  time.sleep(20)
  soup = BeautifulSoup(wd.page_source, 'html.parser')

  # Find Totals
  g_all = soup.find_all('g',{"class": "amcharts-pie-item"})

  pattern = re.compile(r'aria-label=\"(.+) \"')
  demo = []
  for i in range(len(g_all)):
    cat = re.findall(pattern,str(g_all[i]))[0].split()
    lcat = len(cat)
    if lcat > 3:
      for j in range(1,lcat-2):
        cat[0] = cat[0] + '_' + cat[j]
      cat[1:3] = cat[lcat-2:lcat]
    demo.append(cat)

  df_demo = pd.DataFrame(demo).loc[:,[0,2]]
  df_demo.iloc[:,1].replace(',','', regex=True, inplace=True)
  df_demo.iloc[:,1] = df_demo.iloc[:,1].astype('int')

  df_demo_cases = df_demo.loc[5:11,:]
  df_demo_cases.columns = ['Category','Cases']

  df_demo_deaths = df_demo.loc[14:20,:]
  df_demo.loc[14] = ['Hispanic:', 0]
  df_demo_deaths.columns = ['Category','Deaths']

  display(df_demo_cases)
  display(df_demo_deaths)

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('I19',dataToWrite)

    # Write Data To Sheet
    writeTable(df_demo_cases,'','H20',ws)
    writeTable(df_demo_deaths,'','K20',ws)

# WA
def runWA(ws,write):
  def parse_table(t,col_names):
    table_body = t.find('tbody')
    rows = table_body.find_all('tr')
    data = []
    col_names.extend(['',''])
    for row in rows:
        cols = row.find_all('td')
        cols = [ele.text.strip() for ele in cols]
        data.append([ele for ele in cols if ele]) # Get rid of empty values
    df = pd.DataFrame(data, columns=col_names)
    df.drop(df.columns[[2,3]], axis = 1, inplace = True)
    df.iloc[:,1].replace(',','', regex=True, inplace=True)
    df.iloc[:,1] = df.iloc[:,1].astype('int')
    display(df)
    return df

  url = 'https://www.doh.wa.gov/Emergencies/COVID19/DataDashboard#tables'
  wd = init_driver()
  wd.get(url)
  time.sleep(10)
  soup = BeautifulSoup(wd.page_source, 'html.parser')

  # Find Tables
  t_all = soup.find_all('table',{"class": "table table-bordered table-striped"})

  for t in t_all:
    if 'Cumulative COVID-19 Cases by Race/Ethnicity' in t.text:
      cases_table = t
    if 'Cumulative COVID-19 Hospitalizations by Race/Ethnicity' in t.text:
      hosp_table = t
    if 'Cumulative COVID-19 Deaths by Race/Ethnicity' in t.text:
      deaths_table = t

  df_cases = parse_table(cases_table,['Category','Cases'])
  df_hosp = parse_table(hosp_table,['Category','Hospitalizations'])
  df_deaths = parse_table(deaths_table,['Category','Deaths'])

  #Template for writing to state page
  if write == True:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('B33',dataToWrite)

    # Write Data To Sheet
    writeTable(df_cases,'','A35',ws)
    writeTable(df_hosp,'','G35',ws)
    writeTable(df_deaths,'','D35',ws)

#WI
#from io import StringIO, BytesIO
#from bs4 import BeautifulSoup
#import pandas as pd
def runWI(ws, write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC

  url = 'https://opendata.arcgis.com/datasets/859602b5d427456f821de3830f89301c_11.csv?outSR=%7B%22latestWkid%22%3A3857%2C%22wkid%22%3A102100%7D'

  #TOTALS
  df_cases = pd.read_csv(url,encoding='utf-8')
  maxdate = df_cases['DATE'].max()
  print("Confirmed Case & Death Totals")
  print(maxdate,'\n')
  df_cases = df_cases[df_cases['DATE'] == maxdate]
  #display(df_cases)
  df_totals = df_cases.groupby(['DATE'])[['POSITIVE','DEATHS']].sum()
  display(df_totals)

  #DEMOGRAPHICS
  #urls
  deaths_url="https://www.dhs.wisconsin.gov/covid-19/deaths.htm"
  cases_url="https://www.dhs.wisconsin.gov/covid-19/cases.htm"

  #target strings to tableau view
  cases_string = "casesby-group"
  deaths_string = "deathsby-group"

  #xpaths
  #options on chart
  confirm_radio_xpath = "//*[@id='[Parameters].[Case status Parameter]_0']/div[2]/input"
  prob_radio_xpath = "//*[@id='[Parameters].[Case status Parameter]_1']/div[2]/input"
  race_radio_xpath = "//*[@id='[Parameters].[Select a subgroup (copy)]_2']/div[2]/input"
  eth_radio_xpath = "//*[@id='[Parameters].[Select a subgroup (copy)]_3']/div[2]/input"
  main_download_btn_xpath = "//*[@id='download-ToolbarButton']"
  #download form options
  #crosstab_btn_xpath = "//*[@id='DownloadDialog-Dialog-Body-Id']/div/button[3]"
  crosstab_btn_xpath = "//button[text()='Crosstab']"
  csv_radio_xpath = "//*[@id='export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id']/div/div[2]/div[2]/div/label[2]"
  fin_dwnld_xpath = "//*[@id='export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id']/div/div[3]/button"

  #downloaded csv paths
  #csv_cases = "/content/Case chart.csv"
  csv_cases = "Case chart.csv"
  #csv_deaths = "/content/Death chart.csv"
  csv_deaths = "Death chart.csv"

  #visit metric url and go to tableau src page
  def visitNextURL(url, target_string):
    wd.get(url)
    wait = WebDriverWait(wd, 60)
    print('waiting')
    time.sleep(20)
    soup = BeautifulSoup(wd.page_source, "html.parser")
    iframes = soup.find_all("iframe")
    #cases_src = [tag["src"] for tag in iframes if target_string in tag["src"]]
    cases_src = []
    for tag in iframes:
        try:
            if target_string in tag["src"]:
                cases_src.append(tag["src"])
        except Exception as e:
            print('Got Exception')
    target_src = cases_src[0]
    print("Loading new URL")
    wd.get(target_src)
    #time.sleep(3)

  #download view and convert to df
  def getCSV(status_choice, demo_choice, metric_csv):
    wait = WebDriverWait(wd, 20)
    #choose confirmed/probable on chart
    wait.until(EC.presence_of_element_located((By.XPATH, status_choice))).click()
    print("clicked case status")
    time.sleep(1)
    #choose demographic on chart
    wait.until(EC.presence_of_element_located((By.XPATH, demo_choice))).click()
    print("clicked demographic")
    time.sleep(1)

    #click download button at button of tableau to open download dialog box
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ".tab-icon-download"))).click()
    print("clicked download button")

    #click crosstab option
    wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Crosstab']"))).click()
    print("clicked crosstab button")

    #click csv option
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "label[data-tb-test-id='crosstab-options-dialog-radio-csv-Label']"))).click()
    print("clicked csv option")

    #download csv
    wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Download']"))).click()
    print("clicked download button")
    time.sleep(2) #wait for download

    #make df
    df = pd.read_csv(metric_csv, sep="\t", encoding="utf-16") #dumb tableau encoding
    return df

  #start webdriver
  wd = init_driver()
  visitNextURL(cases_url, cases_string)

  #CONFIRMED CASES
  #by race
  print("-" * 10)
  print("CONFIRMED CASES:")
  df_casesConfirmRace = getCSV(confirm_radio_xpath, race_radio_xpath, csv_cases)
  df_casesConfirmRace['Number of cases'] = df_casesConfirmRace['Number of cases'].str.replace(',', '').astype('int')
  df_casesConfirmRace.rename(columns={list(df_casesConfirmRace)[0]:'Demographic'}, inplace=True)
  df_casesConfirmRace = df_casesConfirmRace[['Demographic','Number of cases']]
  display(df_casesConfirmRace)
  #by ethnicity
  df_casesConfirmEth = getCSV(confirm_radio_xpath,eth_radio_xpath, csv_cases)
  df_casesConfirmEth['Number of cases'] = df_casesConfirmEth['Number of cases'].str.replace(',', '').astype('int')
  df_casesConfirmEth.rename(columns={list(df_casesConfirmEth)[0]:'Demographic'}, inplace=True)
  df_casesConfirmEth = df_casesConfirmEth[['Demographic','Number of cases']]
  display(df_casesConfirmEth)
  print("-" * 10)
  print("PROBABLE CASES:")
  #PROBABLE CASES
  #by race
  df_casesProbsRace = getCSV(prob_radio_xpath, race_radio_xpath, csv_cases)
  df_casesProbsRace['Number of cases'] = df_casesProbsRace['Number of cases'].str.replace(',', '').astype('int')
  df_casesProbsRace.rename(columns={list(df_casesProbsRace)[0]:'Demographic'}, inplace=True)
  df_casesProbsRace = df_casesProbsRace[['Demographic','Number of cases']]
  display(df_casesProbsRace)
  #by ethnicity
  df_casesProbsEth = getCSV(prob_radio_xpath, eth_radio_xpath, csv_cases)
  df_casesProbsEth['Number of cases'] = df_casesProbsEth['Number of cases'].str.replace(',', '').astype('int')
  df_casesProbsEth.rename(columns={list(df_casesProbsEth)[0]:'Demographic'}, inplace=True)
  df_casesProbsEth = df_casesProbsEth[['Demographic','Number of cases']]
  display(df_casesProbsEth)

  #reset driver
  wd.quit()

  #start webdriver
  wd = init_driver()
  visitNextURL(deaths_url, deaths_string)

  print("-" * 10)
  print("CONFIRMED DEATHS:")
  #CONFIRMED DEATHS
  #by race
  df_deathsConfirmRace = getCSV(confirm_radio_xpath, race_radio_xpath, csv_deaths)
  df_deathsConfirmRace['Deaths'] = df_deathsConfirmRace['Deaths'].str.replace(',', '').astype('int')
  df_deathsConfirmRace.rename(columns={list(df_deathsConfirmRace)[0]:'Demographic'}, inplace=True)
  df_deathsConfirmRace = df_deathsConfirmRace[['Demographic','Deaths']]
  display(df_deathsConfirmRace)
  #by ethnicity
  df_deathsConfirmEth = getCSV(confirm_radio_xpath, eth_radio_xpath, csv_deaths)
  df_deathsConfirmEth['Deaths'] = df_deathsConfirmEth['Deaths'].str.replace(',', '').astype('int')
  df_deathsConfirmEth.rename(columns={list(df_deathsConfirmEth)[0]:'Demographic'}, inplace=True)
  df_deathsConfirmEth = df_deathsConfirmEth[['Demographic','Deaths']]
  display(df_deathsConfirmEth)

  print("-" * 10)
  print("PROBABLE DEATHS:")
  #PROBABLE DEATHS
  #by race
  df_deathsProbsRace = getCSV(prob_radio_xpath, race_radio_xpath, csv_deaths)
  df_deathsProbsRace.rename(columns={list(df_deathsProbsRace)[0]:'Demographic'}, inplace=True)
  df_deathsProbsRace = df_deathsProbsRace[['Demographic','Deaths']]
  display(df_deathsProbsRace)
  #by ethnicity
  df_deathsProbsEth = getCSV(prob_radio_xpath, eth_radio_xpath, csv_deaths)
  df_deathsProbsEth.rename(columns={list(df_deathsProbsEth)[0]:'Demographic'}, inplace=True)
  df_deathsProbsEth = df_deathsProbsEth[['Demographic','Deaths']]
  display(df_deathsProbsEth)

  #reset driver
  wd.quit()

  if write == 1:
    # Write Paste Date To Sheet
    dataToWrite = [[date.today().strftime('%m/%d')]]
    #ws.update('J15',dataToWrite)

    # Write Data To Sheet
    writeTable(df_totals,'Confirmed Case & Death Totals','L15',ws)

    writeTable(df_casesConfirmRace, 'Confirmed Cases by Race', 'K19',ws)
    writeTable(df_casesConfirmEth, 'Confirmed Cases by Ethnicity', 'K28',ws)
    writeTable(df_casesProbsRace, 'Probable Cases by Race', 'N19',ws)
    writeTable(df_casesProbsEth, 'Probable Cases by Ethnicity', 'N28',ws)

    writeTable(df_deathsConfirmRace, 'Confirmed Deaths by Race', 'K34',ws)
    writeTable(df_deathsConfirmEth, 'Confirmed Deaths by Ethnicity', 'K43',ws)
    writeTable(df_deathsProbsRace, 'Probable Deaths by Race', 'N34',ws)
    writeTable(df_deathsProbsEth, 'Probable Deaths by Ethnicity', 'N43',ws)

# WY ************

def runWY(ws,write):
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  #url = 'https://public.tableau.com/vizql/w/EpiCOVIDtest/v/Dashboard/tempfile/sessions/324E4F03F0184305838EC00E7A01CB86-0:0/?key=4098009697&keepfile=yes&attachment=yes'
  srcCases="https://public.tableau.com/views/EpiCOVIDtest/Dashboard?:embed=y&amp;:showVizHome=no&amp;:host_url=https%3A%2F%2Fpublic.tableau.com%2F&amp;:embed_code_version=3&amp;:tabs=no&amp;:toolbar=no&amp;:animate_transition=yes&amp;:display_static_image=no&amp;:display_spinner=no&amp;:display_overlay=yes&amp;:display_count=yes&amp;publish=yes&amp;:loadOrderID=0"
  srcDeaths="https://public.tableau.com/views/EpiCOVIDtest/COVID-19RelatedDeaths?%3Aembed=y&amp%3B%3AshowVizHome=no&amp%3B%3Ahost_url=https%3A%2F%2Fpublic.tableau.com%2F&amp%3B%3Aembed_code_version=3&amp%3B%3Atabs=no&amp%3B%3Atoolbar=no&amp%3B%3Aanimate_transition=yes&amp%3B%3Adisplay_static_image=no&amp%3B%3Adisplay_spinner=no&amp%3B%3Adisplay_overlay=yes&amp%3B%3Adisplay_count=yes&amp%3Bpublish=yes&amp%3B%3AloadOrderID=0"

  #download form options
  data_btn_xpath ='//*[@id="download-ToolbarButton"]/span[1]'

  #xpaths from tableau
  crosstab_xpath='//*[@id="DownloadDialog-Dialog-Body-Id"]/div/fieldset/button[3]'
  csv_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[2]/div[2]/div/label[2]'
  cases_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[4]/div/div/div[1]/div'
  deaths_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[5]/div/div/div'
  probables_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[10]/div/div/div'
  raceth_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[11]/div/div/div'
  racethdeath_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[1]/div[2]/div/div/div[5]/div/div/div'
  #download button for getting the file
  dnld_xpath='//*[@id="export-crosstab-options-dialog-Dialog-BodyWrapper-Dialog-Body-Id"]/div/div[3]/button'

  #file csvs
  csv_cases = "cases.csv"
  csv_deaths = "death.csv"
  csv_probables = 'probable cases.csv'
  csv_raceth = 'raceth.csv'
  csv_racethdeath = 'raceth (death).csv'

  #press download button
  def getCSV(metric_xpath,csv_metric):
      wait = WebDriverWait(wd, 20)
      #click download button at button of tableau to open download dialog box 
      wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ".tab-icon-download"))).click()
      print("clicked download button")

      #click crosstab option
      wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Crosstab']"))).click()
      print("clicked crosstab button")

      #click csv option
      wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "label[data-tb-test-id='crosstab-options-dialog-radio-csv-Label']"))).click()
      print("clicked csv option")

      #select metric
      wait.until(EC.element_to_be_clickable((By.XPATH, metric_xpath))).click()
      print("clicked metric")
      #download csv
      wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Download']"))).click()
      print("clicked download button")
      time.sleep(2) #wait for download

      #make df
      df=pd.read_csv(csv_metric,sep="\t", encoding="utf-16")
      df=df.fillna('0')
      return df

  #download view and convert to df
  #cases
  wd=init_driver()
  wd.get(srcCases)
  time.sleep(3)
  print("\nCases, Deaths, Probables, Raceth")
  print(srcCases)
  #Cases
  print("-" * 10)
  print("Cases")
  df_cases=getCSV(cases_xpath,csv_cases)
  #convert to integer
  df_cases['Laboratory Confirmed Cases'] = df_cases['Laboratory Confirmed Cases'].str.replace(r",",'', 1)
  df_cases = df_cases.astype({"Laboratory Confirmed Cases": int})
  display(df_cases)
  time.sleep(3)
  #Deaths
  print("-" * 10)
  print("Deaths")
  df_deaths=getCSV(deaths_xpath,csv_deaths)
  display(df_deaths)
  time.sleep(3)
  #Probables
  print("-" * 10)
  print("Probables")
  df_probables=getCSV(probables_xpath,csv_probables)
  df_probables['Probable Cases'] = df_probables['Probable Cases'].str.replace(r",",'', 1).astype('float')
  display(df_probables)
  time.sleep(3)
  #Race Ethnicity for Cases
  print("-" * 10)
  print("Race Ethnicity for Cases")
  df_raceth=getCSV(raceth_xpath,csv_raceth)
  tcols=list(df_raceth.columns)
  tcols=['Race','Count','%']
  df_raceth.columns=tcols
  df_raceth['Count']= df_raceth['Count'].str.replace(r",",'',1)
  df_raceth['Count'] = df_raceth['Count'].astype(int)
  display(df_raceth)
  wd.quit()

  wd=init_driver()
  wd.get(srcDeaths)
  print("\nCases, Deaths, Probables, Raceth")
  print(srcDeaths)
  time.sleep(3)
  #Race Ethnicity for Deaths
  print("-" * 10)
  print("Race Ethnicity for Deaths")
  df_racethdeath=getCSV(racethdeath_xpath,csv_racethdeath)
  tcols=list(df_racethdeath.columns)
  tcols=['Race','Count','%']
  df_racethdeath.columns=tcols
  display(df_racethdeath)
  display(df_probables)
  wd.quit()

  if write == 1:
      # Write Paste Date To Sheet
      dataToWrite = [[date.today().strftime('%m/%d')]]
      #ws.update('G16',dataToWrite)

      # Write Data To Sheet
      writeTable(df_cases,'Case Totals','M17',ws)
      writeTable(df_probables,'Probable Totals','M20',ws)
      writeTable(df_deaths,'Death Totals','M32',ws)
      writeTable(df_raceth,'Cases by Race & Ethnicity','H17',ws)
      writeTable(df_racethdeath,'Deaths by Race & Ethnicity','H32',ws)

